---
phase: 8.3-gap-closure-queue-dispatch-inserted
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/queue/worker.py
autonomous: true
requirements:
  - R4.2
  - R4.3
gap_closure: true

must_haves:
  truths:
    - "_replay_command() inspects job_type before reading payload.get('command')"
    - "Jobs with job_type='capture_git_commits' call process_pending_commits() directly"
    - "Jobs with job_type='capture_git_commits' no longer land in the dead letter queue"
    - "Generic command-replay path is unchanged for other job types"
    - "Direct calls to process_pending_commits() still work (not broken by this change)"
  artifacts:
    - path: "src/queue/worker.py"
      provides: "Fixed BackgroundWorker with job-type dispatch"
      contains: "_handle_capture_git_commits"
      exports: ["BackgroundWorker"]
  key_links:
    - from: "src/queue/worker.py:_replay_command"
      to: "src/queue/worker.py:_handle_capture_git_commits"
      via: "job_type == 'capture_git_commits' branch"
      pattern: "if.*job_type.*capture_git_commits"
    - from: "src/queue/worker.py:_handle_capture_git_commits"
      to: "src/capture/git_worker.py:process_pending_commits"
      via: "asyncio.run() call"
      pattern: "asyncio\\.run.*process_pending_commits"
---

<objective>
Fix `BackgroundWorker._replay_command()` in `src/queue/worker.py` to dispatch
`capture_git_commits` jobs directly to `process_pending_commits()` instead of
attempting invalid subprocess reconstruction.

Purpose: Restores Flow 3 (git post-commit hook → queue → worker → LLM → Kuzu).
Currently every git capture job lands in the dead letter queue because the worker
reads `payload.get('command', '')` which returns `""` for this job type (payload
has `pending_file`, not `command`). The fix adds job-type dispatch before the
generic CLI-replay path.

Output: Updated `src/queue/worker.py` with `_handle_capture_git_commits()` method
and dispatch guard in `_replay_command()`.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/queue/worker.py
@src/capture/git_worker.py
@src/queue/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add job-type dispatch to _replay_command and implement _handle_capture_git_commits</name>
  <files>src/queue/worker.py</files>
  <action>
Modify `src/queue/worker.py` with two targeted changes:

**Change 1 — Add asyncio import at the top of the file** (after the existing `import subprocess`
line, add `import asyncio`). The file currently imports: subprocess, time, concurrent.futures,
threading, typing, structlog, src.queue.storage. Add `import asyncio` after `import subprocess`.

**Change 2 — Modify `_replay_command()`** (currently at line 257). Insert a job-type dispatch
guard as the FIRST thing in the method, before the existing `payload = item['payload']` line:

```python
def _replay_command(self, item: dict) -> None:
    """Replay CLI command from job payload.

    Reconstructs CLI command from payload and executes via subprocess.
    This is the CLI-first architecture: worker replays commands, CLI remains
    single source of truth (per user decision).

    For job types that do not use the generic CLI-replay format (i.e., they
    have a structured payload rather than a 'command' key), dispatch to a
    dedicated handler before falling through to the generic path.

    Args:
        item: Job dict with payload containing command and args

    Raises:
        RuntimeError: If command execution fails (non-zero exit code)
    """
    # Dispatch by job type for structured payloads (not generic CLI replay)
    job_type = item.get('job_type', '')
    if job_type == 'capture_git_commits':
        success = self._handle_capture_git_commits(item)
        if not success:
            raise RuntimeError(
                f"capture_git_commits handler failed for job {item['id']}: "
                "payload missing 'pending_file' key"
            )
        return

    payload = item['payload']

    # Extract command and arguments from payload
    # Expected format: {"command": "add", "args": ["content"], "kwargs": {"scope": "project"}}
    command = payload.get('command', '')
    args = payload.get('args', [])
    kwargs = payload.get('kwargs', {})

    # Construct CLI command: ["graphiti", command, *args, *flags]
    cli_command = ["graphiti", command] + args + self._kwargs_to_flags(kwargs)

    # Execute command via subprocess
    result = subprocess.run(
        cli_command,
        capture_output=True,
        text=True
    )

    # Check result
    if result.returncode != 0:
        raise RuntimeError(
            f"Command failed with exit code {result.returncode}: {result.stderr}"
        )

    # Log success
    self._logger.info(
        "command_replayed",
        job_id=item['id'],
        command=command,
        args=args,
        kwargs=kwargs
    )
```

**Change 3 — Add `_handle_capture_git_commits()` method** after `_replay_command()` and
before `_kwargs_to_flags()`:

```python
def _handle_capture_git_commits(self, item: dict) -> bool:
    """Handle capture_git_commits job type.

    This job type is enqueued by src/capture/git_worker.enqueue_git_processing()
    with payload={"pending_file": "/path/to/pending_commits"}. It does NOT use
    the generic CLI-replay format (no 'command' key), so it must be dispatched
    directly to process_pending_commits().

    Calling process_pending_commits() directly (rather than via subprocess) avoids
    the PATH dependency of the generic replay path and is safe because
    process_pending_commits() is pure Python with no global state mutations.

    Args:
        item: Job dict; payload must contain 'pending_file' key

    Returns:
        True if handler dispatched successfully, False if payload is malformed
    """
    payload = item.get('payload', {})
    pending_file_str = payload.get('pending_file')

    if not pending_file_str:
        self._logger.error(
            "capture_git_commits_missing_pending_file",
            job_id=item['id'],
            payload=payload
        )
        return False

    from pathlib import Path
    from src.capture.git_worker import process_pending_commits

    pending_file = Path(pending_file_str)

    self._logger.info(
        "capture_git_commits_dispatching",
        job_id=item['id'],
        pending_file=str(pending_file)
    )

    asyncio.run(process_pending_commits(pending_file=pending_file))

    self._logger.info(
        "capture_git_commits_complete",
        job_id=item['id'],
        pending_file=str(pending_file)
    )
    return True
```

**Placement note:** Insert `_handle_capture_git_commits()` between the closing of
`_replay_command()` and the `@staticmethod` decorator of `_kwargs_to_flags()`.

**Do NOT change:** `_execute_with_retry()`, `_run()`, `start()`, `stop()`, `is_running()`,
`_process_single_job()`, `_process_parallel_batch()`, `_kwargs_to_flags()`, or any imports
other than adding `import asyncio`.

**Why direct call instead of subprocess:** `process_pending_commits()` is a pure async
function with no global side effects. The subprocess path requires `graphiti` to be on
PATH, which is not guaranteed inside the worker thread. Direct call is faster and more
reliable. This matches the existing pattern in `src/llm/__init__.py` where processing
functions are called directly, not via subprocess.
  </action>
  <verify>
Run the following from the project root to confirm the changes are syntactically correct
and the dispatch path is reachable:

```bash
.venv/bin/python -c "
from src.queue.worker import BackgroundWorker
from src.queue.storage import JobQueue
import tempfile, pathlib

# Construct a fake job matching what git_worker.enqueue_git_processing() creates
job = {
    'id': 'test-job-001',
    'job_type': 'capture_git_commits',
    'payload': {'pending_file': '/tmp/does_not_exist_pending'},
    'parallel': False,
    'attempts': 0,
    'created_at': 0.0,
}

# Verify _handle_capture_git_commits returns False for missing file
# (pending file doesn't exist, process_pending_commits handles that gracefully)
with tempfile.TemporaryDirectory() as tmp:
    q = JobQueue(db_path=tmp)
    w = BackgroundWorker(q)
    # Missing file path returns empty list, not an error — so handler returns True
    # (process_pending_commits reads-and-clears; missing file = no commits = [])
    print('Import OK: BackgroundWorker has _handle_capture_git_commits:', hasattr(w, '_handle_capture_git_commits'))
    print('Import OK: _replay_command exists:', hasattr(w, '_replay_command'))
"
```

Also verify with `grep`:
- `grep -n "_handle_capture_git_commits" src/queue/worker.py` shows lines in both
  `_replay_command` and the method definition.
- `grep -n "import asyncio" src/queue/worker.py` shows the import exists.
- `grep -n "capture_git_commits" src/queue/worker.py` shows at least 3 occurrences.
  </verify>
  <done>
- `src/queue/worker.py` imports `asyncio` at the top
- `_replay_command()` checks `job_type == 'capture_git_commits'` before reading `payload.get('command', '')`
- `_handle_capture_git_commits()` method exists, reads `pending_file` from payload, calls `asyncio.run(process_pending_commits(...))`
- Python import check passes with no ImportError or AttributeError
- grep confirms all three identifiers (`import asyncio`, `_handle_capture_git_commits`, `capture_git_commits`) are present
- Generic CLI-replay path for other job types is untouched
  </done>
</task>

</tasks>

<verification>
After Task 1 completes:

1. `python -c "from src.queue.worker import BackgroundWorker"` exits 0 (no import errors)
2. `grep -c "capture_git_commits" src/queue/worker.py` returns >= 3
3. `grep -n "import asyncio" src/queue/worker.py` shows line <= 25 (top of file)
4. `grep -n "job_type.*capture_git_commits\|capture_git_commits.*job_type" src/queue/worker.py` shows the dispatch guard
5. Existing job types (e.g., `add_knowledge`) that use `payload.get('command', ...)` are unaffected — they fall through to the original code path
</verification>

<success_criteria>
- `_replay_command()` never reads `payload.get('command', '')` for `capture_git_commits` jobs
- `_handle_capture_git_commits()` calls `asyncio.run(process_pending_commits(pending_file=...))` with the path from payload
- Code is syntactically valid Python (import check passes)
- No existing method signatures changed
- Flow 3 can now execute: job hits worker → `_replay_command` dispatches → `_handle_capture_git_commits` → `process_pending_commits`
</success_criteria>

<output>
After completion, create `.planning/phases/8.3-gap-closure-queue-dispatch-inserted/8.3-01-SUMMARY.md`
with what was changed, why, and which key lines were added.
</output>
