---
phase: 7.1-git-indexing-pivot-inserted
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/indexer/__init__.py
  - src/indexer/state.py
  - src/indexer/quality_gate.py
  - src/indexer/indexer.py
  - src/indexer/extraction.py
autonomous: true
requirements:
  - R8.1
  - R8.2

must_haves:
  truths:
    - "GitIndexer.run() processes qualifying commits from git history and stores them via add_episode()"
    - "Incremental runs skip commits already in processed_shas using last_indexed_sha cursor"
    - "Quality gate rejects bot, version-bump-only, pure-merge, and tiny commits (<=3 lines) before any LLM call"
    - "Two-pass extraction produces two add_episode() calls per qualifying commit (structured + freeform), both tagged with source_description starting with 'git-history-index:'"
    - "Large diffs (>300 lines) get summarized first before extraction passes"
    - "State file is written atomically via Path.replace() and lives at .graphiti/index-state.json"
    - "is_within_cooldown() returns True if last run was within 5 minutes"
  artifacts:
    - path: "src/indexer/__init__.py"
      provides: "Public module exports — GitIndexer and IndexState"
      exports: ["GitIndexer", "IndexState"]
    - path: "src/indexer/state.py"
      provides: "JSON state management for SHA cursor, processed set, cooldown"
      exports: ["IndexState", "load_state", "save_state", "is_within_cooldown"]
    - path: "src/indexer/quality_gate.py"
      provides: "should_skip_commit() returning (bool, reason_str)"
      exports: ["should_skip_commit"]
    - path: "src/indexer/indexer.py"
      provides: "GitIndexer class with run(), reset_full() methods"
      exports: ["GitIndexer"]
    - path: "src/indexer/extraction.py"
      provides: "extract_commit_knowledge() async function"
      exports: ["extract_commit_knowledge"]
  key_links:
    - from: "src/indexer/indexer.py"
      to: "src/indexer/quality_gate.py"
      via: "should_skip_commit() call before diff fetch"
      pattern: "should_skip_commit"
    - from: "src/indexer/indexer.py"
      to: "src/indexer/extraction.py"
      via: "extract_commit_knowledge() awaited per qualifying commit"
      pattern: "extract_commit_knowledge"
    - from: "src/indexer/extraction.py"
      to: "graphiti_core add_episode()"
      via: "graphiti_instance.add_episode() called twice per commit"
      pattern: "add_episode"
    - from: "src/indexer/indexer.py"
      to: "src/indexer/state.py"
      via: "save_state() after each processed commit for crash recovery"
      pattern: "save_state"
    - from: "src/indexer/indexer.py"
      to: "src/capture/git_capture.py"
      via: "fetch_commit_diff() reuse"
      pattern: "fetch_commit_diff"
---

<objective>
Build the git history indexer core module at src/indexer/. This is the engine that bootstraps Kuzu knowledge from past git commits — the central deliverable of Phase 7.1.

Purpose: Brownfield projects need their history indexed once for the knowledge graph to have context about past decisions. This module implements the traversal, quality filtering, extraction, and state management pipeline.

Output: A fully functional src/indexer/ Python package with GitIndexer class, JSON state management, commit quality gate, and two-pass LLM extraction pipeline.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/7.1-git-indexing-pivot-inserted/7.1-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create indexer/state.py and indexer/quality_gate.py</name>
  <files>
    src/indexer/__init__.py
    src/indexer/state.py
    src/indexer/quality_gate.py
  </files>
  <action>
Create the `src/indexer/` directory and its foundation modules.

**src/indexer/__init__.py** — minimal module init:
```python
"""Git history indexer for bootstrapping Kuzu knowledge from past commits.

Provides GitIndexer for on-demand git history traversal and knowledge extraction.
Phase 6 post-commit hook handles ongoing real-time capture; this module
handles historical bootstrap (brownfield) and stale re-indexing.
"""
from src.indexer.indexer import GitIndexer
from src.indexer.state import IndexState

__all__ = ["GitIndexer", "IndexState"]
```

**src/indexer/state.py** — JSON state management:

Create a dataclass `IndexState` with fields: `version: str = "1.0"`, `last_indexed_sha: str | None = None`, `processed_shas: list[str] = field(default_factory=list)`, `last_run_at: str | None = None`, `indexed_commits_count: int = 0`.

Implement these functions:

`STATE_FILE_NAME = "index-state.json"` (relative path within .graphiti/)

`load_state(project_root: Path) -> IndexState`: Read `.graphiti/index-state.json` if it exists, deserialize to IndexState, return. If file missing or malformed, return a fresh IndexState. Handle JSONDecodeError gracefully by returning fresh state.

`save_state(project_root: Path, state: IndexState) -> None`: Write to `.graphiti/index-state.json` using the atomic rename pattern (write to .tmp file, then Path.replace()). Create parent dir with mkdir(parents=True, exist_ok=True). Serialize the dataclass to dict via dataclasses.asdict().

`is_within_cooldown(project_root: Path, cooldown_minutes: int = 5) -> bool`: Load state, check `last_run_at` field. If None, return False. Parse as ISO format datetime (datetime.fromisoformat()). Compare against datetime.now(timezone.utc). Return True if elapsed < timedelta(minutes=cooldown_minutes).

`is_sha_processed(state: IndexState, sha: str) -> bool`: Return `sha in state.processed_shas` (or sha[:8] in processed_shas — see below).

`add_processed_sha(state: IndexState, sha: str) -> None`: Append sha to state.processed_shas. Cap the list at 10,000 entries by trimming the front (oldest) when it exceeds that limit — prevents unbounded growth on large repos. Store only the first 8 chars of the SHA for compactness: `state.processed_shas.append(sha[:8])`.

`clear_index_state(project_root: Path) -> None`: Delete `.graphiti/index-state.json` if it exists. Used by `--full` flag.

**src/indexer/quality_gate.py** — commit skip filters:

Define constants at module top:
```python
BOT_EMAIL_PATTERNS = [
    r'\[bot\]@users\.noreply\.github\.com$',
    r'@dependabot\.com$',
    r'noreply@github\.com$',
    r'\d+\+[a-z\-]+\[bot\]@',
]
BOT_MESSAGE_PREFIXES = [
    'chore(deps):',
    'chore(deps-dev):',
    'build(deps):',
    'chore(release):',
]
VERSION_FILE_PATTERNS = ['package.json', 'pyproject.toml', '__version__', 'CHANGELOG', 'setup.py', 'setup.cfg']
TINY_COMMIT_THRESHOLD = 3  # lines (insertions + deletions)
```

Implement `should_skip_commit(commit) -> tuple[bool, str]` where commit is a `git.Commit` object:
1. Check author email against BOT_EMAIL_PATTERNS with re.search (case-insensitive). If match: return (True, f"bot_author:{email}")
2. Check commit message against BOT_MESSAGE_PREFIXES (case-insensitive startswith). If match: return (True, f"bot_message:{prefix}")
3. If len(commit.parents) > 1: call `_has_substantive_diff(commit)`. If no substantive diff: return (True, "pure_merge_no_diff")
4. Try to get stats (wrap in try/except — stats calls git subprocess): if total_lines <= TINY_COMMIT_THRESHOLD: return (True, f"tiny:{total_lines}_lines")
5. Get changed_files from commit.stats.files.keys(). If all changed files match VERSION_FILE_PATTERNS: return (True, "version_bump_only")
6. On any exception in steps 4-5: return (False, "") — fail open (process the commit)

`_has_substantive_diff(commit) -> bool`: Try commit.stats.total.get('files', 0) > 0. Return True (has diff) or False (empty merge). Wrap in try/except, return True on exception (conservative: assume substantive).

Import: `import re`, `from git import Commit` (type hint only; do not import at runtime if not needed — use `TYPE_CHECKING` guard).
  </action>
  <verify>
cd /home/tasostilsi/Development/Projects/graphiti-knowledge-graph && python -c "
from src.indexer.state import load_state, save_state, is_within_cooldown, add_processed_sha, clear_index_state, IndexState
from pathlib import Path
import tempfile, os

with tempfile.TemporaryDirectory() as tmp:
    root = Path(tmp)
    (root / '.graphiti').mkdir()

    # Fresh state
    s = load_state(root)
    assert s.last_indexed_sha is None, 'fresh state should have no sha'

    # Save and reload
    s.last_indexed_sha = 'abc12345'
    save_state(root, s)
    s2 = load_state(root)
    assert s2.last_indexed_sha == 'abc12345', f'reload failed: {s2.last_indexed_sha}'

    # SHA tracking
    add_processed_sha(s2, 'deadbeef1234')
    assert 'deadbeef' in s2.processed_shas, 'sha not added'

    # Cooldown (fresh state has no last_run_at, should not be in cooldown)
    assert not is_within_cooldown(root), 'fresh state should not be in cooldown'

    print('OK: state module works correctly')

from src.indexer.quality_gate import should_skip_commit
print('OK: quality_gate imported successfully')
"
  </verify>
  <done>src/indexer/__init__.py, state.py, and quality_gate.py exist. State module correctly loads, saves, and reads back IndexState. Cooldown returns False for fresh state. Quality gate module imports without errors. All assertions in the verification script pass.</done>
</task>

<task type="auto">
  <name>Task 2: Create indexer/indexer.py and indexer/extraction.py</name>
  <files>
    src/indexer/indexer.py
    src/indexer/extraction.py
  </files>
  <action>
Create the main indexer control flow and the two-pass LLM extraction module.

**src/indexer/extraction.py** — two-pass LLM extraction:

Import `asyncio`, `structlog`, `EpisodeType` from graphiti_core.nodes, `datetime`, `timezone`.

Define prompt templates as module-level constants:

```python
STRUCTURED_EXTRACTION_PROMPT = """\
Analyze this git commit and answer concisely:

Commit: {sha_short} by {author}
Message: {message}

Diff:
{diff_content}

Answer these questions:
1. What decision was made?
2. What components or files were changed?
3. Why was this change made (purpose/motivation)?
4. What was the impact or risk?
"""

FREE_FORM_EXTRACTION_PROMPT = """\
Extract all entities and relationships from this git commit as natural language knowledge graph facts.
Focus on: people, components, architectural decisions, bugs fixed, features added, dependencies introduced.

Commit: {sha_short} by {author}
Message: {message}

Diff content:
{diff_content}
"""

DIFF_SUMMARIZATION_PROMPT = """\
Summarize this large git diff concisely for knowledge extraction.
Describe what changed, which components are affected, and the apparent purpose.
Keep the summary under 300 words.

Diff:
{diff_content}
"""

LARGE_DIFF_THRESHOLD_LINES = 300
DIFF_CONTENT_CHAR_LIMIT = 4000  # chars for prompt
```

Implement `async def extract_commit_knowledge(commit_sha: str, commit_message: str, commit_author: str, diff_content: str, graphiti_instance, group_id: str, reference_time: datetime) -> dict`:

1. Check if diff is large: `is_large = diff_content.count('\n') > LARGE_DIFF_THRESHOLD_LINES`
2. If is_large: call `_summarize_diff(diff_content, graphiti_instance)` to get a shorter diff_content for extraction. Log that large diff summarization is occurring.
3. Truncate diff_content to DIFF_CONTENT_CHAR_LIMIT characters if still large
4. Pass 1 — structured extraction: format STRUCTURED_EXTRACTION_PROMPT, call `await graphiti_instance.add_episode(name=f"git-commit-structured-{commit_sha[:7]}", episode_body=structured_text, source_description=f"git-history-index:structured:{commit_sha[:7]}", reference_time=reference_time, source=EpisodeType.text, group_id=group_id)`
5. Pass 2 — free-form extraction: format FREE_FORM_EXTRACTION_PROMPT, call `await graphiti_instance.add_episode(name=f"git-commit-freeform-{commit_sha[:7]}", episode_body=freeform_text, source_description=f"git-history-index:freeform:{commit_sha[:7]}", reference_time=reference_time, source=EpisodeType.text, group_id=group_id)`
6. Return dict: `{"sha": commit_sha[:7], "passes": 2, "was_large": is_large}`
7. Wrap entire function body in try/except Exception; on exception log error and return `{"sha": commit_sha[:7], "passes": 0, "error": str(e)}`

Implement `async def _summarize_diff(diff_content: str, graphiti_instance) -> str`: Use graphiti_instance's underlying LLM client to get a summary. Look at how existing capture code calls the LLM — use `src.llm` module's `chat()` or `generate()` function (read src/llm/__init__.py to find the right call). Pass DIFF_SUMMARIZATION_PROMPT with diff truncated to 8000 chars. Return the response text. On exception, return diff_content[:DIFF_CONTENT_CHAR_LIMIT] (fall back to truncation).

**src/indexer/indexer.py** — main GitIndexer class:

Import: `asyncio`, `git`, `structlog`, `datetime`, `timezone`, `Path`, `rich.console.Console`.
Import from project: `src.indexer.state`, `src.indexer.quality_gate`, `src.indexer.extraction`, `src.graph.service.GraphService`, `src.config` (for scope/paths), `src.capture.git_capture.fetch_commit_diff`.

Read `src/capture/git_capture.py` to understand the `fetch_commit_diff()` signature.
Read `src/graph/service.py` to understand how to get a graphiti instance.

Class `GitIndexer`:

`__init__(self, project_root: Path, scope=None)`:
- Store project_root
- Initialize structlog logger
- console = Console()

`run(self, since: str | None = None, full: bool = False, verbose: bool = False, status_callback=None) -> dict`:
- Check cooldown: if not full and `is_within_cooldown(self.project_root)`: log info "within cooldown, skipping", return `{"commits_processed": 0, "commits_skipped": 0, "entities_created": 0, "elapsed_seconds": 0.0, "skipped_reason": "cooldown"}`
- Load state: `state = load_state(self.project_root)`
- Open git repo: `repo = git.Repo(self.project_root, search_parent_directories=True)`
- Determine iteration kwargs: if since is a valid SHA-like string (40 hex chars), use as cursor; if since looks like a date string, pass as `since` kwarg to iter_commits. If full=True, reset state first via clear_index_state() and reload.
- Determine since_sha cursor for incremental: if not full and not since, use `state.last_indexed_sha` as the cursor (stop when we reach this SHA)
- Iterate commits: for commit in repo.iter_commits(max_count=configured_max if set, since=since_date if date given): stop when commit.hexsha == since_sha cursor. Skip if `is_sha_processed(state, commit.hexsha)` (secondary dedup check).
- For each commit: call `should_skip_commit(commit)` — if skip: log debug with reason, increment skipped count, continue.
- For qualifying commits: get diff via `fetch_commit_diff(commit_sha=commit.hexsha, repo_path=self.project_root)` — wrap in try/except.
- Get commit reference_time: `datetime.fromtimestamp(commit.committed_date, tz=timezone.utc)`
- Get graphiti instance: use `GraphService` to get the graphiti object — check how the Phase 6 capture code does this in `src/capture/git_worker.py` or `src/graph/service.py` and follow the same pattern.
- Call `asyncio.run(extract_commit_knowledge(...))` for each commit (or gather all in async context).
- After each commit: `add_processed_sha(state, commit.hexsha)`, update `state.last_indexed_sha = commit.hexsha`, `state.indexed_commits_count += 1`, call `save_state(self.project_root, state)` for crash recovery.
- Update `state.last_run_at` to ISO UTC timestamp at the end of the run.
- Save final state.
- Return stats dict: `{"commits_processed": N, "commits_skipped": M, "entities_created": K, "elapsed_seconds": T}`

`reset_full(self) -> None`:
- Call `clear_index_state(self.project_root)` to wipe the cursor state
- Delete all Kuzu episodes tagged with `source_description` containing `"git-history-index:"` — this requires a Kuzu query. Use the approach described in RESEARCH.md open question 2: get graphiti/driver, execute `MATCH (e:Episodic) WHERE e.group_id = $group_id AND e.source CONTAINS 'git-history-index' RETURN e.uuid AS uuid`, then delete those UUIDs using `Node.delete_by_uuids()`. If the Cypher query fails (unsupported syntax), log a warning and continue — the SHA state has been cleared so re-indexing will re-add them (duplication is acceptable; graphiti deduplication handles it).

Read `src/capture/git_worker.py` and `src/graph/service.py` to understand the correct way to get a graphiti instance and run async calls consistently with Phase 6 patterns.

Note on async: If the CLI command calls `indexer.run()` synchronously (from a typer command), the run() method should use `asyncio.run()` internally for the async parts. Alternatively, make run() itself async and let the CLI handle it — check how Phase 6 handles this in hooks/capture.
  </action>
  <verify>
cd /home/tasostilsi/Development/Projects/graphiti-knowledge-graph && python -c "
from src.indexer import GitIndexer, IndexState
from src.indexer.extraction import extract_commit_knowledge, LARGE_DIFF_THRESHOLD_LINES
from src.indexer.quality_gate import should_skip_commit
print(f'OK: all imports succeed')
print(f'OK: LARGE_DIFF_THRESHOLD_LINES = {LARGE_DIFF_THRESHOLD_LINES}')
print(f'OK: GitIndexer has run method: {callable(getattr(GitIndexer, \"run\", None))}')
print(f'OK: GitIndexer has reset_full method: {callable(getattr(GitIndexer, \"reset_full\", None))}')
"
  </verify>
  <done>src/indexer/indexer.py and extraction.py exist and import cleanly. GitIndexer has run() and reset_full() methods. extract_commit_knowledge is async. The module imports without errors. LARGE_DIFF_THRESHOLD_LINES = 300. All imports from src.indexer package succeed.</done>
</task>

</tasks>

<verification>
Full indexer core verification:
1. `python -c "from src.indexer import GitIndexer, IndexState; print('OK')"` exits 0
2. `python -c "from src.indexer.quality_gate import should_skip_commit; print('OK')"` exits 0
3. `python -c "from src.indexer.state import is_within_cooldown, save_state, load_state; print('OK')"` exits 0
4. State roundtrip test (see Task 1 verify block) passes
5. GitIndexer class has `run()` and `reset_full()` methods
6. extraction.py defines `extract_commit_knowledge` as an async function
7. `grep -r "git-history-index:" src/indexer/extraction.py` returns lines with both "structured" and "freeform" source_description tags
</verification>

<success_criteria>
The src/indexer/ package exists with all five files. All modules import without errors. State module correctly reads, writes, and atomically saves JSON state. Quality gate correctly classifies commits (bot, tiny, version-bump, merge). Extraction module defines the two-pass async pipeline with correct source_description tags. GitIndexer class orchestrates the full pipeline with incremental cursor and cooldown support.
</success_criteria>

<output>
After completion, create `.planning/phases/7.1-git-indexing-pivot-inserted/7.1-02-SUMMARY.md`
</output>
