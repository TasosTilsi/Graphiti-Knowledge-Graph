---
phase: 06-automatic-capture
plan: 03
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - src/capture/conversation.py
  - src/capture/git_worker.py
  - src/capture/__init__.py
autonomous: true

must_haves:
  truths:
    - "Conversation capture reads Claude Code transcript JSONL and extracts recent turns"
    - "Conversation capture tracks last_captured_turn to avoid re-processing"
    - "Git worker reads pending commits, batches 10, filters for relevance, summarizes, and stores"
    - "Git worker processes through Phase 5 queue infrastructure for retry/dead-letter support"
    - "Security filtering runs before LLM for both git and conversation capture"
  artifacts:
    - path: "src/capture/conversation.py"
      provides: "Conversation transcript extraction and capture"
      contains: "capture_conversation"
    - path: "src/capture/git_worker.py"
      provides: "Git capture worker processing pipeline"
      contains: "process_pending_commits"
    - path: "src/capture/__init__.py"
      provides: "Updated module exports"
      exports: ["capture_conversation", "process_pending_commits"]
  key_links:
    - from: "src/capture/git_worker.py"
      to: "src/capture/git_capture.py"
      via: "reads pending commits and fetches diffs"
      pattern: "from src\\.capture\\.git_capture import"
    - from: "src/capture/git_worker.py"
      to: "src/capture/summarizer.py"
      via: "summarize_and_store for batch processing"
      pattern: "from src\\.capture\\.summarizer import"
    - from: "src/capture/git_worker.py"
      to: "src/capture/relevance.py"
      via: "filter_relevant_commit for pre-filtering"
      pattern: "from src\\.capture\\.relevance import"
    - from: "src/capture/conversation.py"
      to: "src/capture/summarizer.py"
      via: "summarize_and_store for conversation summaries"
      pattern: "from src\\.capture\\.summarizer import"
---

<objective>
Build conversation capture from Claude Code transcripts and the git commit worker processing pipeline. These modules wire the capture pipeline (Plan 01) into end-to-end processing flows for both capture sources.

Purpose: Completes the capture system by adding conversation extraction and the git worker that reads pending commits, batches them, runs relevance + security + LLM summarization, and stores results.
Output: src/capture/conversation.py, src/capture/git_worker.py
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-automatic-capture/06-CONTEXT.md
@.planning/phases/06-automatic-capture/06-RESEARCH.md
@.planning/phases/06-automatic-capture/06-01-SUMMARY.md
@src/capture/__init__.py
@src/capture/git_capture.py
@src/capture/batching.py
@src/capture/relevance.py
@src/capture/summarizer.py
@src/queue/__init__.py
@src/security/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create conversation capture module</name>
  <files>
    src/capture/conversation.py
  </files>
  <action>
**src/capture/conversation.py** - Conversation capture from Claude Code transcripts:

Claude's discretion area: conversation capture observation mechanism and extraction strategy. Based on research, use Claude Code Stop hook approach with transcript_path.

- `METADATA_FILE = Path.home() / ".graphiti" / "capture_metadata.json"`: Stores last_captured_turn per session_id to avoid re-processing.
- `def _load_metadata() -> dict`: Load capture metadata from METADATA_FILE. Returns empty dict if file doesn't exist or is malformed. Use json module.
- `def _save_metadata(metadata: dict) -> None`: Save metadata to METADATA_FILE. Ensure parent directory exists. Write atomically (write to .tmp, rename).
- `def _get_last_captured_turn(session_id: str) -> int`: Get the last captured turn index for a session. Returns 0 if not tracked.
- `def _set_last_captured_turn(session_id: str, turn_index: int) -> None`: Update the last captured turn for a session.
- `def read_transcript(transcript_path: Path, since_turn: int = 0) -> list[dict]`: Read Claude Code JSONL transcript file. Each line is a JSON object representing a conversation turn. Parse lines, skip turns with index <= since_turn. Return list of turn dicts. Handle FileNotFoundError gracefully (log warning, return empty list). Handle malformed JSON lines by skipping them with a warning log.
- `def extract_conversation_text(turns: list[dict]) -> str`: Extract meaningful text content from transcript turns. For each turn, extract the "content" or "message" field (adapt to actual transcript format). Join turns into a single text block with turn separators (`\n---\nTurn {i}:\n`). This is the raw text that gets sent to summarization.
- `async def capture_conversation(transcript_path: Path, session_id: str, auto: bool = False) -> dict | None`: Main conversation capture function:
  1. If auto mode: check last_captured_turn for this session_id, only process new turns
  2. Read transcript from transcript_path using read_transcript(since_turn=last_turn)
  3. If no new turns, log debug and return None
  4. Extract text from turns via extract_conversation_text()
  5. Call `summarize_and_store()` from src.capture.summarizer with source="conversation", tags=["auto-capture", "conversation", session_id[:8]]
  6. Update last_captured_turn metadata
  7. Return the stored entity dict or None
  If not auto mode (manual `graphiti capture`): read ALL turns (since_turn=0), ignore metadata tracking, process full transcript.
- `async def capture_manual(transcript_path: Path | None = None) -> dict | None`: Manual capture entry point for `graphiti capture` CLI. If transcript_path not provided, attempt to find it from Claude Code environment (check `CLAUDE_TRANSCRIPT_PATH` env var, or common locations). If no transcript found, raise ValueError with helpful message. Calls `capture_conversation()` with auto=False.

Use structlog for logging. Import `summarize_and_store` from `src.capture.summarizer`.
  </action>
  <verify>
Run: `python -c "from src.capture.conversation import read_transcript, extract_conversation_text, capture_conversation, capture_manual; print('All imports OK')"` -- imports work.
Run: `python -c "from src.capture.conversation import _load_metadata, _save_metadata; m = _load_metadata(); assert isinstance(m, dict); print('Metadata OK')"` -- metadata operations work.
Run: `python -c "from src.capture.conversation import read_transcript; from pathlib import Path; turns = read_transcript(Path('/nonexistent/path')); assert turns == []; print('Graceful handling OK')"` -- handles missing file.
  </verify>
  <done>
conversation.py reads Claude Code JSONL transcripts, tracks last_captured_turn per session to avoid re-processing, extracts text from turns, and calls summarize_and_store(). Supports both auto mode (incremental, from Stop hook) and manual mode (full transcript, from CLI). Metadata persisted to ~/.graphiti/capture_metadata.json.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create git worker processing pipeline and update exports</name>
  <files>
    src/capture/git_worker.py
    src/capture/__init__.py
  </files>
  <action>
**src/capture/git_worker.py** - Git capture worker pipeline:

This module provides the end-to-end processing function that the background worker calls to process pending git commits.

- `DEFAULT_BATCH_SIZE = 10`: Per locked user decision.
- `DEFAULT_MAX_LINES_PER_FILE = 500`: Per research recommendation (Claude's discretion area).
- `async def process_pending_commits(pending_file: Path | None = None, batch_size: int = DEFAULT_BATCH_SIZE, max_lines_per_file: int = DEFAULT_MAX_LINES_PER_FILE, scope: GraphScope | None = None, project_root: Path | None = None) -> list[dict]`: Main git capture processing function:
  1. Call `read_and_clear_pending_commits(pending_file)` to atomically get pending commit hashes
  2. If no pending commits, return empty list
  3. Log the number of pending commits found
  4. For each commit hash, call `fetch_commit_diff(sha, max_lines_per_file=max_lines_per_file)` to get the full diff with truncation
  5. Pre-filter: extract commit message from each diff (first line after "Date:" in git show output, or parse the "Subject:" line). Run `filter_relevant_commit(commit_message)` on each. Skip irrelevant commits (log as skipped). This respects the locked decision on relevance filtering.
  6. If no relevant commits remain after filtering, return empty list
  7. Batch the relevant diffs using BatchAccumulator(batch_size). For each full batch returned by accumulator.add():
     - Call `await summarize_and_store(batch_diffs, source="git-commits", scope=scope, project_root=project_root, tags=["auto-capture", "git-commits"])`
     - Collect results
  8. After all commits processed, flush any partial batch via accumulator.flush(). If non-empty, summarize_and_store the partial batch too.
  9. Return list of stored entity dicts (filtering out None results)

- `def enqueue_git_processing(pending_file: Path | None = None) -> str | None`: Enqueue git processing as a background job via Phase 5 queue. Creates a job with type="capture_git_commits" and payload containing pending_file path. Calls `src.queue.enqueue()` with parallel=False (sequential -- only one git processor should run at a time). Returns job_id or None if no pending commits exist.

- `def _extract_commit_message(diff_output: str) -> str`: Extract commit message from git show output. Parse the output to find the commit subject line (typically after the "Date:" line, blank line, then indented message). Return first line of the message. Handle edge cases (empty output, no message).

Import from: `src.capture.git_capture` (read_and_clear_pending_commits, fetch_commit_diff), `src.capture.batching` (BatchAccumulator), `src.capture.relevance` (filter_relevant_commit), `src.capture.summarizer` (summarize_and_store), `src.queue` (enqueue), `src.models` (GraphScope). Use structlog for logging.

**Update src/capture/__init__.py** to add new exports:
- Add: `capture_conversation`, `capture_manual`, `process_pending_commits`, `enqueue_git_processing`
  </action>
  <verify>
Run: `python -c "from src.capture.git_worker import process_pending_commits, enqueue_git_processing, DEFAULT_BATCH_SIZE, DEFAULT_MAX_LINES_PER_FILE; assert DEFAULT_BATCH_SIZE == 10; assert DEFAULT_MAX_LINES_PER_FILE == 500; print('Git worker OK')"` -- module loads with correct defaults.
Run: `python -c "from src.capture.git_worker import _extract_commit_message; msg = _extract_commit_message('commit abc\\nAuthor: Test\\nDate: now\\n\\n    feat: add new feature\\n'); assert 'feat' in msg; print('Message extraction OK')"` -- commit message extraction works.
Run: `python -c "from src.capture import capture_conversation, process_pending_commits, enqueue_git_processing; print('All exports OK')"` -- package exports work.
  </verify>
  <done>
git_worker.py provides process_pending_commits() that reads pending commits atomically, fetches diffs with 500-line truncation, pre-filters for relevance, batches into groups of 10, summarizes via LLM with security filtering, and stores results. enqueue_git_processing() queues the work through Phase 5 infrastructure. conversation.py and git_worker.py exports added to __init__.py.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.capture import capture_conversation, capture_manual, process_pending_commits, enqueue_git_processing; print('All Phase 6 Plan 03 exports OK')"` -- all new exports work
2. `python -c "from src.capture.conversation import METADATA_FILE; print(METADATA_FILE)"` -- metadata file path is under ~/.graphiti/
3. `python -c "from src.capture.git_worker import DEFAULT_BATCH_SIZE; assert DEFAULT_BATCH_SIZE == 10"` -- batch size matches locked decision
4. `python -c "from src.capture.git_worker import DEFAULT_MAX_LINES_PER_FILE; assert DEFAULT_MAX_LINES_PER_FILE == 500"` -- truncation limit set
</verification>

<success_criteria>
- src/capture/conversation.py reads JSONL transcripts and tracks last_captured_turn per session
- Conversation capture supports both auto (incremental) and manual (full) modes
- src/capture/git_worker.py processes pending commits end-to-end: read -> filter -> batch -> summarize -> store
- Default batch size is 10 (locked decision)
- Default truncation is 500 lines per file (Claude's discretion)
- Relevance filtering runs before batching (skip irrelevant commits early)
- Processing integrates with Phase 5 queue via enqueue_git_processing()
- All new functions exported from src/capture/__init__.py
</success_criteria>

<output>
After completion, create `.planning/phases/06-automatic-capture/06-03-SUMMARY.md`
</output>
