---
phase: 08-mcp-server
plan: 04
type: execute
wave: 3
depends_on:
  - 08-03
files_modified: []
autonomous: false

requirements:
  - R6.1
  - R6.2
  - R6.3

must_haves:
  truths:
    - "MCP tools are callable from Claude Code with both stdio and HTTP transports"
    - "Relevant context is injected on session start from local Kuzu DB"
    - "Context injection completes in <100ms p95"
    - "Context respects 8K token budget"
    - "Conversations can be captured non-blockingly via graphiti_capture"
    - "Tool errors propagate clearly with actionable messages"
    - "graphiti mcp install sets up everything without manual JSON editing"
  artifacts:
    - path: "src/mcp_server/server.py"
      provides: "Running FastMCP server accepting stdio connections"
    - path: "src/mcp_server/install.py"
      provides: "Working install that produces callable server"
    - path: "~/.claude/skills/graphiti/SKILL.md"
      provides: "SKILL.md teaching Claude Code graphiti behaviors"
  key_links:
    - from: "Claude Code"
      to: "src/mcp_server/server.py"
      via: "stdio transport (graphiti mcp serve)"
      pattern: "graphiti mcp serve"
    - from: "graphiti_capture in Claude Code"
      to: "background process"
      via: "non-blocking Popen"
      pattern: "Capture started in background"
---

<objective>
Human verification that the Phase 8 MCP server works end-to-end from Claude Code. This checkpoint confirms all five success criteria from the phase goal before marking Phase 8 complete.

Purpose: Automated tests cannot verify that tools actually appear in Claude Code's tool panel, that context injection happens on session start, or that the UX flows correctly. Human eyes are required.

Output: Confirmed working MCP server in Claude Code, or specific issues for gap closure.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/08-mcp-server/08-CONTEXT.md
@.planning/phases/08-mcp-server/08-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run automated integration smoke tests</name>
  <files></files>
  <action>
Before the human checkpoint, run these automated smoke tests and report results:

**Test 1 — Server starts without error (stdio transport):**
```bash
timeout 3 python -c "
import subprocess, sys, json
proc = subprocess.Popen(
    ['graphiti', 'mcp', 'serve'],
    stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE
)
# Send MCP initialize request
init_msg = json.dumps({'jsonrpc':'2.0','id':1,'method':'initialize','params':{'protocolVersion':'2024-11-05','capabilities':{},'clientInfo':{'name':'test','version':'0.0.1'}}})
content = f'Content-Length: {len(init_msg)}\r\n\r\n{init_msg}'
proc.stdin.write(content.encode())
proc.stdin.flush()
import time; time.sleep(1)
proc.terminate()
out = proc.stdout.read().decode(errors='replace')
err = proc.stderr.read().decode(errors='replace')
assert 'error' not in out.lower() or 'initialize' in out, f'unexpected error: {out[:200]}'
print('server start OK')
print('stdout:', out[:200] if out else '(empty)')
print('stderr:', err[:200] if err else '(empty)')
" 2>&1 || echo "server test timed out (expected for stdio)"
```

**Test 2 — Context resource returns a string (not error):**
```bash
python -c "
from src.mcp_server.context import get_context
import time
start = time.time()
result = get_context()
elapsed = (time.time() - start) * 1000
assert isinstance(result, str), f'Expected str, got {type(result)}'
print(f'Context returned {len(result)} chars in {elapsed:.0f}ms')
assert elapsed < 5000, f'Context took {elapsed:.0f}ms — stale detection may be blocking'
"
```

**Test 3 — graphiti_capture is non-blocking:**
```bash
python -c "
import time
from src.mcp_server.tools import graphiti_capture
start = time.time()
try:
    result = graphiti_capture()
    elapsed = (time.time() - start) * 1000
    print(f'graphiti_capture returned in {elapsed:.0f}ms: {result}')
    assert elapsed < 2000, f'capture blocked for {elapsed:.0f}ms (should be <2s)'
    print('non-blocking OK')
except RuntimeError as e:
    elapsed = (time.time() - start) * 1000
    print(f'graphiti_capture raised RuntimeError in {elapsed:.0f}ms: {e}')
    # RuntimeError is OK if graphiti CLI not found; what matters is it returned fast
    assert elapsed < 2000, f'even on error, should return fast: {elapsed:.0f}ms'
    print('fast error return OK')
"
```

**Test 4 — Tool error propagation:**
```bash
python -c "
from src.mcp_server.tools import graphiti_show
try:
    graphiti_show('__nonexistent_entity_xyz_test__')
    print('show returned without error (entity may have been found by partial match)')
except RuntimeError as e:
    print(f'RuntimeError raised (expected): {str(e)[:100]}')
    assert str(e).strip(), 'Error message must not be empty'
    print('error propagation OK')
"
```

**Test 5 — SKILL.md has all 4 behavior patterns:**
```bash
python -c "
from pathlib import Path
skill_path = Path.home() / '.claude' / 'skills' / 'graphiti' / 'SKILL.md'
assert skill_path.exists(), f'SKILL.md not found at {skill_path}'
content = skill_path.read_text()
required = ['Session start', 'After key moments', 'Topic surfacing', 'Explicit requests']
for r in required:
    assert r in content, f'Missing behavior: {r}'
print('SKILL.md has all 4 behaviors OK')
print(f'SKILL.md size: {len(content)} chars')
"
```

Print a summary of pass/fail for each test. If any test fails, report the error clearly.
  </action>
  <verify>All 5 automated tests run and results reported. Failures clearly identified with error messages.</verify>
  <done>Automated smoke test results reported. Any failures documented for human review.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify MCP server works end-to-end in Claude Code</name>
  <what-built>
Phase 8 MCP server — complete implementation including:
1. SKILL.md at ~/.claude/skills/graphiti/SKILL.md (teaches Claude Code graphiti behaviors)
2. 10 MCP tools (graphiti_add, graphiti_search, graphiti_list, graphiti_show, graphiti_delete, graphiti_summarize, graphiti_compact, graphiti_capture, graphiti_health, graphiti_config)
3. graphiti://context resource for session-start context injection
4. Both stdio and streamable-http transports
5. graphiti mcp install command for zero-config setup
  </what-built>
  <how-to-verify>
**Step 1: Install and restart**

```bash
graphiti mcp install
```

Expected output: "MCP server registered in ~/.claude.json" and "SKILL.md installed to ..."

Restart Claude Code (close and reopen, or run `/restart`).

**Step 2: Verify tools appear**

In a new Claude Code session, type: "What graphiti tools do you have available?"

Expected: Claude lists the graphiti_* tools (graphiti_add, graphiti_search, etc.)

**Step 3: Test context injection**

In the same session, observe whether Claude appears to have knowledge of the project
from the knowledge graph (if it has been indexed). If the graph is empty, context
injection is silent — this is correct behavior.

Alternatively, add a test entity first:
```bash
graphiti add "Test decision: prefer subprocess wrappers over direct library calls"
```
Then restart Claude Code and check if it mentions this knowledge.

**Step 4: Test tool invocation**

Ask Claude: "Search the graphiti knowledge graph for 'architecture decisions'"

Expected: Claude calls graphiti_search, receives TOON response, and presents results
as natural prose (not raw TOON format).

**Step 5: Test non-blocking capture**

Ask Claude: "Capture this conversation to the knowledge graph"

Expected: Claude calls graphiti_capture and reports "Capture started in background"
almost immediately (within 2 seconds).

**Step 6: Test HTTP transport (optional)**

```bash
graphiti mcp serve --transport streamable-http &
sleep 2
curl -s http://localhost:8000/ | head -20  # or check for MCP endpoint
kill %1
```

Expected: Server starts and accepts connections on port 8000.

**Report any of these issues:**
- Tools don't appear in Claude Code after restart
- Context injection causes visible delay (>1 second on session start)
- TOON format is shown directly to user (should be translated to prose)
- graphiti_capture blocks for more than 2 seconds
- Any tool returns an unhelpful error message
  </how-to-verify>
  <resume-signal>
Type "approved" if all checks pass, or describe specific issues found (e.g., "tools not appearing", "context too slow", "TOON shown to user").
  </resume-signal>
  <action>Human verification — follow how-to-verify steps above.</action>
  <verify>Human confirms all verification steps pass (or documents issues).</verify>
  <done>Human types "approved" or describes issues for gap closure.</done>
</task>

</tasks>

<verification>
Phase 8 complete when:
1. Automated smoke tests all pass (Task 1)
2. Human confirms tools appear in Claude Code (Step 2)
3. Human confirms context injection works (Step 3 or silent pass)
4. Human confirms tool responses shown as natural prose (Step 4)
5. Human confirms capture is non-blocking (Step 5)
</verification>

<success_criteria>
All five Phase 8 success criteria verified:
1. MCP tools callable from Claude Code with both stdio and HTTP transports
2. Relevant context injected on session start based on current file and commits (<100ms p95)
3. Context injection respects token budget (under 8K tokens)
4. Conversations captured automatically via MCP hooks without blocking
5. Tool errors propagate clearly with actionable messages
</success_criteria>

<output>
After completion, create `.planning/phases/08-mcp-server/08-04-SUMMARY.md` with:
- Automated test results (pass/fail for each)
- Human verification outcome
- Any issues found and whether they were resolved or deferred
- Phase 8 completion status
</output>
