---
phase: 08-mcp-server
plan: 03
type: execute
wave: 2
depends_on:
  - 08-01
  - 08-02
files_modified:
  - src/mcp_server/context.py
  - src/mcp_server/server.py
  - src/mcp_server/install.py
  - src/cli/commands/mcp.py
  - src/cli/__init__.py
autonomous: true
requirements:
  - R6.1
  - R6.2
  - R6.3

must_haves:
  truths:
    - "graphiti mcp serve --transport stdio starts the FastMCP server on stdio"
    - "graphiti mcp serve --transport streamable-http starts HTTP server on port 8000"
    - "graphiti://context resource returns TOON-encoded decisions+architecture context under 8K tokens"
    - "graphiti mcp install writes correct stdio entry to ~/.claude.json without destroying existing config"
    - "graphiti mcp install also writes SKILL.md to ~/.claude/skills/graphiti/SKILL.md"
    - "Stale index triggers background re-index (non-blocking) — context injection returns immediately"
    - "Empty graph returns empty string from context resource (no error, no prompt to fill)"
  artifacts:
    - path: "src/mcp_server/context.py"
      provides: "graphiti://context resource with stale detection and TOON encoding"
      min_lines: 60
      exports: ["get_context"]
    - path: "src/mcp_server/server.py"
      provides: "FastMCP app with 10 tools + context resource registered, both transport modes"
      min_lines: 80
      exports: ["mcp", "main"]
    - path: "src/mcp_server/install.py"
      provides: "graphiti mcp install logic: writes ~/.claude.json and installs SKILL.md"
      min_lines: 60
      exports: ["install_mcp_server", "SKILL_MD_CONTENT"]
    - path: "src/cli/commands/mcp.py"
      provides: "graphiti mcp Typer command group with serve and install subcommands"
      min_lines: 40
      exports: ["mcp_app"]
    - path: "src/cli/__init__.py"
      provides: "mcp_app registered with app.add_typer"
      contains: "mcp_app"
  key_links:
    - from: "src/mcp_server/server.py"
      to: "src/mcp_server/tools.py"
      via: "@mcp.tool() decorating each graphiti_* function"
      pattern: "@mcp\\.tool\\(\\)"
    - from: "src/mcp_server/server.py"
      to: "src/mcp_server/context.py"
      via: "@mcp.resource('graphiti://context') decorating get_context"
      pattern: "@mcp\\.resource.*graphiti://context"
    - from: "src/mcp_server/context.py"
      to: "graphiti CLI"
      via: "subprocess.run(['graphiti', 'search', ...]) for context queries"
      pattern: "subprocess\\.run.*graphiti.*search"
    - from: "src/mcp_server/install.py"
      to: "~/.claude.json"
      via: "json.load/dump to merge mcpServers entry"
      pattern: "claude_json_path"
    - from: "src/cli/__init__.py"
      to: "src/cli/commands/mcp.py"
      via: "app.add_typer(mcp_app, name='mcp')"
      pattern: "add_typer.*mcp_app"
---

<objective>
Wire the MCP server together: implement the context resource, register all tools in the FastMCP server, create the CLI `mcp` command group, and implement `graphiti mcp install` for zero-config setup. This plan also contains the SKILL.md content embedded in install.py as a constant.

Purpose: Plans 01 and 02 created the building blocks (TOON utilities and 10 tool functions). This plan connects them into a running server and makes it discoverable via the existing CLI.

Output:
- server.py: FastMCP app with all tools registered + graphiti://context resource
- context.py: context resource with stale detection, TOON encoding, token budget enforcement
- install.py: zero-config installer (writes ~/.claude.json + SKILL.md)
- mcp.py CLI command group: graphiti mcp serve, graphiti mcp install
- cli/__init__.py updated to register mcp command group
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/08-mcp-server/08-CONTEXT.md
@.planning/phases/08-mcp-server/08-RESEARCH.md
@.planning/phases/08-mcp-server/08-01-SUMMARY.md
@.planning/phases/08-mcp-server/08-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement context resource and FastMCP server</name>
  <files>
    src/mcp_server/context.py
    src/mcp_server/server.py
  </files>
  <action>
**CRITICAL: Both files must direct ALL output to stderr. Never write to stdout — stdio transport uses it for JSON-RPC messages. Use `logging.basicConfig(stream=sys.stderr)` at the top of each module. Never import Rich console or structlog in these modules.**

---

**src/mcp_server/context.py** — implement the `get_context()` function that will be registered as `@mcp.resource("graphiti://context")` in server.py:

```python
import json
import logging
import os
import subprocess
import sys
from pathlib import Path

logging.basicConfig(stream=sys.stderr, level=logging.WARNING)
logger = logging.getLogger(__name__)

def _get_project_root() -> str | None:
    """Get project root at request time (not server start).

    Checks GRAPHITI_PROJECT_ROOT env var first (set by Claude Code per-project),
    then falls back to CWD. Returns None if not in a git project.
    """
    return os.environ.get("GRAPHITI_PROJECT_ROOT") or None


def _is_index_stale(project_root: str | None) -> bool:
    """Check if git index is stale in <10ms.

    Compares current git HEAD SHA against the last_indexed_sha stored in
    .graphiti/index-state.json. Returns False (not stale) if project_root
    is None or any error occurs.
    """
    if not project_root:
        return False
    try:
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            capture_output=True, text=True, timeout=5, cwd=project_root
        )
        if result.returncode != 0:
            return False
        current_sha = result.stdout.strip()[:8]

        state_file = Path(project_root) / ".graphiti" / "index-state.json"
        if not state_file.exists():
            return True  # No index at all — stale
        data = json.loads(state_file.read_text())
        last_sha = (data.get("last_indexed_sha") or "")[:8]
        return current_sha != last_sha
    except Exception as e:
        logger.debug("Stale check failed (non-blocking): %s", e)
        return False  # Fail safe: don't trigger re-index on error


def _trigger_background_reindex(project_root: str | None) -> None:
    """Start a background re-index without blocking.

    Uses Popen with DEVNULL so it does not inherit the MCP server's stdio.
    Returns immediately — context injection is NOT blocked by re-indexing.
    """
    if not project_root:
        return
    try:
        subprocess.Popen(
            ["graphiti", "index"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True,
            cwd=project_root,
        )
        logger.info("Background re-index triggered for %s", project_root)
    except Exception as e:
        logger.debug("Background re-index failed to start: %s", e)


def _get_token_budget() -> int:
    """Read mcp.context_tokens from config. Default 8192."""
    try:
        result = subprocess.run(
            ["graphiti", "config", "get", "mcp.context_tokens"],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0 and result.stdout.strip().isdigit():
            return int(result.stdout.strip())
    except Exception:
        pass
    return 8192


def get_context() -> str:
    """Session-start context from the local knowledge graph.

    Called by FastMCP when Claude Code accesses graphiti://context.
    Returns TOON-encoded decisions + architecture context, or empty string
    if graph is empty (silent — do not prompt user to fill the graph).

    Latency target: <100ms p95. Stale detection is fast (<10ms).
    Re-indexing is always non-blocking (background subprocess).
    """
    from src.mcp_server.toon_utils import encode_response, trim_to_token_budget

    project_root = _get_project_root()

    # Check staleness in <10ms and trigger background re-index if needed.
    # Do NOT wait for re-index — return current (possibly stale) context immediately.
    if _is_index_stale(project_root):
        _trigger_background_reindex(project_root)

    # Query for high-priority context: decisions + architecture first
    try:
        result = subprocess.run(
            ["graphiti", "search", "decisions architecture patterns", "--limit", "20", "--format", "json"],
            capture_output=True, text=True, timeout=30,
            cwd=project_root,
        )
    except subprocess.TimeoutExpired:
        logger.warning("Context query timed out")
        return ""
    except Exception as e:
        logger.warning("Context query failed: %s", e)
        return ""

    if result.returncode != 0 or not result.stdout.strip():
        return ""  # Empty graph: silent, inject nothing

    try:
        data = json.loads(result.stdout)
    except json.JSONDecodeError:
        return ""

    if not data:
        return ""

    # Encode as TOON and trim to token budget
    encoded = encode_response(data)
    budget = _get_token_budget()
    return trim_to_token_budget(encoded, budget)
```

---

**src/mcp_server/server.py** — the FastMCP app with all tools and the context resource registered:

```python
"""FastMCP server for graphiti knowledge graph.

Entry point: `graphiti mcp serve` (via CLI command group in src/cli/commands/mcp.py)

Transports:
  stdio          — default, Claude Code manages lifecycle
  streamable-http — standalone server on localhost:8000

All tool responses use TOON format for arrays, JSON/plain text for scalars.
Context resource returns TOON-encoded decisions + architecture context.
"""
import logging
import sys

logging.basicConfig(stream=sys.stderr, level=logging.WARNING)

from mcp.server.fastmcp import FastMCP

from src.mcp_server.tools import (
    graphiti_add,
    graphiti_search,
    graphiti_list,
    graphiti_show,
    graphiti_delete,
    graphiti_summarize,
    graphiti_compact,
    graphiti_capture,
    graphiti_health,
    graphiti_config,
)
from src.mcp_server.context import get_context

# Create the FastMCP server with instructions for Claude
mcp = FastMCP(
    "graphiti",
    instructions=(
        "graphiti is this user's personal knowledge graph for coding projects. "
        "Tool responses use TOON format (a compact wire encoding) — always present "
        "results as natural human-readable prose, never show TOON to the user. "
        "Use --limit flags to self-manage token budgets. "
        "Capture important decisions with graphiti_add after architecture discussions."
    )
)

# Register all 10 CLI tools with graphiti_ prefix
mcp.tool()(graphiti_add)
mcp.tool()(graphiti_search)
mcp.tool()(graphiti_list)
mcp.tool()(graphiti_show)
mcp.tool()(graphiti_delete)
mcp.tool()(graphiti_summarize)
mcp.tool()(graphiti_compact)
mcp.tool()(graphiti_capture)
mcp.tool()(graphiti_health)
mcp.tool()(graphiti_config)

# Register context resource for session-start injection
mcp.resource("graphiti://context")(get_context)


def main(transport: str = "stdio", port: int = 8000) -> None:
    """Start the MCP server with the specified transport.

    Args:
        transport: "stdio" (default) or "streamable-http"
        port: HTTP port for streamable-http transport (default 8000)
    """
    if transport == "streamable-http":
        mcp.run(transport="streamable-http", port=port)
    else:
        mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
```

Verify server.py: Run `python -c "from src.mcp_server.server import mcp, main; print(type(mcp).__name__)"` — must print "FastMCP".
  </action>
  <verify>
Run: `python -c "from src.mcp_server.server import mcp, main; print('server OK')"` — must print "server OK".

Run: `python -c "from src.mcp_server.context import get_context; print('context OK')"` — must print "context OK".

Run: `python -c "from src.mcp_server.server import mcp; tools = [t.name for t in mcp._tool_manager.list_tools()]; print(sorted(tools))"` — must list all 10 graphiti_* tools.

Verify no stdout: `python -c "import io, sys; sys.stdout = io.StringIO(); from src.mcp_server import server; captured = sys.stdout.getvalue(); sys.stdout = sys.__stdout__; assert not captured, f'stdout leaked: {captured[:100]}'; print('no stdout contamination OK')"` — must print "no stdout contamination OK".
  </verify>
  <done>server.py and context.py exist. FastMCP server has all 10 tools and graphiti://context resource registered. No stdout writes on import. Context resource returns empty string for empty graph, TOON-trimmed content for non-empty graph. Stale detection triggers non-blocking background re-index.</done>
</task>

<task type="auto">
  <name>Task 2: Implement mcp install command, SKILL.md, and CLI registration</name>
  <files>
    src/mcp_server/install.py
    src/cli/commands/mcp.py
    src/cli/__init__.py
  </files>
  <action>
**src/mcp_server/install.py** — install logic including the SKILL.md content as a constant:

```python
"""graphiti mcp install — zero-config MCP server registration for Claude Code.

Writes the stdio server entry to ~/.claude.json so Claude Code auto-starts
the graphiti MCP server. Also installs SKILL.md to ~/.claude/skills/graphiti/
to teach Claude how to use graphiti autonomously.
"""
import json
import shutil
import sys
from pathlib import Path

# SKILL.md content — embedded here so it can be installed without external files.
# Written to ~/.claude/skills/graphiti/SKILL.md (user-scope: all projects).
SKILL_MD_CONTENT = """\
---
name: graphiti
description: >
  graphiti is this user's personal knowledge graph for coding projects.
  Use when: starting a new session (inject context), after key decisions or
  architecture discussions (capture knowledge), when user mentions a specific
  file or topic (surface relevant knowledge), or when explicitly asked about
  past context or preferences. Never show TOON format or raw CLI output to
  the user — always present results as natural human-readable prose.
---

## What graphiti does

graphiti stores and retrieves project knowledge: decisions made, architecture
patterns chosen, bug fixes, library preferences, and workflow discoveries.
It learns from conversations and git commit history.

## When to act (automatic behaviors)

**Session start**: Call `graphiti_search` with query "decisions architecture recent"
to load relevant context. If results exist, weave them into your understanding
silently — do not announce "I queried graphiti." Only mention findings if
they change how you approach the user's question.

**After key moments**: After architecture decisions, important bug resolutions,
library choices, or stated preferences, call `graphiti_add` with a concise
summary. Keep the entry focused on the "why", not the "what". Example:
"Decided to use Kuzu instead of SQLite for graph storage because of native
graph query support and embedded deployment model."

**Topic surfacing**: When the user mentions a file, feature, or concept you
haven't encountered in this session, call `graphiti_search` proactively.
Only surface findings if they are actually relevant to the current task.

**Explicit requests**: When asked "what do you know about X", "check your
memory", or "do you remember when we decided Y" — always use graphiti.

## How to present results

TOON format is an internal wire encoding — never show it to the user. Always
translate tool results into natural conversational prose:

- BAD: "Here are the TOON results: [3,]{id,name}:\\n1,auth-decision..."
- GOOD: "From your knowledge graph: the auth module uses JWT with refresh
  token rotation, a decision made to avoid session storage complexity."

## Tool reference

| Tool | Purpose | When to use |
|------|---------|-------------|
| `graphiti_search(query, limit=10)` | Semantic search | Session start, topic surfacing |
| `graphiti_add(content, tags="")` | Store knowledge | After decisions, bug fixes |
| `graphiti_list(limit=15)` | List all items | Overview, browsing |
| `graphiti_show(name_or_id)` | Show one item | Detail view |
| `graphiti_delete(name_or_id)` | Remove an item | Cleanup |
| `graphiti_summarize()` | Graph summary | Status overview |
| `graphiti_compact()` | Deduplicate | Maintenance |
| `graphiti_capture()` | Capture conversation | Non-blocking, use sparingly |
| `graphiti_health()` | System check | Debugging |
| `graphiti_config(key, value="")` | Get/set config | Configuration |

## Token self-management

Always use `--limit` flags. Default (10-15 items) is fine for most searches.
For broad exploratory searches, use `--limit 5`. Never fetch more than 20 items.
The `graphiti://context` resource at session start already respects the 8K
token budget — you do not need to manually limit it.

## Scope

graphiti supports two scopes:
- **project** (default in git repos): knowledge specific to this project
- **global**: preferences and patterns that apply across all projects

Use `scope="global"` for personal preferences; `scope="project"` or default
for project-specific decisions.
"""


def _find_graphiti_executable() -> str:
    """Find the graphiti executable path."""
    path = shutil.which("graphiti")
    if path:
        return path
    # Fallback: invoke as a Python module
    return f"{sys.executable} -m src.cli"


def install_mcp_server(force: bool = False) -> dict:
    """Write graphiti MCP server config to ~/.claude.json and install SKILL.md.

    This enables zero-config Claude Code integration:
    1. Adds 'graphiti' entry to mcpServers in ~/.claude.json
    2. Writes SKILL.md to ~/.claude/skills/graphiti/SKILL.md

    Args:
        force: Overwrite existing entries even if already present

    Returns:
        Dict with 'claude_json_updated' and 'skill_md_installed' boolean results
    """
    results = {"claude_json_updated": False, "skill_md_installed": False}
    graphiti_path = _find_graphiti_executable()

    # --- 1. Write to ~/.claude.json ---
    claude_json_path = Path.home() / ".claude.json"
    config = {}
    if claude_json_path.exists():
        try:
            config = json.loads(claude_json_path.read_text())
        except (json.JSONDecodeError, IOError):
            config = {}  # Treat as empty if malformed

    if "mcpServers" not in config:
        config["mcpServers"] = {}

    already_present = "graphiti" in config["mcpServers"]
    if not already_present or force:
        config["mcpServers"]["graphiti"] = {
            "type": "stdio",
            "command": graphiti_path,
            "args": ["mcp", "serve"],
            "env": {}
        }
        claude_json_path.write_text(json.dumps(config, indent=2))
        results["claude_json_updated"] = True

    # --- 2. Install SKILL.md ---
    skill_dir = Path.home() / ".claude" / "skills" / "graphiti"
    skill_path = skill_dir / "SKILL.md"

    if not skill_path.exists() or force:
        skill_dir.mkdir(parents=True, exist_ok=True)
        skill_path.write_text(SKILL_MD_CONTENT)
        results["skill_md_installed"] = True

    return results
```

---

**src/cli/commands/mcp.py** — Typer command group following the same pattern as hooks.py and queue_cmd.py:

```python
"""MCP server command group for Graphiti CLI.

Provides graphiti mcp serve and graphiti mcp install subcommands.
"""
import typer
from typing import Annotated, Optional

from src.cli.output import console, print_success, print_json, print_error

mcp_app = typer.Typer(
    name="mcp",
    help="MCP server for Claude Code integration",
    no_args_is_help=True,
)


@mcp_app.command(name="serve")
def serve_command(
    transport: Annotated[str, typer.Option("--transport", "-t",
        help="Transport type: stdio or streamable-http")] = "stdio",
    port: Annotated[int, typer.Option("--port", "-p",
        help="HTTP port (streamable-http only)")] = 8000,
):
    """Start the graphiti MCP server.

    stdio transport (default): Claude Code spawns and manages the process.
    streamable-http: Standalone server on localhost:<port>.

    Examples:
        graphiti mcp serve                         # stdio (Claude Code managed)
        graphiti mcp serve --transport streamable-http --port 8080
    """
    from src.mcp_server.server import main as run_server
    run_server(transport=transport, port=port)


@mcp_app.command(name="install")
def install_command(
    force: Annotated[bool, typer.Option("--force", help="Overwrite existing entries")] = False,
    format: Annotated[Optional[str], typer.Option("--format", "-f",
        help="Output format: json")] = None,
):
    """Install graphiti MCP server for Claude Code (zero-config setup).

    Writes the server configuration to ~/.claude.json and installs SKILL.md
    to ~/.claude/skills/graphiti/SKILL.md.

    After running this command, restart Claude Code to activate the server.

    Examples:
        graphiti mcp install         # Install (skip if already present)
        graphiti mcp install --force # Overwrite existing entries
    """
    try:
        from src.mcp_server.install import install_mcp_server
        results = install_mcp_server(force=force)

        if format == "json":
            print_json(results)
            return

        if results["claude_json_updated"]:
            print_success("MCP server registered in ~/.claude.json")
        else:
            console.print("[dim]MCP server already registered in ~/.claude.json (use --force to update)[/dim]")

        if results["skill_md_installed"]:
            from pathlib import Path
            skill_path = Path.home() / ".claude" / "skills" / "graphiti" / "SKILL.md"
            print_success(f"SKILL.md installed to {skill_path}")
        else:
            console.print("[dim]SKILL.md already installed (use --force to update)[/dim]")

        if results["claude_json_updated"] or results["skill_md_installed"]:
            console.print("\n[cyan]Restart Claude Code to activate the graphiti MCP server.[/cyan]")

    except Exception as e:
        print_error(f"Install failed: {str(e)}")
        raise typer.Exit(1)
```

---

**src/cli/__init__.py** — add mcp command group registration. Follow the exact same pattern as hooks_app and queue_app. Add these two lines in the correct positions:

In the "Command imports" section, add:
```python
from src.cli.commands.mcp import mcp_app
```

In the "Register commands" section, add:
```python
# Register mcp command group
app.add_typer(mcp_app, name="mcp", help="MCP server for Claude Code integration")
```

Update the final comment to reflect 14 commands total (add mcp to the list).
  </action>
  <verify>
Run: `graphiti mcp --help` — must show "serve" and "install" subcommands.

Run: `python -c "from src.mcp_server.install import install_mcp_server, SKILL_MD_CONTENT; assert 'graphiti' in SKILL_MD_CONTENT; assert 'TOON' in SKILL_MD_CONTENT; print('install OK')"` — must print "install OK".

Run: `graphiti --help` — must list "mcp" in the commands list.

Run: `python -c "from src.cli.commands.mcp import mcp_app; print('mcp_app OK')"` — must print "mcp_app OK".

Test mcp install (dry run check):
`python -c "
from pathlib import Path
import json
from src.mcp_server.install import install_mcp_server
results = install_mcp_server(force=True)
claude_json = Path.home() / '.claude.json'
config = json.loads(claude_json.read_text())
assert 'graphiti' in config['mcpServers'], 'graphiti not in mcpServers'
assert config['mcpServers']['graphiti']['args'] == ['mcp', 'serve'], 'wrong args'
skill_path = Path.home() / '.claude' / 'skills' / 'graphiti' / 'SKILL.md'
assert skill_path.exists(), 'SKILL.md not installed'
print('mcp install OK')
"` — must print "mcp install OK".
  </verify>
  <done>
graphiti mcp serve and graphiti mcp install subcommands work. install_mcp_server() writes correct entry to ~/.claude.json without destroying other entries. SKILL.md installed at ~/.claude/skills/graphiti/SKILL.md with all four behavior patterns (session start, post-decision, topic surfacing, explicit requests). mcp_app registered in CLI.
  </done>
</task>

</tasks>

<verification>
After both tasks complete, run these end-to-end checks:

1. **All tools registered:** `python -c "from src.mcp_server.server import mcp; tools = sorted([t.name for t in mcp._tool_manager.list_tools()]); print(tools); assert len(tools) == 10"` — 10 tools

2. **No stdout on import:** `python -c "import io,sys; buf=io.StringIO(); sys.stdout=buf; from src.mcp_server import server; sys.stdout=sys.__stdout__; assert not buf.getvalue(), f'stdout: {buf.getvalue()[:50]}'; print('clean')"` — prints "clean"

3. **SKILL.md content quality:** `grep -c "Session start\|After key moments\|Topic surfacing\|Explicit requests" ~/.claude/skills/graphiti/SKILL.md` — must output 4 (all four behavior patterns present)

4. **~/.claude.json entry:** `python -c "import json; from pathlib import Path; c=json.loads((Path.home()/'.claude.json').read_text()); assert c['mcpServers']['graphiti']['args']==['mcp','serve']; print('claude.json OK')"` — prints "claude.json OK"

5. **Context resource returns empty for empty graph (no crash):** `python -c "from src.mcp_server.context import get_context; result = get_context(); assert isinstance(result, str); print(f'context returns str ({len(result)} chars)')"` — returns string (possibly empty)

6. **CLI help updated:** `graphiti --help | grep mcp` — shows mcp in command list
</verification>

<success_criteria>
- FastMCP server with 10 tools + graphiti://context resource running on both transports
- graphiti mcp install writes correct entry to ~/.claude.json without corrupting existing config
- SKILL.md installed at ~/.claude/skills/graphiti/SKILL.md with all 4 automatic behavior patterns
- Context resource returns TOON under 8K tokens, empty string for empty graph, non-blocking on stale index
- graphiti mcp command group registered in CLI (graphiti --help shows mcp)
- No stdout contamination anywhere in src/mcp_server/ modules
</success_criteria>

<output>
After completion, create `.planning/phases/08-mcp-server/08-03-SUMMARY.md` with:
- All files created and their purpose
- FastMCP tool registration pattern used
- SKILL.md location and content summary
- ~/.claude.json entry written
- Verification commands run and their output
</output>
