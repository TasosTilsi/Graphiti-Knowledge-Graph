---
phase: 05-background-queue
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/queue/worker.py
  - src/queue/__init__.py
autonomous: true

must_haves:
  truths:
    - "Worker processes jobs in background thread without blocking main thread"
    - "Sequential jobs process alone (barrier); parallel jobs batch together concurrently"
    - "Failed jobs retry with exponential backoff (10s, 20s, 40s)"
    - "Jobs exceeding 3 retries move to dead letter table"
    - "Worker shuts down gracefully — completes current job, then stops"
    - "Public API provides enqueue(), get_status(), process_queue() functions"
  artifacts:
    - path: "src/queue/worker.py"
      provides: "BackgroundWorker class with threading.Event lifecycle, ThreadPoolExecutor for parallel batches, exponential backoff retry"
      contains: "class BackgroundWorker"
    - path: "src/queue/__init__.py"
      provides: "Public API functions: enqueue, get_status, process_queue, start_worker, stop_worker"
      exports: ["enqueue", "get_status", "process_queue", "start_worker", "stop_worker"]
  key_links:
    - from: "src/queue/worker.py"
      to: "src/queue/storage.py"
      via: "BackgroundWorker calls JobQueue.get_batch(), ack(), nack(), move_to_dead_letter()"
      pattern: "self\\._queue\\.(get_batch|ack|nack|move_to_dead_letter)"
    - from: "src/queue/__init__.py"
      to: "src/queue/worker.py"
      via: "Module-level functions manage singleton BackgroundWorker"
      pattern: "_worker\\.(start|stop)"
    - from: "src/queue/__init__.py"
      to: "src/queue/storage.py"
      via: "Module-level enqueue delegates to singleton JobQueue"
      pattern: "_queue\\.enqueue"
---

<objective>
Implement the background worker thread with parallel batch processing, exponential backoff retry, and expose the public queue API for use by CLI commands and future MCP server.

Purpose: Enable non-blocking job processing — the core value proposition of Phase 5. Worker runs as background thread, processes jobs from SQLite queue, retries failures, and moves exhausted jobs to dead letter.
Output: Fully functional background worker + public API ready for CLI integration.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-background-queue/05-RESEARCH.md
@.planning/phases/05-background-queue/05-CONTEXT.md
@.planning/phases/05-background-queue/05-01-SUMMARY.md
@src/queue/models.py
@src/queue/storage.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement BackgroundWorker with parallel batching and retry</name>
  <files>src/queue/worker.py</files>
  <action>
Create src/queue/worker.py implementing the BackgroundWorker class following the research pattern (Pattern 1 + 3 + 5 from 05-RESEARCH.md).

**BackgroundWorker class:**

Constructor `__init__(self, job_queue: JobQueue, max_workers: int = 4)`:
- Store reference to JobQueue instance
- Create `threading.Event()` as `self._stop_event`
- Set `self._thread = None`
- Set `self._executor = None` (ThreadPoolExecutor, created lazily in worker thread)
- Set `self._max_workers = max_workers` (4 per research recommendation — I/O-bound CLI replay)
- Set `self._max_retries = 3` (per user decision)
- Set `self._base_backoff = 10` (seconds, per user decision: 10s, 20s, 40s)
- Use `structlog.get_logger()` for logging

**start(self) -> None:**
- If thread is None or not alive: create new Thread(target=self._run, daemon=False) and start it
- Log "background_worker_started"
- Non-daemon thread per research (anti-pattern: daemon threads lose in-progress jobs)

**stop(self, timeout: float = 30.0) -> None:**
- If thread is alive: set stop_event, join with timeout
- If thread still alive after timeout: log warning "background_worker_timeout"
- Else: log "background_worker_stopped"
- Reset thread to None

**is_running(self) -> bool:**
- Return True if thread is not None and is_alive()

**_run(self) -> None:** (main worker loop)
- Create ThreadPoolExecutor(max_workers=self._max_workers)
- try/finally: shutdown executor in finally block
- Loop while not stop_event.is_set():
  1. Call `self._job_queue.get_batch(max_items=self._max_workers)`
  2. If empty batch: `self._stop_event.wait(timeout=1.0)` then continue (responsive shutdown, not busy-wait)
  3. If batch has single sequential job (parallel=False): call `self._process_single_job(batch[0])`
  4. If batch has parallel jobs: call `self._process_parallel_batch(batch)`

**_process_single_job(self, item: dict) -> None:**
- Call `self._execute_with_retry(item)`

**_process_parallel_batch(self, items: list[dict]) -> None:**
- Use `concurrent.futures.as_completed()` pattern
- Submit each item to executor via `self._executor.submit(self._execute_with_retry, item)`
- Collect futures, wait for all to complete
- Individual job failures are handled inside _execute_with_retry (full isolation per user decision)

**_execute_with_retry(self, item: dict) -> None:**
- Check current attempts against max_retries
- If attempts >= max_retries: move_to_dead_letter immediately (job was requeued too many times)
- Try to execute: call `self._replay_command(item)`
- On success: call `self._job_queue.ack(item)`, log "job_completed"
- On exception:
  - If attempts + 1 < max_retries: calculate backoff = base_backoff * (2 ** attempts) = 10, 20, 40
    - Log "job_failed_retrying" with attempt number, delay, error
    - Sleep for backoff duration (use `self._stop_event.wait(timeout=delay)` for responsive shutdown during backoff)
    - Call `self._job_queue.nack(item)` to requeue
  - If attempts + 1 >= max_retries: call `self._job_queue.move_to_dead_letter(item, str(e))`, log "job_exhausted_dead_letter"

**_replay_command(self, item: dict) -> None:**
- Extract command and args from `item['payload']`
- Expected payload format: `{"command": "add", "args": ["content here"], "kwargs": {"scope": "project"}}`
- Use `subprocess.run()` to replay CLI command: construct `["graphiti", command, *args, **kwargs_to_flags]`
- Check returncode: if non-zero, raise RuntimeError with stderr
- Log "command_replayed" with command details
- **IMPORTANT:** This is the CLI-first architecture — worker replays CLI commands, CLI remains single source of truth (per user decision)

**Helper: _kwargs_to_flags(kwargs: dict) -> list[str]:**
- Convert dict to CLI flag list: `{"scope": "project", "force": True}` -> `["--scope", "project", "--force"]`
- Boolean True values produce flag only (no value)
- Boolean False values are skipped
- All other values produce `--key value` pair

**Thread safety notes:**
- BackgroundWorker uses its own thread for processing
- ThreadPoolExecutor creates additional threads for parallel batches
- JobQueue handles its own thread safety via persistqueue
- Dead letter operations use per-call sqlite3 connections (set up in storage.py Plan 01)
  </action>
  <verify>
Run: `python -c "
from src.queue.storage import JobQueue
from src.queue.worker import BackgroundWorker
import tempfile, os, time

q = JobQueue(db_path=os.path.join(tempfile.mkdtemp(), 'test_queue'))
w = BackgroundWorker(q)

# Verify worker starts and stops
w.start()
assert w.is_running(), 'Worker should be running'
time.sleep(0.5)
w.stop(timeout=5)
assert not w.is_running(), 'Worker should be stopped'
print('Worker lifecycle ok')
"`

Verify: Worker thread starts, runs, and stops cleanly without hanging.
  </verify>
  <done>BackgroundWorker processes jobs in background thread. Sequential jobs execute alone (barrier). Parallel jobs batch and run concurrently via ThreadPoolExecutor. Failed jobs retry 3 times with exponential backoff (10s, 20s, 40s). Exhausted jobs move to dead letter. Worker shuts down gracefully via Event signal.</done>
</task>

<task type="auto">
  <name>Task 2: Create public queue API with singleton management</name>
  <files>src/queue/__init__.py</files>
  <action>
Update src/queue/__init__.py to provide the module-level public API following the established pattern from src/llm/__init__.py (singleton client with convenience functions per Phase 3).

**Module-level singletons:**
- `_queue: JobQueue | None = None`
- `_worker: BackgroundWorker | None = None`

**get_queue(db_path: Path | None = None, max_size: int = 100) -> JobQueue:**
- Lazy singleton: create JobQueue on first call, reuse thereafter
- Store in module-level `_queue`

**get_worker() -> BackgroundWorker:**
- Lazy singleton: create BackgroundWorker with get_queue() on first call
- Store in module-level `_worker`

**enqueue(job_type: str, payload: dict, parallel: bool = False, silent: bool | None = None) -> str:**
- Get singleton queue via get_queue()
- Call queue.enqueue(job_type, payload, parallel)
- Determine feedback mode: if silent is None, use `is_hook_context()` from detector
- If not silent: log one-liner confirmation (for explicit --async per user decision)
- If silent: only structured debug log (for auto-detected hooks per user decision)
- Check if worker is running; if so, no action needed (worker will pick up job)
- Return job_id

**get_status() -> dict:**
- Get singleton queue stats via get_queue().get_stats()
- Add worker status: running (bool) from get_worker().is_running()
- Add dead letter jobs list from get_queue().get_dead_letter_jobs()
- Determine health level (same pattern as `graphiti health` per user decision):
  - "ok" if pending < 80% capacity
  - "warning" if pending >= 80% capacity
  - "error" if pending >= 100% capacity
- Return dict with all status info

**process_queue() -> tuple[int, int]:**
- CLI fallback for manual processing when MCP isn't running (per user decision)
- Get singleton queue and worker
- If worker not running: start it
- Process until queue empty or stop signaled
- Return (success_count, failure_count)
- This is the `graphiti process-queue` equivalent

**start_worker() -> None:**
- Conditional eager startup per user decision: check if backlog exists (threshold=1 per research)
- If get_queue().get_pending_count() > 0 or force: start worker
- Intended to be called on MCP boot

**stop_worker() -> None:**
- Stop worker if running, clean shutdown

**reset() -> None:**
- For testing: reset singletons to None (same pattern as src/llm reset_client)

Keep existing exports from Plan 01 (QueuedJob, JobStatus, QueueStats, DeadLetterJob, is_hook_context).
Add new exports: enqueue, get_status, process_queue, start_worker, stop_worker, get_queue, get_worker, reset.
Update `__all__` list.
  </action>
  <verify>
Run: `python -c "
from src.queue import enqueue, get_status, start_worker, stop_worker, reset
import tempfile, os

# Override db path for testing
from src.queue import get_queue
os.environ['GRAPHITI_QUEUE_PATH'] = os.path.join(tempfile.mkdtemp(), 'test_api')

reset()
job_id = enqueue('test_job', {'command': 'add', 'args': ['test content']}, parallel=True, silent=True)
print(f'Enqueued: {job_id}')
status = get_status()
print(f'Status: health={status.get(\"health\")}, pending={status.get(\"pending\")}')
print('Public API ok')
reset()
del os.environ['GRAPHITI_QUEUE_PATH']
"`

Verify: enqueue returns job ID, get_status returns dict with health level and pending count.
  </verify>
  <done>Public API exposes enqueue(), get_status(), process_queue(), start_worker(), stop_worker(). Singleton pattern matches Phase 3 LLM client. Context-aware feedback: silent for hooks, verbose for interactive CLI. Worker starts on backlog detection (threshold=1).</done>
</task>

</tasks>

<verification>
1. `python -c "from src.queue import enqueue, get_status, process_queue, start_worker, stop_worker"` succeeds
2. `python -c "from src.queue.worker import BackgroundWorker"` succeeds
3. Worker starts and stops without hanging (lifecycle test with 5s timeout)
4. Enqueue job, start worker, verify job gets processed (wait up to 5s)
5. Enqueue job with bad payload, verify it retries and eventually moves to dead letter
6. get_status() returns correct health level based on queue capacity
</verification>

<success_criteria>
- Background worker processes jobs in separate thread
- Worker shuts down gracefully (no hanging, no lost jobs)
- Parallel batch processing works via ThreadPoolExecutor
- Exponential backoff: 10s, 20s, 40s between retries
- Dead letter after 3 failures
- Public API functions work correctly
- Singleton pattern enables shared state across CLI and future MCP
</success_criteria>

<output>
After completion, create `.planning/phases/05-background-queue/05-02-SUMMARY.md`
</output>
