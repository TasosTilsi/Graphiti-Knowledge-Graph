---
phase: 05-background-queue
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/queue/__init__.py
  - src/queue/models.py
  - src/queue/storage.py
  - src/queue/detector.py
autonomous: true

must_haves:
  truths:
    - "Jobs persist in SQLite across process restarts"
    - "FIFO ordering preserved — oldest jobs dequeued first"
    - "Parallel jobs batch together, sequential jobs act as barriers"
    - "Dead letter table stores failed jobs with failure metadata"
    - "Hook context auto-detected via environment variables and TTY status"
    - "Queue always accepts jobs regardless of size (soft limit with warnings)"
  artifacts:
    - path: "src/queue/models.py"
      provides: "QueuedJob dataclass, JobStatus enum, QueueStats dataclass, DeadLetterJob dataclass"
      contains: "class QueuedJob"
    - path: "src/queue/storage.py"
      provides: "JobQueue class with SQLite persistence, ack/nack, dead letter, batch retrieval"
      contains: "class JobQueue"
    - path: "src/queue/detector.py"
      provides: "Hook context detection function"
      contains: "def is_hook_context"
    - path: "src/queue/__init__.py"
      provides: "Package initialization with model exports"
  key_links:
    - from: "src/queue/storage.py"
      to: "src/queue/models.py"
      via: "imports QueuedJob, JobStatus, DeadLetterJob"
      pattern: "from src\\.queue\\.models import"
    - from: "src/queue/storage.py"
      to: "persistqueue.SQLiteAckQueue"
      via: "SQLite-backed persistent queue"
      pattern: "SQLiteAckQueue"
---

<objective>
Create the foundational queue data models, SQLite-backed persistent storage with FIFO batching and dead letter support, and hook context detection.

Purpose: Establish the data layer and detection mechanisms that all other queue components (worker, CLI, public API) build upon.
Output: src/queue/ package with models.py, storage.py, detector.py ready for worker integration.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-background-queue/05-RESEARCH.md
@.planning/phases/05-background-queue/05-CONTEXT.md
@src/llm/queue.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create queue data models and context detector</name>
  <files>src/queue/__init__.py, src/queue/models.py, src/queue/detector.py</files>
  <action>
Create src/queue/ package directory.

**src/queue/models.py** — Define all queue data types:

1. `JobStatus` enum with values: PENDING, PROCESSING, FAILED, DEAD (use Python `enum.Enum`).

2. `QueuedJob` dataclass with fields:
   - `id: str` — UUID4 string
   - `job_type: str` — e.g., "add_knowledge", "capture_commit"
   - `payload: dict` — CLI command + arguments as dict (minimal payload per user decision)
   - `parallel: bool` — whether job can run in parallel batch (caller sets at enqueue time per user decision)
   - `created_at: float` — Unix timestamp (time.time())
   - `status: JobStatus` — defaults to PENDING
   - `attempts: int` — defaults to 0
   - `last_error: str | None` — defaults to None

3. `DeadLetterJob` dataclass (mirrors QueuedJob + failure metadata per research recommendation):
   - All fields from QueuedJob
   - `failed_at: float` — Unix timestamp when moved to dead letter
   - `final_error: str` — error message from last attempt
   - `retry_count: int` — total attempts before death

4. `QueueStats` dataclass with fields:
   - `pending: int`
   - `processing: int`
   - `failed: int`
   - `dead_letter: int`
   - `max_size: int`
   - `capacity_pct: float` — computed as (pending / max_size * 100)

All dataclasses should be regular (not frozen) since status and attempts change during processing. Use `from dataclasses import dataclass, field, asdict` for serialization support.

**src/queue/detector.py** — Hook context detection:

Create `is_hook_context() -> bool` function that returns True (silent mode) when:
1. Any environment variable starting with `CLAUDE_` exists (Claude Code hook marker)
2. `sys.stdin.isatty()` returns False (piped/non-interactive)
3. Any CI/CD environment variable exists: CI, GITHUB_ACTIONS, GITLAB_CI, JENKINS_HOME

Returns False (interactive CLI mode) otherwise.

Use `os.environ` and `sys.stdin` — no external dependencies. Add docstrings explaining the detection logic and when to use silent vs verbose feedback.

**src/queue/__init__.py** — Basic package init:

Export key types: QueuedJob, JobStatus, QueueStats, DeadLetterJob, is_hook_context. Use `__all__` list. Include module docstring describing the queue package purpose. Public API functions (enqueue, get_status, process_queue) will be added in Plan 02.
  </action>
  <verify>
Run: `python -c "from src.queue.models import QueuedJob, JobStatus, QueueStats, DeadLetterJob; from src.queue.detector import is_hook_context; print('imports ok')"`

Verify JobStatus has PENDING, PROCESSING, FAILED, DEAD values.
Verify QueuedJob can be serialized with asdict().
Verify is_hook_context() returns False in interactive terminal.
  </verify>
  <done>All queue data types importable and serializable. Hook context detection works correctly in terminal (returns False) and would return True in hook context (env var or non-TTY).</done>
</task>

<task type="auto">
  <name>Task 2: Create SQLite-backed JobQueue with dead letter support</name>
  <files>src/queue/storage.py</files>
  <action>
Create src/queue/storage.py implementing the JobQueue class. Follow the established pattern from src/llm/queue.py (LLMRequestQueue) but extend with custom schema per user decisions.

**JobQueue class:**

Constructor `__init__(self, db_path: Path | None = None, max_size: int = 100)`:
- Default db_path: `~/.graphiti/job_queue` (same pattern as LLM queue)
- Create directory with `mkdir(parents=True, exist_ok=True)`
- Initialize `persistqueue.SQLiteAckQueue(str(db_path), auto_commit=True)` for the main queue
- Store max_size (configurable, default 100 per user decision)
- Initialize dead letter table via custom SQLite (separate from persistqueue) — use `sqlite3` stdlib
- Create dead letter table schema: `dead_letter_jobs(id TEXT PRIMARY KEY, job_type TEXT, payload TEXT, parallel INTEGER, created_at REAL, failed_at REAL, final_error TEXT, retry_count INTEGER)`
- Use `structlog.get_logger()` for logging

**enqueue(self, job_type: str, payload: dict, parallel: bool = False) -> str:**
- Create QueuedJob with uuid4 id, current timestamp, PENDING status, 0 attempts
- Check queue size: if >= max_size, log warning (SOFT limit — never reject per user decision: "always accept jobs, never lose knowledge")
- Put `asdict(job)` into SQLiteAckQueue
- Log structured event "job_enqueued" with job_id, job_type, parallel flag, queue_size
- Return job ID

**get_batch(self, max_items: int = 10) -> list[dict]:**
- Get first item from queue (non-blocking). If empty, return [].
- If first item has `parallel=False`: return [item] (sequential barrier — process alone per user decision)
- If first item has `parallel=True`: collect consecutive parallel items up to max_items. When a non-parallel item is encountered, nack it back and stop collecting.
- Return batch list

**ack(self, item: dict) -> None:**
- Call `self._queue.ack(item)` to permanently remove completed job

**nack(self, item: dict) -> None:**
- Increment item['attempts']
- Call `self._queue.nack(item)` to requeue for retry

**move_to_dead_letter(self, item: dict, error: str) -> None:**
- Insert into dead_letter_jobs table via sqlite3 connection
- Use `json.dumps(item['payload'])` for payload column
- Set failed_at to current timestamp, final_error to error string, retry_count to item['attempts']
- Ack the item from main queue to remove it
- Log "job_moved_to_dead_letter" with job_id

**get_dead_letter_jobs(self) -> list[DeadLetterJob]:**
- Query all rows from dead_letter_jobs table
- Return as list of DeadLetterJob dataclass instances
- Parse payload back from JSON string to dict

**retry_dead_letter(self, job_id: str) -> bool:**
- Find job in dead_letter_jobs by id
- If found: delete from dead_letter_jobs, enqueue back to main queue with reset attempts
- Return True if found and requeued, False if not found

**get_stats(self) -> QueueStats:**
- Query pending count from main queue (qsize())
- Query dead letter count from dead_letter_jobs table
- Return QueueStats with computed capacity_pct
- processing and failed counts are 0 at rest (these are transient states tracked by worker)

**get_pending_count(self) -> int:**
- Delegate to `self._queue.qsize()`

**IMPORTANT:** For the dead letter SQLite table, create a SEPARATE sqlite3 connection (not using persistqueue). Store the dead letter DB file alongside the main queue. Use `threading.local()` or create connection per call for thread safety (per research pitfall #1). Enable WAL mode on the dead letter database for better concurrency.
  </action>
  <verify>
Run: `python -c "
from src.queue.storage import JobQueue
import tempfile, os
q = JobQueue(db_path=os.path.join(tempfile.mkdtemp(), 'test_queue'))
jid = q.enqueue('test_job', {'cmd': 'add', 'args': ['hello']}, parallel=True)
print(f'Enqueued: {jid}')
batch = q.get_batch()
print(f'Batch size: {len(batch)}')
q.ack(batch[0])
print(f'Pending after ack: {q.get_pending_count()}')
stats = q.get_stats()
print(f'Stats: pending={stats.pending}, dead_letter={stats.dead_letter}')
print('storage ok')
"`

Verify: enqueue returns UUID string, get_batch returns list of dicts, ack removes from queue, stats reflect correct counts.
  </verify>
  <done>JobQueue persists jobs to SQLite, supports FIFO batching with parallel/sequential logic, dead letter table stores failed jobs, stats report queue health. Queue always accepts jobs regardless of size (soft limit with warning logs).</done>
</task>

</tasks>

<verification>
1. `python -c "from src.queue import QueuedJob, JobStatus, QueueStats, DeadLetterJob, is_hook_context"` succeeds
2. `python -c "from src.queue.storage import JobQueue"` succeeds
3. Create temporary JobQueue, enqueue 3 parallel + 1 sequential job, verify get_batch returns 3 parallel jobs, then 1 sequential
4. Verify dead letter: enqueue job, get_batch, move_to_dead_letter, verify get_dead_letter_jobs returns it
5. Verify retry_dead_letter moves job back to main queue
6. Verify queue survives process restart (create, enqueue, destroy, recreate with same path, verify job still there)
</verification>

<success_criteria>
- Queue data types defined with all required fields
- SQLite persistence verified across restarts
- FIFO batch logic correct: parallel jobs batch, sequential jobs barrier
- Dead letter table stores and retrieves failed jobs
- Hook context detection returns correct values
- Soft limit: jobs always accepted, warnings logged at capacity
</success_criteria>

<output>
After completion, create `.planning/phases/05-background-queue/05-01-SUMMARY.md`
</output>
