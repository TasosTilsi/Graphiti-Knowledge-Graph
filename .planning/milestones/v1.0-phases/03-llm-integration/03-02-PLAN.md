---
phase: 03-llm-integration
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/llm/client.py
autonomous: true

must_haves:
  truths:
    - "Cloud Ollama is tried first when not in cooldown"
    - "Errors trigger retry with fixed delay before failover"
    - "Rate-limit (429) triggers 10-minute cooldown"
    - "Non-rate-limit errors do NOT set cooldown; cloud is tried on very next request"
    - "Failover to local Ollama happens automatically on cloud failure"
    - "Missing local models fail with clear instructions"
    - "LLMUnavailableError message format includes queue info placeholder"
  artifacts:
    - path: "src/llm/client.py"
      provides: "OllamaClient with failover logic"
      exports: ["OllamaClient", "LLMUnavailableError"]
      min_lines: 150
  key_links:
    - from: "src/llm/client.py"
      to: "ollama.Client"
      via: "cloud and local client instances"
      pattern: "Client\\(host="
    - from: "src/llm/client.py"
      to: "tenacity"
      via: "retry decorator"
      pattern: "@retry"
    - from: "src/llm/client.py"
      to: "src/llm/config.py"
      via: "LLMConfig import"
      pattern: "from src\\.llm\\.config import"
---

<objective>
Implement OllamaClient with cloud-first failover and tenacity-based retry logic.

Purpose: The core LLM client that implements the cloud-first/local-fallback pattern. All LLM operations flow through this client which handles retries, cooldowns, and automatic failover per the decisions in CONTEXT.md.
Output: OllamaClient class with chat(), generate(), and embed() methods, all with automatic failover.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-integration/03-CONTEXT.md
@.planning/phases/03-llm-integration/03-RESEARCH.md
@.planning/phases/03-llm-integration/03-01-SUMMARY.md
@src/llm/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement OllamaClient with cloud/local failover</name>
  <files>src/llm/client.py</files>
  <action>
Create `src/llm/client.py` with:

1. **Imports:**
   - ollama.Client, ollama.ResponseError
   - tenacity (retry, stop_after_attempt, wait_fixed, retry_if_exception_type)
   - httpx.Timeout for granular timeout control
   - time for cooldown tracking
   - structlog for failover logging
   - LLMConfig from src.llm.config

2. **OllamaClient class:**

   **__init__(self, config: LLMConfig):**
   - Store config
   - Create cloud Client with:
     - host=config.cloud_endpoint
     - headers={'Authorization': f'Bearer {config.cloud_api_key}'} if api_key present
     - timeout=httpx.Timeout(connect=5.0, read=config.request_timeout_seconds, write=10.0, pool=20.0)
   - Create local Client with:
     - host=config.local_endpoint
     - timeout=httpx.Timeout(connect=2.0, read=60.0, write=10.0, pool=5.0)
   - Initialize cloud_cooldown_until = 0 (unix timestamp)
   - Load cooldown state from llm_state.json if exists (per RESEARCH.md pitfall 5)
   - Initialize structlog logger
   - Track _current_provider: str = "none" for status reporting
   - Handle local_auto_start config:
     - If config.local_auto_start is True, log a debug message: "local_auto_start is configured but auto-start is not yet implemented. Start Ollama manually: ollama serve"
     - This field is config-ready for future implementation. The current behavior when local Ollama is not running is to fail with clear instructions ("Local Ollama not running. Start with: ollama serve") regardless of this setting.
     - Add a code comment: `# TODO: local_auto_start implementation deferred. Config field exists for future use.`

   **_is_cloud_available(self) -> bool:**
   - Check if cloud_api_key is set
   - Check if current time >= cloud_cooldown_until
   - Return True if both conditions met
   - IMPORTANT: This method ONLY checks rate-limit cooldown. Non-429 errors do NOT
     set any cooldown, so cloud is always tried again on the very next request after
     a non-rate-limit failure. This is by design per CONTEXT.md.

   **_save_cooldown_state(self):**
   - Write {"cloud_cooldown_until": self.cloud_cooldown_until} to llm_state.json
   - Handle file write errors gracefully (log warning, don't crash)

   **_load_cooldown_state(self):**
   - Read from llm_state.json if exists
   - Set self.cloud_cooldown_until from file
   - Handle missing/corrupt file gracefully

   **_retry_cloud(self, operation: str, **kwargs):**
   - Use tenacity @retry decorator with:
     - stop=stop_after_attempt(config.retry_max_attempts)
     - wait=wait_fixed(config.retry_delay_seconds)
     - retry=retry_if_exception_type((ConnectionError, ResponseError))
     - before_sleep callback to log retry attempt
   - Call appropriate cloud client method based on operation
   - On success, set _current_provider = "cloud"
   - Return result

   **_try_local(self, operation: str, model: str | None, **kwargs):**
   - First, verify local Ollama is running and has required models
   - If model not specified, select from config.local_models using fallback chain
   - Try each model in fallback chain until one works
   - If model not pulled, raise clear error: "Model {model} not available. Run: ollama pull {model}"
   - On success, set _current_provider = "local"
   - Return result

   **_check_local_models(self) -> list[str]:**
   - Call local_client.list()
   - Return list of available model names
   - On connection error, raise with message: "Local Ollama not running. Start with: ollama serve"

   **_handle_cloud_error(self, error: ResponseError):**
   - If status_code == 429 (rate limit):
     - Set cooldown: cloud_cooldown_until = time.time() + config.rate_limit_cooldown_seconds
     - Save cooldown state to disk
     - Log warning: "Cloud rate-limited (429). Cooldown for {config.rate_limit_cooldown_seconds}s"
   - For ALL OTHER errors (non-429):
     - Do NOT set any cooldown. Do NOT modify cloud_cooldown_until.
     - Log warning with status code and error message
     - Add explicit comment in code:
       ```python
       # CONTEXT.md decision: Non-rate-limit errors do NOT set cooldown.
       # Cloud will be tried again on the very next request.
       # Only 429 errors trigger the 10-minute cooldown period.
       ```
   - If config.failover_logging: log failover event (for both 429 and non-429)

   **chat(self, model: str | None = None, messages: list[dict], **kwargs):**
   - If _is_cloud_available():
     - Try _retry_cloud("chat", model=model or cloud_default, messages=messages, **kwargs)
     - On ResponseError: _handle_cloud_error(e), fall through to local
   - Return _try_local("chat", model, messages=messages, **kwargs)

   **generate(self, model: str | None = None, prompt: str, **kwargs):**
   - Same pattern as chat()

   **embed(self, model: str | None = None, input: str | list[str], **kwargs):**
   - Default model to config.embeddings_model
   - Same failover pattern
   - Note: embeddings typically use local nomic-embed-text, but support cloud

   **@property current_provider(self) -> str:**
   - Return _current_provider ("cloud", "local", or "none")

3. **Helper function:**

   **get_largest_available_model(models: list[str], available: list[str]) -> str | None:**
   - Filter models to only those in available
   - Sort by parameter count (extract from name like "9b" -> 9_000_000_000)
   - Return largest, or None if none available
   - Use regex to handle variations (9b, 9B, etc.)

4. **Exception class:**

   **LLMUnavailableError(Exception):**
   - Standard message format (per CONTEXT.md): "LLM unavailable. Request queued for retry. ID: {request_id}"
   - Attributes:
     - message: str
     - request_id: str | None = None (populated when queue integration exists in Plan 03-04)
   - Constructor should accept optional request_id parameter:
     ```python
     class LLMUnavailableError(Exception):
         """Raised when both cloud and local LLM providers fail.

         Message format per CONTEXT.md:
           "LLM unavailable. Request queued for retry. ID: <uuid>"

         The request_id is populated by the queue module (Plan 03-04).
         Before queue integration, request_id is None and message is:
           "LLM unavailable. Request will be queued for retry."
         """
         def __init__(self, message: str | None = None, request_id: str | None = None):
             self.request_id = request_id
             if message is None:
                 if request_id:
                     message = f"LLM unavailable. Request queued for retry. ID: {request_id}"
                 else:
                     message = "LLM unavailable. Request will be queued for retry."
             super().__init__(message)
     ```
   - When raised in chat()/generate()/embed() before queue integration (Plan 03-04),
     use: `raise LLMUnavailableError()` (no request_id yet).
   - Plan 03-04 will update the raise sites to include the actual request_id from the queue.

Important implementation notes:
- Do NOT use circuit breaker pattern (per CONTEXT.md - always try cloud on each request)
- Use fixed delay, NOT exponential backoff (per CONTEXT.md)
- Streaming errors: When stream=True, check each yielded object for 'error' key (per RESEARCH.md pitfall 4)
- Non-429 errors must NOT set any persistent cooldown. The `_is_cloud_available()` check gates ONLY on rate-limit cooldown, ensuring cloud is attempted on the very next request after non-rate-limit failures.
  </action>
  <verify>
Run unit test:
```python
# Test that client initializes with config
from src.llm.config import load_config
from src.llm.client import OllamaClient, LLMUnavailableError
config = load_config()
client = OllamaClient(config)
print(f"Cloud available: {client._is_cloud_available()}")
print(f"Current provider: {client.current_provider}")

# Test LLMUnavailableError message formats
err_no_id = LLMUnavailableError()
print(f"No ID: {err_no_id}")  # "LLM unavailable. Request will be queued for retry."

err_with_id = LLMUnavailableError(request_id="abc-123")
print(f"With ID: {err_with_id}")  # "LLM unavailable. Request queued for retry. ID: abc-123"
```
Verify: Should print "Cloud available: False" (no API key), "Current provider: none", and both message formats.
  </verify>
  <done>OllamaClient implements cloud-first with local fallback, retry with fixed delay, rate-limit cooldown (429 only -- non-429 errors do NOT set cooldown so cloud is tried on next request), clear error messages for missing models, and LLMUnavailableError with CONTEXT.md-compliant message format.</done>
</task>

<task type="auto">
  <name>Task 2: Add OllamaClient to llm module exports</name>
  <files>src/llm/__init__.py</files>
  <action>
Update `src/llm/__init__.py` to export:
- OllamaClient
- LLMUnavailableError

Add to existing exports from Plan 01 (LLMConfig, load_config).

Update __all__ list to include all exports.
  </action>
  <verify>Run `python -c "from src.llm import OllamaClient, LLMUnavailableError, LLMConfig, load_config; print('All exports available')"` - should succeed</verify>
  <done>OllamaClient and LLMUnavailableError exported from src.llm module.</done>
</task>

</tasks>

<verification>
- [ ] OllamaClient initializes with LLMConfig
- [ ] _is_cloud_available() returns False when no API key configured
- [ ] _is_cloud_available() returns False when in cooldown period
- [ ] _is_cloud_available() returns True after non-429 cloud error (no cooldown set)
- [ ] Retry decorator configured with fixed delay (not exponential)
- [ ] Rate-limit (429) sets 10-minute cooldown
- [ ] Non-429 errors do NOT set any cooldown (cloud tried on next request)
- [ ] Cooldown state persists to llm_state.json
- [ ] Local fallback tries models in configured order
- [ ] Missing model raises clear error with "ollama pull" instruction
- [ ] local_auto_start config acknowledged but implementation deferred with TODO
- [ ] LLMUnavailableError generates correct message format per CONTEXT.md
- [ ] LLMUnavailableError supports optional request_id parameter
- [ ] All classes exported from src.llm module
</verification>

<success_criteria>
- OllamaClient with chat(), generate(), embed() methods
- Cloud-first with automatic local fallback
- tenacity retry with fixed delay
- Rate-limit cooldown persisted across restarts (429 only)
- Non-429 errors: cloud tried again on very next request (no cooldown)
- Clear error messages for local Ollama issues
- current_provider property reports active provider
- LLMUnavailableError with CONTEXT.md message format
- local_auto_start config-ready with deferred TODO
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-02-SUMMARY.md`
</output>
