---
phase: 03-llm-integration
plan: 04
type: execute
wave: 3
depends_on: ["03-02", "03-03"]
files_modified:
  - src/llm/client.py
  - src/llm/__init__.py
autonomous: true

must_haves:
  truths:
    - "OllamaClient integrates quota tracking after successful cloud calls"
    - "OllamaClient queues failed requests when both cloud and local fail"
    - "Exception includes queue ID for caller tracking"
    - "Public API provides high-level convenience functions"
  artifacts:
    - path: "src/llm/client.py"
      provides: "OllamaClient with full quota and queue integration"
      contains: "QuotaTracker"
    - path: "src/llm/__init__.py"
      provides: "Complete public API with convenience functions"
      exports: ["get_client", "chat", "generate", "embed"]
  key_links:
    - from: "src/llm/client.py"
      to: "src/llm/quota.py"
      via: "QuotaTracker usage"
      pattern: "QuotaTracker|quota_tracker"
    - from: "src/llm/client.py"
      to: "src/llm/queue.py"
      via: "LLMRequestQueue usage"
      pattern: "LLMRequestQueue|request_queue"
    - from: "src/llm/__init__.py"
      to: "src/llm/client.py"
      via: "get_client singleton"
      pattern: "get_client|_client"
---

<objective>
Integrate quota tracking and request queue into OllamaClient, create high-level public API.

Purpose: Wire together all LLM components into a cohesive system. OllamaClient tracks quota after cloud calls and queues failed requests. Public API provides simple functions for common operations.
Output: Fully integrated LLM system with quota tracking, request queueing, and clean public API.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-integration/03-CONTEXT.md
@.planning/phases/03-llm-integration/03-RESEARCH.md
@.planning/phases/03-llm-integration/03-01-SUMMARY.md
@.planning/phases/03-llm-integration/03-02-SUMMARY.md
@.planning/phases/03-llm-integration/03-03-SUMMARY.md
@src/llm/client.py
@src/llm/quota.py
@src/llm/queue.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate QuotaTracker and LLMRequestQueue into OllamaClient</name>
  <files>src/llm/client.py</files>
  <action>
Update `src/llm/client.py` to integrate quota tracking and request queue:

1. **Add imports:**
   - QuotaTracker from src.llm.quota
   - LLMRequestQueue from src.llm.queue

2. **Update __init__:**
   - Create self._quota_tracker = QuotaTracker(config.quota_warning_threshold)
   - Create self._request_queue = LLMRequestQueue(config)

3. **Update _retry_cloud() to track quota:**
   - After successful response, check if response has headers attribute
   - If httpx response available: self._quota_tracker.update_from_headers(response.headers)
   - If headers not available: self._quota_tracker.increment_local_count()
   - Note: ollama library may not expose raw headers - check response type and handle gracefully

4. **Update chat(), generate(), embed() methods:**
   - Wrap the entire method in try/except
   - If both cloud and local fail (LLMUnavailableError scenario):
     - Queue the request: request_id = self._request_queue.enqueue(operation, params, str(error))
     - Update LLMUnavailableError to include request_id
     - Raise: raise LLMUnavailableError(f"LLM unavailable. Request queued for retry. ID: {request_id}", request_id=request_id)

5. **Add quota and queue accessor methods:**

   **get_quota_status(self) -> QuotaInfo:**
   - Return self._quota_tracker.get_status()

   **get_queue_stats(self) -> dict:**
   - Return self._request_queue.get_queue_stats()

   **process_queue(self) -> tuple[int, int]:**
   - Define processor function that calls appropriate method
   - Return self._request_queue.process_all(processor)
   - Log results

6. **Handle streaming responses:**
   - For stream=True calls, check each yielded object for 'error' key
   - If error found mid-stream, handle as failure (per RESEARCH.md pitfall 4)
   - Example:
     ```python
     for chunk in response:
         if 'error' in chunk:
             raise ResponseError(chunk['error'])
         yield chunk
     ```
  </action>
  <verify>
Run:
```python
from src.llm import OllamaClient, load_config

config = load_config()
client = OllamaClient(config)

# Test quota and queue accessors exist
print(f"Quota status: {client.get_quota_status()}")
print(f"Queue stats: {client.get_queue_stats()}")
```
Should show quota info and queue stats without errors.
  </verify>
  <done>OllamaClient tracks quota after cloud calls, queues failed requests with IDs, exposes status methods.</done>
</task>

<task type="auto">
  <name>Task 2: Create high-level public API</name>
  <files>src/llm/__init__.py</files>
  <action>
Update `src/llm/__init__.py` to provide a clean public API:

1. **Singleton client pattern:**
   ```python
   _client: OllamaClient | None = None
   _config: LLMConfig | None = None

   def get_client(config: LLMConfig | None = None) -> OllamaClient:
       """Get or create the singleton OllamaClient.

       Args:
           config: Optional config, uses load_config() if not provided.
                   Only used on first call; subsequent calls return existing client.

       Returns:
           Configured OllamaClient instance
       """
       global _client, _config
       if _client is None:
           _config = config or load_config()
           _client = OllamaClient(_config)
       return _client

   def reset_client() -> None:
       """Reset the singleton client. Useful for testing."""
       global _client, _config
       _client = None
       _config = None
   ```

2. **Convenience functions:**
   ```python
   def chat(messages: list[dict], model: str | None = None, **kwargs) -> dict:
       """Send chat messages to LLM.

       Args:
           messages: List of message dicts with 'role' and 'content'
           model: Optional model name, uses default from config
           **kwargs: Additional arguments passed to ollama.chat

       Returns:
           Response dict from Ollama

       Raises:
           LLMUnavailableError: If both cloud and local fail (request queued)
       """
       return get_client().chat(model=model, messages=messages, **kwargs)

   def generate(prompt: str, model: str | None = None, **kwargs) -> dict:
       """Generate text from prompt.

       Args:
           prompt: Text prompt
           model: Optional model name
           **kwargs: Additional arguments

       Returns:
           Response dict from Ollama

       Raises:
           LLMUnavailableError: If both cloud and local fail (request queued)
       """
       return get_client().generate(model=model, prompt=prompt, **kwargs)

   def embed(input: str | list[str], model: str | None = None) -> dict:
       """Generate embeddings for text.

       Args:
           input: Text or list of texts to embed
           model: Optional model name, defaults to nomic-embed-text

       Returns:
           Response dict with embeddings

       Raises:
           LLMUnavailableError: If both cloud and local fail (request queued)
       """
       return get_client().embed(model=model, input=input)

   def get_status() -> dict:
       """Get LLM system status.

       Returns:
           Dict with current_provider, quota_status, queue_stats
       """
       client = get_client()
       return {
           "current_provider": client.current_provider,
           "quota": client.get_quota_status(),
           "queue": client.get_queue_stats(),
       }
   ```

3. **Update __all__:**
   ```python
   __all__ = [
       # Config
       "LLMConfig",
       "load_config",
       # Client
       "OllamaClient",
       "LLMUnavailableError",
       # Quota
       "QuotaTracker",
       "QuotaInfo",
       # Queue
       "LLMRequestQueue",
       "QueuedRequest",
       # Convenience API
       "get_client",
       "reset_client",
       "chat",
       "generate",
       "embed",
       "get_status",
   ]
   ```

4. **Module docstring:**
   ```python
   """LLM integration with cloud Ollama primary, local Ollama fallback.

   Quick start:
       from src.llm import chat, embed, get_status

       # Chat with automatic failover
       response = chat([{"role": "user", "content": "Hello"}])

       # Generate embeddings (uses nomic-embed-text by default)
       embeddings = embed("Text to embed")

       # Check system status
       status = get_status()
       print(f"Using: {status['current_provider']}")

   Configuration:
       Copy config/llm.toml to ~/.graphiti/llm.toml and customize.
       Set OLLAMA_API_KEY environment variable for cloud access.

   Error handling:
       If both cloud and local fail, LLMUnavailableError is raised.
       The request is automatically queued for retry.
       The exception includes the queue ID for tracking.
   """
   ```
  </action>
  <verify>
Run:
```python
from src.llm import chat, embed, get_status, get_client, reset_client

# Test singleton pattern
client1 = get_client()
client2 = get_client()
assert client1 is client2, "Should be same instance"

reset_client()
client3 = get_client()
assert client1 is not client3, "Should be new instance after reset"

# Test status
status = get_status()
print(f"Status: {status}")

print("Public API verified!")
```
  </verify>
  <done>High-level public API with get_client(), chat(), generate(), embed(), get_status() convenience functions.</done>
</task>

</tasks>

<verification>
- [ ] OllamaClient._quota_tracker exists and tracks usage
- [ ] OllamaClient._request_queue exists and queues failures
- [ ] LLMUnavailableError includes request_id
- [ ] get_client() returns singleton
- [ ] reset_client() clears singleton
- [ ] chat(), generate(), embed() convenience functions work
- [ ] get_status() returns provider, quota, queue info
- [ ] All exports in __all__ list
- [ ] Module docstring provides quick start guide
</verification>

<success_criteria>
- OllamaClient fully integrated with quota tracking and request queue
- Failed requests automatically queued with tracking ID
- Clean public API with convenience functions
- Singleton pattern for shared client
- Comprehensive docstrings and documentation
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-04-SUMMARY.md`
</output>
