---
phase: 02-security-filtering
plan: 04
type: execute
wave: 3
depends_on: ["02-02", "02-03"]
files_modified:
  - src/security/sanitizer.py
autonomous: true

must_haves:
  truths:
    - "Detected secrets are replaced with typed placeholders"
    - "Placeholder format is [REDACTED:type] per user decision"
    - "Sanitized content is always stored (storage never blocked)"
    - "Original content is preserved in SanitizationResult for debugging"
    - "All sanitization events are audit logged"
  artifacts:
    - path: "src/security/sanitizer.py"
      provides: "Content sanitization with typed placeholder masking"
      exports: ["ContentSanitizer", "sanitize_content"]
      min_lines: 80
  key_links:
    - from: "src/security/sanitizer.py"
      to: "src/security/detector.py"
      via: "uses SecretDetector.detect()"
      pattern: "from src.security.detector import"
    - from: "src/security/sanitizer.py"
      to: "src/security/allowlist.py"
      via: "checks Allowlist before masking"
      pattern: "from src.security.allowlist import"
    - from: "src/security/sanitizer.py"
      to: "src/security/audit.py"
      via: "logs all sanitization events"
      pattern: "log_sanitization_event"
---

<objective>
Implement content sanitizer that masks detected secrets with typed placeholders.

Purpose: The sanitizer is the integration point that combines detection, allowlist checking, masking, and audit logging. It produces sanitized content safe for storage in knowledge graphs.

Output: ContentSanitizer class that takes raw content and returns SanitizationResult with sanitized content and detection metadata.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-security-filtering/02-CONTEXT.md
@.planning/phases/02-security-filtering/02-RESEARCH.md
@src/models/security.py
@src/config/security.py
@src/security/detector.py
@src/security/allowlist.py
@src/security/audit.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create content sanitizer</name>
  <files>src/security/sanitizer.py</files>
  <action>
Create src/security/sanitizer.py:

```python
"""Content sanitization with typed placeholder masking.

Combines secret detection, allowlist checking, and masking
to produce content safe for knowledge graph storage.

User decisions:
- Storage never blocked: Always return sanitized content
- Typed placeholders: [REDACTED:type] format
- Very aggressive: Prefer false positives over leaking secrets
"""
from pathlib import Path
from typing import Optional

from src.config.security import REDACTION_PLACEHOLDER
from src.models.security import (
    SecretFinding,
    SanitizationResult,
    DetectionType,
)
from src.security.detector import SecretDetector
from src.security.allowlist import Allowlist
from src.security.audit import get_audit_logger, log_sanitization_event


# Map DetectionType to placeholder suffix
PLACEHOLDER_TYPE_MAP: dict[DetectionType, str] = {
    DetectionType.AWS_KEY: "aws_key",
    DetectionType.GITHUB_TOKEN: "github_token",
    DetectionType.JWT: "jwt",
    DetectionType.GENERIC_API_KEY: "api_key",
    DetectionType.PRIVATE_KEY: "private_key",
    DetectionType.CONNECTION_STRING: "connection_string",
    DetectionType.HIGH_ENTROPY_BASE64: "high_entropy",
    DetectionType.HIGH_ENTROPY_HEX: "high_entropy",
}


def get_placeholder(detection_type: DetectionType) -> str:
    """Get placeholder string for a detection type.

    Args:
        detection_type: Type of secret detected

    Returns:
        Placeholder string like [REDACTED:aws_key]
    """
    type_suffix = PLACEHOLDER_TYPE_MAP.get(detection_type, detection_type.name.lower())
    return REDACTION_PLACEHOLDER.format(type=type_suffix)


class ContentSanitizer:
    """Sanitizes content by detecting and masking secrets.

    Integrates:
    - SecretDetector for finding secrets
    - Allowlist for false positive overrides
    - Audit logging for traceability
    """

    def __init__(
        self,
        project_root: Path | None = None,
        enable_allowlist: bool = True,
    ):
        """Initialize sanitizer.

        Args:
            project_root: Project root for allowlist location
            enable_allowlist: Whether to check allowlist (default True)
        """
        self._detector = SecretDetector()
        self._allowlist = Allowlist(project_root) if enable_allowlist else None
        self._audit = get_audit_logger()

    def sanitize(
        self,
        content: str,
        file_path: str | None = None,
    ) -> SanitizationResult:
        """Sanitize content by masking detected secrets.

        IMPORTANT: Storage never blocked. This method always returns
        sanitized content, even if secrets are found.

        Args:
            content: Raw content to sanitize
            file_path: Optional source file path for audit logging

        Returns:
            SanitizationResult with sanitized content and metadata
        """
        # Step 1: Detect secrets
        all_findings = self._detector.detect(content, file_path)

        # Step 2: Filter out allowlisted findings
        findings_to_mask: list[SecretFinding] = []
        allowlisted_count = 0

        for finding in all_findings:
            if self._allowlist and self._allowlist.is_allowed(finding.matched_text):
                # Allowlisted - log but don't mask
                allowlisted_count += 1
                entry = self._allowlist.get_entry(finding.matched_text)
                self._audit.log_allowlist_check(
                    finding_hash=Allowlist.compute_hash(finding.matched_text),
                    was_allowed=True,
                    comment=entry.comment if entry else None,
                )
            else:
                findings_to_mask.append(finding)

        # Step 3: Mask secrets in content
        sanitized = content
        for finding in findings_to_mask:
            placeholder = get_placeholder(finding.detection_type)

            # Replace the matched text with placeholder
            sanitized = sanitized.replace(finding.matched_text, placeholder)

            # Log the sanitization event
            log_sanitization_event(
                finding=finding,
                action="masked",
                placeholder=placeholder,
            )

        # Step 4: Build result
        # Note: findings_to_mask contains what was actually masked
        # allowlisted ones are counted but not in the list
        return SanitizationResult(
            original_content=content,
            sanitized_content=sanitized,
            findings=findings_to_mask,
            allowlisted_count=allowlisted_count,
        )


def sanitize_content(
    content: str,
    file_path: str | None = None,
    project_root: Path | None = None,
) -> SanitizationResult:
    """Convenience function to sanitize content.

    Args:
        content: Raw content to sanitize
        file_path: Optional source file path
        project_root: Project root for allowlist

    Returns:
        SanitizationResult with sanitized content
    """
    sanitizer = ContentSanitizer(project_root)
    return sanitizer.sanitize(content, file_path)
```

Key implementation notes:
- NEVER blocks storage - always returns sanitized content
- Uses typed placeholders: [REDACTED:aws_key], [REDACTED:high_entropy], etc.
- Checks allowlist before masking
- Logs all events (masked and allowlisted)
- Preserves original content in result for debugging
  </action>
  <verify>
python -c "
from src.security.sanitizer import ContentSanitizer, sanitize_content, get_placeholder
from src.models.security import DetectionType

# Test placeholder generation
assert get_placeholder(DetectionType.AWS_KEY) == '[REDACTED:aws_key]'
assert get_placeholder(DetectionType.HIGH_ENTROPY_BASE64) == '[REDACTED:high_entropy]'
print('Placeholders OK')

# Test basic sanitization
content = '''
# Config file
AWS_ACCESS_KEY_ID = \"AKIAIOSFODNN7EXAMPLE\"
normal_value = \"hello world\"
'''

result = sanitize_content(content)
print(f'Found {len(result.findings)} secrets, allowlisted {result.allowlisted_count}')
print(f'Was modified: {result.was_modified}')

# Verify original preserved
assert result.original_content == content, 'Original should be preserved'

# Storage never blocked - result always has sanitized content
assert result.sanitized_content is not None, 'Should always have sanitized content'

print('Content sanitizer OK')
"
  </verify>
  <done>
ContentSanitizer masks secrets with typed placeholders. Allowlist checking works. Audit logging integrated. Storage never blocked - always returns sanitized content.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create integrated sanitization pipeline</name>
  <files>src/security/sanitizer.py, src/security/__init__.py</files>
  <action>
1. Add file-level exclusion integration to ContentSanitizer:

Add to src/security/sanitizer.py:

```python
from src.security.exclusions import is_excluded_file

class ContentSanitizer:
    # ... existing code ...

    def should_process_file(self, file_path: Path) -> bool:
        """Check if file should be processed through sanitizer.

        Excluded files should not even reach content sanitization -
        they are blocked at the file level.

        Args:
            file_path: Path to check

        Returns:
            True if file should be processed, False if excluded
        """
        if is_excluded_file(file_path):
            self._audit.log_file_excluded(file_path, "<default_exclusions>")
            return False
        return True

    def sanitize_file(self, file_path: Path) -> Optional[SanitizationResult]:
        """Sanitize content from a file.

        Checks file exclusions first, then sanitizes content.

        Args:
            file_path: Path to file to sanitize

        Returns:
            SanitizationResult if processed, None if file excluded
        """
        if not self.should_process_file(file_path):
            return None

        content = file_path.read_text()
        return self.sanitize(content, str(file_path))
```

2. Update src/security/__init__.py to provide clean public API:

```python
"""Security filtering for knowledge graphs.

Defense-in-depth security filtering to prevent secrets and PII
from entering knowledge graphs. Content is sanitized before
storage - secrets are detected and masked with placeholders.

Usage:
    from src.security import sanitize_content, ContentSanitizer

    # Simple usage
    result = sanitize_content("my content with API_KEY=secret123")
    safe_content = result.sanitized_content

    # With file processing
    sanitizer = ContentSanitizer()
    if sanitizer.should_process_file(Path("myfile.py")):
        result = sanitizer.sanitize_file(Path("myfile.py"))
"""
from src.security.exclusions import FileExcluder, is_excluded_file
from src.security.audit import (
    SecurityAuditLogger,
    get_audit_logger,
    log_sanitization_event,
)
from src.security.detector import SecretDetector, detect_secrets_in_content
from src.security.patterns import get_detection_type, get_confidence
from src.security.allowlist import Allowlist, is_allowlisted
from src.security.sanitizer import (
    ContentSanitizer,
    sanitize_content,
    get_placeholder,
)

__all__ = [
    # File exclusions
    "FileExcluder",
    "is_excluded_file",

    # Audit logging
    "SecurityAuditLogger",
    "get_audit_logger",
    "log_sanitization_event",

    # Secret detection
    "SecretDetector",
    "detect_secrets_in_content",
    "get_detection_type",
    "get_confidence",

    # Allowlist
    "Allowlist",
    "is_allowlisted",

    # Sanitization (main entry point)
    "ContentSanitizer",
    "sanitize_content",
    "get_placeholder",
]
```
  </action>
  <verify>
python -c "
from pathlib import Path
import tempfile

# Test clean imports from package
from src.security import (
    sanitize_content,
    ContentSanitizer,
    is_excluded_file,
    Allowlist,
)

# Test file exclusion integration
sanitizer = ContentSanitizer()

# .env should be excluded
assert not sanitizer.should_process_file(Path('.env')), '.env should be excluded'
assert not sanitizer.should_process_file(Path('secrets.json')), 'secrets.json should be excluded'

# Normal files should be processed
assert sanitizer.should_process_file(Path('main.py')), 'main.py should be processed'

# Test full pipeline
with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
    f.write('API_KEY = \"sk-12345678901234567890123456789012\"')
    f.flush()
    temp_path = Path(f.name)

try:
    result = sanitizer.sanitize_file(temp_path)
    assert result is not None, 'Should process .py file'
    print(f'Sanitized: {result.sanitized_content[:50]}...')
finally:
    temp_path.unlink()

print('Integrated pipeline OK')
"
  </verify>
  <done>
Complete sanitization pipeline working:
- File exclusions checked first (excluded files return None)
- Content sanitization with typed placeholders
- Allowlist integration
- Audit logging throughout
- Clean public API from src.security package
  </done>
</task>

</tasks>

<verification>
- [ ] `from src.security import sanitize_content, ContentSanitizer` works
- [ ] Excluded files (*.env, *secret*) return None from sanitize_file
- [ ] Normal files return SanitizationResult with sanitized content
- [ ] Detected secrets replaced with [REDACTED:type] placeholders
- [ ] Allowlisted items are not masked
- [ ] All events logged to audit.log
</verification>

<success_criteria>
Plan complete when:
1. ContentSanitizer integrates detection, allowlist, and masking
2. File exclusions work before content processing
3. Typed placeholders used: [REDACTED:aws_key], [REDACTED:high_entropy], etc.
4. Storage never blocked - always returns sanitized content or None for excluded files
5. All exports work from src.security package
</success_criteria>

<output>
After completion, create `.planning/phases/02-security-filtering/02-04-SUMMARY.md`
</output>
