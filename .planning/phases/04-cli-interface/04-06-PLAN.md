---
phase: 04-cli-interface
plan: 06
type: execute
wave: 3
depends_on: ["04-02", "04-03", "04-04", "04-05"]
files_modified:
  - tests/test_cli_foundation.py
  - tests/test_cli_commands.py
  - src/cli/__init__.py
autonomous: false

must_haves:
  truths:
    - "All 9 commands execute without import errors or crashes"
    - "Exit codes are correct (0 success, 1 runtime, 2 bad args)"
    - "JSON output mode produces valid parseable JSON"
    - "Typo suggestions work for misspelled commands"
    - "Confirmation prompts block without --force"
    - "stdin piping works for add command"
    - "Scope auto-detection works (project in git repo, global otherwise)"
    - "Help text includes examples for every command"
  artifacts:
    - path: "tests/test_cli_foundation.py"
      provides: "Tests for app instance, output module, input handling, utils"
      contains: "CliRunner"
    - path: "tests/test_cli_commands.py"
      provides: "Tests for all 9 command handlers"
      contains: "CliRunner"
  key_links:
    - from: "tests/test_cli_foundation.py"
      to: "src/cli/__init__.py"
      via: "Tests app via CliRunner"
      pattern: "CliRunner.*invoke.*app"
    - from: "tests/test_cli_commands.py"
      to: "src/cli/__init__.py"
      via: "Tests each command via CliRunner"
      pattern: "invoke.*app.*\\[.*\\]"
---

<objective>
Create comprehensive test suite for the CLI using Typer's CliRunner, then verify the complete CLI works end-to-end with a human verification checkpoint.

Purpose: Tests ensure all commands work correctly, exit codes are proper, output formats are correct, and edge cases (no args, bad args, piped input) are handled. The checkpoint verifies the actual terminal experience.

Output: Test suite passing, human-verified CLI interaction.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-cli-interface/04-CONTEXT.md
@.planning/phases/04-cli-interface/04-01-SUMMARY.md
@.planning/phases/04-cli-interface/04-02-SUMMARY.md
@.planning/phases/04-cli-interface/04-03-SUMMARY.md
@.planning/phases/04-cli-interface/04-04-SUMMARY.md
@.planning/phases/04-cli-interface/04-05-SUMMARY.md
@src/cli/__init__.py
@src/cli/output.py
@src/cli/input.py
@src/cli/utils.py
@src/cli/commands/add.py
@src/cli/commands/search.py
@src/cli/commands/list_cmd.py
@src/cli/commands/show.py
@src/cli/commands/delete.py
@src/cli/commands/summarize.py
@src/cli/commands/compact.py
@src/cli/commands/config.py
@src/cli/commands/health.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CLI test suite</name>
  <files>
    tests/test_cli_foundation.py
    tests/test_cli_commands.py
  </files>
  <action>
Create `tests/test_cli_foundation.py` testing the CLI infrastructure:

Use `from typer.testing import CliRunner` and `from src.cli import app`.

**App tests:**
- `test_app_help()`: invoke with `["--help"]`, assert exit_code==0, assert "graphiti" in output, assert all 9 command names appear in help text.
- `test_app_no_args_shows_help()`: invoke with `[]`, assert help text shown (no_args_is_help=True).
- `test_app_version()`: invoke with `["--version"]`, assert exit_code==0, assert version string in output.

**Utils tests:**
- `test_suggest_command_search()`: assert suggest_command("serch") == "search".
- `test_suggest_command_delete()`: assert suggest_command("delte") == "delete".
- `test_suggest_command_no_match()`: assert suggest_command("xyz") is None.
- `test_exit_codes()`: assert EXIT_SUCCESS==0, EXIT_ERROR==1, EXIT_BAD_ARGS==2.
- `test_resolve_scope_global()`: resolve_scope(global_flag=True) returns (GraphScope.GLOBAL, None).
- `test_resolve_scope_both_flags_error()`: resolve_scope(global_flag=True, project_flag=True) raises typer.BadParameter.

**Input tests:**
- `test_read_content_positional()`: read_content("hello") returns "hello".
- `test_read_content_none_tty_raises()`: Mock sys.stdin.isatty() to return True, read_content(None) raises typer.BadParameter.

**Output tests:**
- `test_console_exists()`: from src.cli.output import console; assert console is not None.
- `test_print_success_runs()`: call print_success("test") -- just assert no exception.
- `test_print_error_runs()`: call print_error("test") -- assert no exception.

Create `tests/test_cli_commands.py` testing all 9 commands:

**Add command tests:**
- `test_add_with_content()`: invoke ["add", "test content"], assert exit_code==0, assert "Added" or success indicator in output.
- `test_add_no_content_no_pipe()`: invoke ["add"], assert exit_code==2 (bad args).
- `test_add_from_stdin()`: invoke ["add"] with input="piped content\n", assert exit_code==0.
- `test_add_json_format()`: invoke ["add", "test", "--format", "json"], assert exit_code==0, assert output is valid JSON (json.loads doesn't throw).
- `test_add_quiet()`: invoke ["add", "test", "--quiet"], assert exit_code==0, assert output is empty or minimal.

**Search command tests:**
- `test_search_basic()`: invoke ["search", "test query"], assert exit_code==0, assert results in output.
- `test_search_json()`: invoke ["search", "test", "--format", "json"], assert valid JSON output.
- `test_search_compact()`: invoke ["search", "test", "--compact"], assert exit_code==0.
- `test_search_exact()`: invoke ["search", "test", "--exact"], assert exit_code==0.
- `test_search_with_limit()`: invoke ["search", "test", "--limit", "5"], assert exit_code==0.

**List command tests:**
- `test_list_basic()`: invoke ["list"], assert exit_code==0.
- `test_list_json()`: invoke ["list", "--format", "json"], assert valid JSON.
- `test_list_compact()`: invoke ["list", "--compact"], assert exit_code==0.

**Show command tests:**
- `test_show_entity()`: invoke ["show", "test_entity"], assert exit_code==0 or 1 (depends on mock).

**Delete command tests:**
- `test_delete_with_force()`: invoke ["delete", "entity1", "--force"], assert exit_code==0.
- `test_delete_confirmation_declined()`: invoke ["delete", "entity1"] with input="n\n", assert exit_code==0, assert "Cancelled" in output.
- `test_delete_json()`: invoke ["delete", "entity1", "--force", "--format", "json"], assert valid JSON.

**Summarize command tests:**
- `test_summarize_whole_graph()`: invoke ["summarize"], assert exit_code==0.
- `test_summarize_topic()`: invoke ["summarize", "architecture"], assert exit_code==0.
- `test_summarize_json()`: invoke ["summarize", "--format", "json"], assert valid JSON.

**Compact command tests:**
- `test_compact_with_force()`: invoke ["compact", "--force"], assert exit_code==0.
- `test_compact_declined()`: invoke ["compact"] with input="n\n", assert "Cancelled" in output.

**Config command tests:**
- `test_config_show_all()`: invoke ["config"], assert exit_code==0, assert table-like output with config keys.
- `test_config_get_key()`: invoke ["config", "--get", "cloud.endpoint"], assert exit_code==0.
- `test_config_invalid_key()`: invoke ["config", "--get", "invalid.key"], assert exit_code==2.
- `test_config_json()`: invoke ["config", "--format", "json"], assert valid JSON.

**Health command tests:**
- `test_health_basic()`: invoke ["health"], assert exit_code in (0, 1) (depends on actual services).
- `test_health_verbose()`: invoke ["health", "--verbose"], assert exit_code in (0, 1).
- `test_health_json()`: invoke ["health", "--format", "json"], assert valid JSON.

Also update `src/cli/__init__.py` if needed to add `--help-all` flag support: when `--help-all` is passed, show comprehensive help with examples for each command. This can be a simple callback that prints extended help text. Add `--version` callback that prints the version from pyproject.toml metadata.

Run all tests: `python -m pytest tests/test_cli_foundation.py tests/test_cli_commands.py -v`
  </action>
  <verify>
Run: `python -m pytest tests/test_cli_foundation.py tests/test_cli_commands.py -v` should pass with all tests green.
Count: at least 30 tests should exist across both files.
  </verify>
  <done>
Test suite covers: app help/version, all 9 commands (basic + JSON + edge cases), utils (typo suggestions, scope resolution, exit codes), input handling, output functions. All tests pass.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify CLI works in terminal</name>
  <what-built>Complete CLI with 9 commands (add, search, list, show, delete, summarize, compact, config, health), Rich output, JSON mode, scope detection, confirmations, typo suggestions, and comprehensive test suite.</what-built>
  <how-to-verify>
1. Install the package in dev mode: `pip install -e .` (from project root)
2. Run: `graphiti --help` -- verify all 9 commands listed with descriptions
3. Run: `gk --help` -- verify alias works identically
4. Run: `graphiti add "Test knowledge entry"` -- verify success message with colored output
5. Run: `graphiti add "JSON test" --format json` -- verify valid JSON output
6. Run: `echo "piped content" | graphiti add` -- verify stdin works
7. Run: `graphiti search "test"` -- verify formatted results table
8. Run: `graphiti search "test" --compact` -- verify one-line-per-result
9. Run: `graphiti list` -- verify entity table
10. Run: `graphiti config` -- verify settings table with masked API key
11. Run: `graphiti health` -- verify pass/fail for each component
12. Run: `graphiti health --verbose` -- verify expanded diagnostics
13. Run: `graphiti delete nonexistent --force` -- verify error handling
14. Run: `graphiti serch "test"` -- verify "Did you mean 'search'?" suggestion (if typo handling in callback)
15. Run: `python -m pytest tests/test_cli_foundation.py tests/test_cli_commands.py -v` -- verify all tests pass
  </how-to-verify>
  <resume-signal>Type "approved" or describe any issues to fix</resume-signal>
</task>

</tasks>

<verification>
- All pytest tests pass
- `graphiti --help` shows all 9 commands
- `gk` alias works
- JSON output mode produces valid JSON for all commands
- Colored output in terminal, plain when piped
- Confirmation prompts work for destructive commands
- Exit codes are correct (tested via `echo $?`)
</verification>

<success_criteria>
1. At least 30 CLI tests pass covering all commands and edge cases
2. Human verifies terminal experience (colors, formatting, help text)
3. Both `graphiti` and `gk` entry points work
4. JSON output parseable with `jq` or similar
5. Piped input works for add command
6. Health check provides useful diagnostics
7. Config shows and modifies settings
8. Overall CLI feels polished and professional
</success_criteria>

<output>
After completion, create `.planning/phases/04-cli-interface/04-06-SUMMARY.md`
</output>
