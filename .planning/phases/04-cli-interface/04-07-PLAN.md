---
phase: 04-cli-interface
plan: 07
type: execute
wave: 1
depends_on: []
files_modified:
  - src/graph/__init__.py
  - src/graph/adapters.py
  - src/graph/service.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "OllamaLLMClient adapter passes graphiti_core's LLMClient interface contract"
    - "OllamaEmbedder adapter produces embeddings via our embed() function"
    - "GraphService initializes Graphiti with correct driver, LLM adapter, and embedder adapter per scope"
    - "GraphService exposes add, search, list, get, delete, summarize, compact, and stats methods"
  artifacts:
    - path: "src/graph/__init__.py"
      provides: "Package exports for GraphService, adapters"
      contains: "GraphService"
    - path: "src/graph/adapters.py"
      provides: "OllamaLLMClient and OllamaEmbedder adapters bridging our LLM to graphiti_core interfaces"
      exports: ["OllamaLLMClient", "OllamaEmbedder"]
    - path: "src/graph/service.py"
      provides: "High-level GraphService wrapping Graphiti with per-scope initialization"
      exports: ["GraphService"]
  key_links:
    - from: "src/graph/adapters.py"
      to: "src/llm"
      via: "import chat, embed, generate from src.llm"
      pattern: "from src\\.llm import"
    - from: "src/graph/adapters.py"
      to: "graphiti_core.llm_client.client.LLMClient"
      via: "OllamaLLMClient extends LLMClient ABC"
      pattern: "class OllamaLLMClient\\(LLMClient\\)"
    - from: "src/graph/adapters.py"
      to: "graphiti_core.embedder.client.EmbedderClient"
      via: "OllamaEmbedder extends EmbedderClient ABC"
      pattern: "class OllamaEmbedder\\(EmbedderClient\\)"
    - from: "src/graph/service.py"
      to: "graphiti_core.Graphiti"
      via: "GraphService creates Graphiti instances with adapters and KuzuDriver"
      pattern: "Graphiti\\("
    - from: "src/graph/service.py"
      to: "src/storage/graph_manager.py"
      via: "Uses GraphManager.get_driver() for KuzuDriver per scope"
      pattern: "GraphManager"
---

<objective>
Create adapter layer and GraphService to bridge the gap between our OllamaClient/GraphManager and graphiti_core's Graphiti class.

Purpose: This is the foundation that all 7 CLI command wiring tasks depend on. Without adapters that implement graphiti_core's LLMClient and EmbedderClient interfaces, Graphiti cannot be instantiated. Without GraphService, commands have no high-level API to call.

Output: src/graph/ package with adapters.py (OllamaLLMClient, OllamaEmbedder) and service.py (GraphService)
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/llm/__init__.py
@src/llm/client.py
@src/storage/graph_manager.py
@src/storage/__init__.py
@src/models/context.py
@src/config/paths.py
@src/security/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create graphiti_core adapter classes for our Ollama LLM and embedder</name>
  <files>src/graph/__init__.py, src/graph/adapters.py</files>
  <action>
Create `src/graph/` package with `__init__.py` and `adapters.py`.

In `adapters.py`, create two adapter classes:

**OllamaLLMClient(LLMClient)** - Adapts our src.llm OllamaClient to graphiti_core's abstract LLMClient interface:
- Import `from graphiti_core.llm_client.client import LLMClient` and `from graphiti_core.llm_client.config import LLMConfig as GraphitiLLMConfig, ModelSize`
- Import `from graphiti_core.prompts.models import Message`
- Import `from src.llm import chat as ollama_chat`
- Constructor: accepts no required args. Create a GraphitiLLMConfig with model=None (we route through our own client). Call `super().__init__(config)`.
- Implement `async def _generate_response(self, messages: list[Message], response_model: type[BaseModel] | None = None, max_tokens: int = 8192, model_size: ModelSize = ModelSize.medium) -> dict[str, Any]`:
  - Convert Message objects to dicts: `[{"role": m.role, "content": m.content} for m in messages]`
  - Call `ollama_chat(message_dicts)` (synchronous - our OllamaClient is sync)
  - Extract response content from the ollama response dict (response["message"]["content"])
  - If response_model is provided, parse the content as JSON and validate against the model, returning the dict
  - If no response_model, return `{"content": response_text}`
  - Handle errors gracefully - wrap in try/except, log failures

IMPORTANT: graphiti_core's LLMClient._generate_response is async but our ollama client is sync. Use `asyncio.get_event_loop().run_in_executor(None, lambda: ollama_chat(...))` to avoid blocking the event loop. Actually, since graphiti_core internally calls this from async context, and our client does blocking I/O, wrapping in run_in_executor is the correct approach.

**OllamaEmbedder(EmbedderClient)** - Adapts our src.llm.embed() to graphiti_core's abstract EmbedderClient interface:
- Import `from graphiti_core.embedder.client import EmbedderClient`
- Import `from src.llm import embed as ollama_embed`
- Implement `async def create(self, input_data: str | list[str] | Iterable[int] | Iterable[Iterable[int]]) -> list[float]`:
  - If input_data is a string, call `ollama_embed(input_data)` via run_in_executor
  - The ollama embed response has structure: `{"embeddings": [[float, ...]]}` - extract first embedding
  - Return the list[float] embedding vector
  - If input_data is a list of strings, embed the first one (graphiti_core calls create() with single strings for node/edge embeddings)

In `__init__.py`, export: OllamaLLMClient, OllamaEmbedder, and GraphService (from service.py, added in Task 2).

Note: Do NOT use jsonwebtoken or any OpenAI-specific libraries. The adapters must work purely through our existing src.llm module which handles cloud/local failover automatically.
  </action>
  <verify>
Run: `cd /home/tasostilsi/Development/Projects/graphiti-knowledge-graph && python -c "from src.graph.adapters import OllamaLLMClient, OllamaEmbedder; print('Adapters import OK'); llm = OllamaLLMClient(); print(f'LLMClient created: {type(llm).__mro__}'); emb = OllamaEmbedder(); print(f'EmbedderClient created: {type(emb).__mro__}')"` should show both classes inherit from the correct graphiti_core base classes.
  </verify>
  <done>OllamaLLMClient inherits from graphiti_core.llm_client.client.LLMClient with _generate_response implemented. OllamaEmbedder inherits from graphiti_core.embedder.client.EmbedderClient with create() implemented. Both route through src.llm convenience API.</done>
</task>

<task type="auto">
  <name>Task 2: Create GraphService wrapping Graphiti with per-scope initialization and high-level operations</name>
  <files>src/graph/service.py, src/graph/__init__.py</files>
  <action>
Create `src/graph/service.py` with a `GraphService` class that provides the high-level API all CLI commands will call.

**GraphService class:**

- Import: `from graphiti_core import Graphiti`, `from graphiti_core.nodes import EpisodeType, EntityNode, EpisodicNode`, `from graphiti_core.edges import EntityEdge`, `from graphiti_core.search.search_config import SearchResults`
- Import: `from src.graph.adapters import OllamaLLMClient, OllamaEmbedder`
- Import: `from src.storage import GraphManager`, `from src.models import GraphScope`, `from src.config.paths import get_project_db_path, GLOBAL_DB_PATH`
- Import: `from src.security import sanitize_content`
- Import: `from src.llm import chat as ollama_chat, LLMUnavailableError`
- Import: `import asyncio, logging`; `from datetime import datetime`; `from pathlib import Path`; `from typing import Optional`

**Singleton pattern** (like GraphManager):
- Module-level `_service: GraphService | None = None`
- `def get_service() -> GraphService` function

**Constructor:**
- Creates `GraphManager()` instance
- Creates adapter instances: `OllamaLLMClient()`, `OllamaEmbedder()`
- Stores adapters for reuse across scopes
- `_graphiti_instances: dict[str, Graphiti] = {}` cache keyed by scope identifier

**_get_graphiti(scope: GraphScope, project_root: Optional[Path] = None) -> Graphiti:**
- Compute cache key: "global" or f"project:{project_root}"
- If not cached, get KuzuDriver via `self._graph_manager.get_driver(scope, project_root)`
- Create `Graphiti(graph_driver=driver, llm_client=self._llm_client, embedder=self._embedder)` - pass cross_encoder=None (we skip reranking for local Kuzu setup)
- Cache and return

**async def add(content: str, scope: GraphScope, project_root: Optional[Path], tags: Optional[list[str]], source: str) -> dict:**
- Sanitize content via `sanitize_content(content)` - use sanitized_content if secrets found
- Get Graphiti instance via `_get_graphiti(scope, project_root)`
- Determine group_id from scope (use "global" for GLOBAL, or project_root name for PROJECT)
- Call `await graphiti.add_episode(name=f"cli_add_{datetime.now().strftime('%Y%m%d_%H%M%S')}", episode_body=sanitized_content, source_description=source, reference_time=datetime.now(), source=EpisodeType.text, group_id=group_id)`
- Return dict with: name (from first extracted node or episode name), type, scope, created_at, tags, source, content_length, nodes_created (count), edges_created (count)

**async def search(query: str, scope: GraphScope, project_root: Optional[Path], exact: bool, limit: int) -> list[dict]:**
- Get Graphiti instance
- Determine group_id
- If not exact (semantic search): call `await graphiti.search(query, group_ids=[group_id], num_results=limit)`
- Returns list[EntityEdge] - convert each to dict: name (edge.name or edge.fact), type ("relationship"), snippet (edge.fact truncated), score (0.0 - no score from basic search), created_at, scope, tags
- If exact: use raw Kuzu query via driver.execute_query to do MATCH with string CONTAINS on Entity.name or Entity.summary
- Return list of result dicts

**async def list_entities(scope: GraphScope, project_root: Optional[Path], limit: Optional[int]) -> list[dict]:**
- Get driver via GraphManager (direct Kuzu query is simpler for listing)
- Run Cypher: `MATCH (n:Entity) WHERE n.group_id = $group_id RETURN n.uuid, n.name, n.labels, n.created_at, n.summary ORDER BY n.created_at DESC` with LIMIT
- Convert results to list of dicts with: name, type (first label or "entity"), created_at, tags (labels joined), scope, relationship_count (separate COUNT query or 0)

**async def get_entity(name: str, scope: GraphScope, project_root: Optional[Path]) -> dict | list[dict] | None:**
- Query: `MATCH (n:Entity) WHERE n.group_id = $group_id AND n.name CONTAINS $name RETURN n`
- If 0 results: return None
- If 1 result: return full entity dict with relationships (query RELATES_TO edges)
- If multiple: return list of brief dicts for disambiguation

**async def delete_entities(names: list[str], scope: GraphScope, project_root: Optional[Path]) -> int:**
- For each name, find matching Entity nodes by name
- Get associated episodes via MENTIONS edges
- Call `await graphiti.remove_episode(episode_uuid)` for each associated episode
- Or use direct Kuzu deletion: `MATCH (n:Entity {uuid: $uuid}) DETACH DELETE n`
- Return count of deleted entities

**async def summarize(scope: GraphScope, project_root: Optional[Path], topic: Optional[str]) -> tuple[str, int]:**
- List entities (with topic filter if provided - filter by name/summary CONTAINS topic)
- Build prompt: list entity names and summaries, ask LLM to summarize
- Call `ollama_chat([{"role": "user", "content": prompt}])` via run_in_executor
- Return (summary_text, entity_count)

**async def compact(scope: GraphScope, project_root: Optional[Path]) -> dict:**
- Get stats first (entity count, edge count, DB size)
- For actual compaction: graphiti_core doesn't have a built-in compact. Do a dedup pass:
  - Find duplicate entities (same name, same group_id)
  - Merge by keeping the one with most relationships
  - Delete orphaned RelatesToNode_ entries
- Return dict: merged_count, removed_count, new_entity_count, new_size_bytes

**async def get_stats(scope: GraphScope, project_root: Optional[Path]) -> dict:**
- Run Kuzu queries: COUNT Entity nodes, COUNT RelatesToNode_ edges, get DB file size
- Return dict: entity_count, relationship_count, duplicate_count, size_bytes

**Helper _run_sync(fn, *args):**
- Wraps sync functions for async context: `await asyncio.get_event_loop().run_in_executor(None, fn, *args)`

**Helper _get_group_id(scope, project_root):**
- Returns "global" for GLOBAL scope, or project_root.name for PROJECT scope

**Convenience wrapper:**
- `def run_graph_operation(coro)` - calls `asyncio.run(coro)` for use from sync CLI context. Handle event loop already running case with `nest_asyncio` or create new loop.

Update `src/graph/__init__.py` to export GraphService, get_service, run_graph_operation, and the adapter classes.
  </action>
  <verify>
Run: `cd /home/tasostilsi/Development/Projects/graphiti-knowledge-graph && python -c "from src.graph import GraphService, get_service, run_graph_operation; print('GraphService imports OK'); svc = GraphService(); print(f'Service created with methods: {[m for m in dir(svc) if not m.startswith(\"_\")]}')"` should show the service with add, search, list_entities, get_entity, delete_entities, summarize, compact, get_stats methods.
  </verify>
  <done>GraphService class exists with all 8 operation methods (add, search, list_entities, get_entity, delete_entities, summarize, compact, get_stats). Each method properly initializes Graphiti per-scope, routes through adapters to our LLM/storage layers, and returns structured dicts. run_graph_operation helper allows sync CLI code to call async graph operations.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.graph import GraphService, OllamaLLMClient, OllamaEmbedder"` succeeds
2. `python -c "from src.graph.adapters import OllamaLLMClient; from graphiti_core.llm_client.client import LLMClient; assert issubclass(OllamaLLMClient, LLMClient)"` passes
3. `python -c "from src.graph.adapters import OllamaEmbedder; from graphiti_core.embedder.client import EmbedderClient; assert issubclass(OllamaEmbedder, EmbedderClient)"` passes
4. `python -c "from src.graph.service import GraphService; svc = GraphService(); print([m for m in dir(svc) if not m.startswith('_')])"` shows all 8 methods
</verification>

<success_criteria>
- OllamaLLMClient properly inherits from graphiti_core LLMClient with _generate_response implemented
- OllamaEmbedder properly inherits from graphiti_core EmbedderClient with create() implemented
- Both adapters route through src.llm (not direct OpenAI/external calls)
- GraphService creates Graphiti instances with correct driver + adapters per scope
- GraphService exposes all 8 operation methods needed by CLI commands
- All imports succeed without errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-cli-interface/04-07-SUMMARY.md`
</output>
