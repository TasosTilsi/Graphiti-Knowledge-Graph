---
phase: 03-llm-integration
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/llm/quota.py
  - src/llm/queue.py
autonomous: true

must_haves:
  truths:
    - "Quota usage is tracked (via headers or local counting)"
    - "Warning logged when quota exceeds 80% threshold"
    - "Failed requests are queued for later retry"
    - "Queued requests include ID for tracking"
    - "Queue is bounded and respects TTL"
  artifacts:
    - path: "src/llm/quota.py"
      provides: "QuotaTracker for usage monitoring"
      exports: ["QuotaTracker"]
      min_lines: 50
    - path: "src/llm/queue.py"
      provides: "LLMRequestQueue for failed request persistence"
      exports: ["LLMRequestQueue"]
      min_lines: 80
  key_links:
    - from: "src/llm/quota.py"
      to: "structlog"
      via: "warning logging"
      pattern: "structlog|logger\\.warning"
    - from: "src/llm/queue.py"
      to: "persistqueue.SQLiteAckQueue"
      via: "queue storage"
      pattern: "SQLiteAckQueue"
    - from: "src/llm/queue.py"
      to: "src/llm/config.py"
      via: "queue configuration"
      pattern: "from src\\.llm\\.config import|LLMConfig"
---

<objective>
Implement quota tracking and persistent request queue for failed LLM operations.

Purpose: Quota tracking warns users before hitting cloud limits. Request queue ensures failed operations aren't lost - they're queued for retry when service recovers. Together they provide resilience per CONTEXT.md decisions.
Output: QuotaTracker for monitoring cloud usage, LLMRequestQueue for persisting failed requests.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-integration/03-CONTEXT.md
@.planning/phases/03-llm-integration/03-RESEARCH.md
@.planning/phases/03-llm-integration/03-01-SUMMARY.md
@src/llm/config.py
@src/security/audit.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement QuotaTracker</name>
  <files>src/llm/quota.py</files>
  <action>
Create `src/llm/quota.py` with:

1. **Imports:**
   - structlog for logging
   - typing for type hints
   - dataclasses for QuotaInfo
   - time for timestamp tracking

2. **QuotaInfo dataclass:**
   ```python
   @dataclass
   class QuotaInfo:
       limit: int | None = None
       remaining: int | None = None
       reset_timestamp: float | None = None
       usage_percent: float | None = None
       source: str = "unknown"  # "headers" or "local_count"
   ```

3. **QuotaTracker class:**

   **__init__(self, warning_threshold: float = 0.8):**
   - Store warning_threshold
   - Initialize _requests_made: int = 0 (local counting fallback)
   - Initialize _last_quota_info: QuotaInfo | None = None
   - Initialize structlog logger

   **update_from_headers(self, headers: dict) -> QuotaInfo | None:**
   - Try to extract quota info from response headers
   - Check for common header names:
     - x-ratelimit-limit, x-ratelimit-remaining, x-ratelimit-reset
     - X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset
   - If found, create QuotaInfo with source="headers"
   - Calculate usage_percent = 1 - (remaining / limit)
   - Check threshold and log warning if exceeded
   - Store in _last_quota_info
   - Return QuotaInfo or None if headers not available

   **increment_local_count(self) -> None:**
   - Increment _requests_made
   - Log debug message with current count
   - Note: Local counting is fallback when headers unavailable

   **check_threshold(self, usage_percent: float) -> bool:**
   - If usage_percent >= warning_threshold:
     - Log warning: "Cloud quota at {percent}% - approaching limit"
     - Return True (threshold exceeded)
   - Return False

   **get_status(self) -> QuotaInfo:**
   - Return _last_quota_info if available
   - Otherwise return QuotaInfo with local_count data

   **reset(self) -> None:**
   - Reset _requests_made to 0
   - Clear _last_quota_info
   - Used when quota period resets

4. **Module-level logger setup:**
   - Use structlog.get_logger() pattern consistent with security/audit.py
  </action>
  <verify>
Run:
```python
from src.llm.quota import QuotaTracker, QuotaInfo
tracker = QuotaTracker(warning_threshold=0.8)

# Test header parsing
headers = {"x-ratelimit-limit": "1000", "x-ratelimit-remaining": "150"}
info = tracker.update_from_headers(headers)
print(f"Usage: {info.usage_percent:.1%}")  # Should print "Usage: 85.0%"

# Test local counting
tracker.increment_local_count()
print(f"Local count: {tracker._requests_made}")  # Should print "Local count: 1"
```
  </verify>
  <done>QuotaTracker monitors cloud usage via headers or local counting, logs warnings at threshold.</done>
</task>

<task type="auto">
  <name>Task 2: Implement LLMRequestQueue</name>
  <files>src/llm/queue.py</files>
  <action>
Create `src/llm/queue.py` with:

1. **Imports:**
   - persistqueue.SQLiteAckQueue
   - uuid for request IDs
   - time for timestamps
   - structlog for logging
   - Path from pathlib
   - dataclasses for QueuedRequest
   - LLMConfig from src.llm.config

2. **QueuedRequest dataclass:**
   ```python
   @dataclass
   class QueuedRequest:
       id: str
       operation: str  # "chat", "generate", "embed"
       params: dict
       timestamp: float
       original_error: str
   ```

3. **LLMRequestQueue class:**

   **__init__(self, config: LLMConfig, queue_path: Path | None = None):**
   - Store config
   - Default queue_path to ~/.graphiti/llm_queue
   - Create directory if not exists
   - Initialize SQLiteAckQueue(str(queue_path), auto_commit=True)
   - Initialize structlog logger
   - Track _max_size = config.queue_max_size
   - Track _item_ttl_seconds = config.queue_item_ttl_hours * 3600

   **enqueue(self, operation: str, params: dict, error: str) -> str:**
   - Check if queue size >= _max_size:
     - If so, log warning and skip oldest items until under limit
   - Generate request_id = str(uuid.uuid4())
   - Create QueuedRequest
   - Put as dict on queue: queue.put(asdict(request))
   - Log info: "Request queued for retry. ID: {request_id}"
   - Return request_id

   **get_pending_count(self) -> int:**
   - Return queue.qsize()

   **process_one(self, processor_fn) -> bool:**
   - Get item from queue (block=False)
   - If no item, return False
   - Check if item is stale (timestamp + ttl < now):
     - If stale, ack and skip, log debug message
     - Return True (to try next)
   - Try processor_fn(item['operation'], item['params']):
     - On success: ack item, log success, return True
     - On failure: nack item (returns to queue), raise

   **process_all(self, processor_fn) -> tuple[int, int]:**
   - Process items until queue empty or all fail
   - Return (success_count, failure_count)
   - Useful for CLI retry command

   **clear_stale(self) -> int:**
   - Remove all items older than TTL
   - Return count of removed items

   **get_queue_stats(self) -> dict:**
   - Return {
       "pending": queue.qsize(),
       "max_size": _max_size,
       "ttl_hours": config.queue_item_ttl_hours
     }

4. **Important notes:**
   - SQLiteAckQueue handles thread-safety
   - ack() removes item permanently, nack() returns to queue
   - Queue bounded to prevent disk fill (per RESEARCH.md pitfall 8)
   - TTL prevents processing ancient requests that may no longer be relevant
  </action>
  <verify>
Run:
```python
from src.llm.config import load_config
from src.llm.queue import LLMRequestQueue
import tempfile
from pathlib import Path

config = load_config()
with tempfile.TemporaryDirectory() as tmpdir:
    queue = LLMRequestQueue(config, Path(tmpdir) / "test_queue")

    # Test enqueue
    req_id = queue.enqueue("chat", {"model": "test", "messages": []}, "test error")
    print(f"Queued request: {req_id}")
    print(f"Pending: {queue.get_pending_count()}")  # Should print 1

    # Test stats
    stats = queue.get_queue_stats()
    print(f"Stats: {stats}")
```
  </verify>
  <done>LLMRequestQueue persists failed requests with SQLite, supports bounded size and TTL, provides retry processing.</done>
</task>

<task type="auto">
  <name>Task 3: Export quota and queue modules</name>
  <files>src/llm/__init__.py</files>
  <action>
Update `src/llm/__init__.py` to export:
- QuotaTracker, QuotaInfo from quota
- LLMRequestQueue, QueuedRequest from queue

Add to existing exports and update __all__ list.
  </action>
  <verify>Run `python -c "from src.llm import QuotaTracker, QuotaInfo, LLMRequestQueue, QueuedRequest; print('Quota and queue exports available')"` - should succeed</verify>
  <done>All quota and queue classes exported from src.llm module.</done>
</task>

</tasks>

<verification>
- [ ] QuotaTracker parses standard rate-limit headers
- [ ] QuotaTracker logs warning when threshold exceeded
- [ ] QuotaTracker falls back to local counting when headers unavailable
- [ ] LLMRequestQueue persists to SQLite file
- [ ] Queue bounded by max_size configuration
- [ ] Stale items (older than TTL) skipped during processing
- [ ] enqueue() returns unique request ID
- [ ] process_one() handles success/failure correctly
- [ ] All classes exported from src.llm module
</verification>

<success_criteria>
- QuotaTracker monitors cloud usage with header parsing + local counting fallback
- Warning logged at configurable threshold (default 80%)
- LLMRequestQueue persists failed requests for later retry
- Queue respects max_size and TTL constraints
- Request IDs enable tracking (per CONTEXT.md queue-and-raise pattern)
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-03-SUMMARY.md`
</output>
