---
phase: 03-llm-integration
plan: 05
type: execute
wave: 4
depends_on: ["03-04"]
files_modified:
  - tests/test_llm_config.py
  - tests/test_llm_client.py
  - tests/test_llm_quota.py
  - tests/test_llm_queue.py
  - tests/test_llm_integration.py
autonomous: false

must_haves:
  truths:
    - "Configuration loads from TOML with env var overrides"
    - "OllamaClient fails over from cloud to local on errors"
    - "Rate-limit (429) triggers 10-minute cooldown"
    - "Non-429 errors do NOT set cooldown; cloud tried on next request"
    - "QuotaTracker warns at threshold"
    - "LLMRequestQueue persists and retrieves requests"
    - "LLMUnavailableError message format matches CONTEXT.md spec"
    - "End-to-end flow works with mocked Ollama"
  artifacts:
    - path: "tests/test_llm_config.py"
      provides: "Config loading and override tests"
      min_lines: 50
    - path: "tests/test_llm_client.py"
      provides: "Client failover and retry tests"
      min_lines: 150
    - path: "tests/test_llm_quota.py"
      provides: "Quota tracking tests"
      min_lines: 40
    - path: "tests/test_llm_queue.py"
      provides: "Request queue tests"
      min_lines: 60
    - path: "tests/test_llm_integration.py"
      provides: "End-to-end integration tests"
      min_lines: 80
  key_links:
    - from: "tests/test_llm_client.py"
      to: "src/llm/client.py"
      via: "mock injection"
      pattern: "monkeypatch|Mock|patch"
---

<objective>
Create comprehensive test suite for LLM integration components.

Purpose: Verify all LLM components work correctly: config loading, client failover, quota tracking, request queue, and full integration. Tests use mocking to avoid requiring actual Ollama services.
Output: Complete test coverage for LLM module with passing tests.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-integration/03-CONTEXT.md
@.planning/phases/03-llm-integration/03-RESEARCH.md
@.planning/phases/03-llm-integration/03-04-SUMMARY.md
@src/llm/__init__.py
@src/llm/config.py
@src/llm/client.py
@src/llm/quota.py
@src/llm/queue.py
@tests/test_security_filtering.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create configuration tests</name>
  <files>tests/test_llm_config.py</files>
  <action>
Create `tests/test_llm_config.py` with tests:

1. **test_load_config_defaults:**
   - Load config with no file present
   - Verify all defaults match LLMConfig dataclass

2. **test_load_config_from_toml:**
   - Create temp TOML file with custom values
   - Load config from that file
   - Verify values are read correctly

3. **test_env_var_overrides:**
   - Set OLLAMA_API_KEY, OLLAMA_CLOUD_ENDPOINT env vars
   - Load config
   - Verify env vars override TOML values

4. **test_config_immutable:**
   - Load config
   - Attempt to modify (should raise FrozenInstanceError)

5. **test_get_state_path:**
   - Verify returns ~/.graphiti/llm_state.json

Use pytest fixtures:
- tmp_path for temp config files
- monkeypatch for env vars

Pattern from tests/test_security_filtering.py for monkeypatch usage.
  </action>
  <verify>Run `pytest tests/test_llm_config.py -v` - all tests should pass</verify>
  <done>Configuration loading tests verify TOML parsing, env overrides, and immutability.</done>
</task>

<task type="auto">
  <name>Task 2: Create client and quota/queue tests</name>
  <files>tests/test_llm_client.py, tests/test_llm_quota.py, tests/test_llm_queue.py</files>
  <action>
**Create `tests/test_llm_client.py`:**

1. **Fixtures:**
   - mock_config: LLMConfig with test values
   - mock_cloud_client: Mock ollama.Client for cloud
   - mock_local_client: Mock ollama.Client for local

2. **Tests:**
   - test_cloud_available_with_api_key: _is_cloud_available() True when key set
   - test_cloud_unavailable_without_api_key: _is_cloud_available() False when no key
   - test_cloud_unavailable_during_cooldown: _is_cloud_available() False in cooldown
   - test_retry_on_cloud_error: Verify tenacity retry happens before failover
   - test_rate_limit_triggers_cooldown: 429 error sets 10-min cooldown
   - test_non_rate_limit_error_no_cooldown: Non-429 error (e.g., 500, 502) does NOT set any cooldown. After a 500 error, _is_cloud_available() should still return True (assuming API key is set). Verify cloud_cooldown_until is NOT modified.
   - test_cloud_retried_after_non_429_error: After a non-429 cloud error and local fallback, the NEXT request should attempt cloud again (not skip to local). This verifies the CONTEXT.md decision that non-rate-limit errors allow immediate cloud retry.
   - test_cooldown_state_persisted: Cooldown saved to llm_state.json
   - test_cooldown_state_loaded: Cooldown restored on init
   - test_fallback_to_local: Cloud failure falls back to local
   - test_local_model_fallback_chain: Tries models in order
   - test_missing_model_error: Clear error with "ollama pull" instruction
   - test_current_provider_tracking: current_provider updates correctly
   - test_both_fail_raises_unavailable: LLMUnavailableError when all fail
   - test_llm_unavailable_error_message_no_id: LLMUnavailableError() produces "LLM unavailable. Request will be queued for retry."
   - test_llm_unavailable_error_message_with_id: LLMUnavailableError(request_id="abc-123") produces "LLM unavailable. Request queued for retry. ID: abc-123"
   - test_llm_unavailable_error_custom_message: LLMUnavailableError("custom msg") preserves custom message

Use unittest.mock.Mock and monkeypatch to mock ollama.Client.

**Create `tests/test_llm_quota.py`:**

1. **Tests:**
   - test_update_from_headers_standard: Parse x-ratelimit-* headers
   - test_update_from_headers_capitalized: Parse X-RateLimit-* headers
   - test_update_from_headers_missing: Returns None when headers missing
   - test_threshold_warning: Warning logged at threshold
   - test_local_counting_fallback: increment_local_count works
   - test_get_status: Returns QuotaInfo

**Create `tests/test_llm_queue.py`:**

1. **Tests:**
   - test_enqueue_returns_id: UUID returned on enqueue
   - test_get_pending_count: Count increases after enqueue
   - test_process_one_success: Item acked on success
   - test_process_one_failure: Item nacked on failure
   - test_stale_items_skipped: Items older than TTL skipped
   - test_queue_bounded: Max size respected
   - test_get_queue_stats: Stats reflect queue state
   - test_persistence: Queue survives reload (use temp path)

Use tmp_path fixture for queue storage.
  </action>
  <verify>Run `pytest tests/test_llm_client.py tests/test_llm_quota.py tests/test_llm_queue.py -v` - all tests should pass</verify>
  <done>Client tests verify failover, retry logic, non-429 no-cooldown behavior, and LLMUnavailableError message format. Quota tests verify tracking. Queue tests verify persistence.</done>
</task>

<task type="auto">
  <name>Task 3: Create integration tests</name>
  <files>tests/test_llm_integration.py</files>
  <action>
Create `tests/test_llm_integration.py` with end-to-end tests:

1. **Fixtures:**
   - mock_ollama: Patch ollama.Client globally for both cloud and local
   - test_config: LLMConfig with test values and tmp paths

2. **Integration tests:**

   **test_chat_cloud_success:**
   - Mock cloud client to return successful response
   - Call chat()
   - Verify cloud was used, quota tracked

   **test_chat_cloud_fail_local_success:**
   - Mock cloud to raise ResponseError
   - Mock local to return success
   - Call chat()
   - Verify failover happened, local used

   **test_chat_rate_limit_cooldown:**
   - Mock cloud to raise ResponseError(status_code=429)
   - Call chat() - should failover to local
   - Call chat() again immediately
   - Verify cloud NOT tried (in cooldown), local used directly

   **test_chat_non_429_error_cloud_retried:**
   - Mock cloud to raise ResponseError(status_code=500) on first call
   - Mock local to return success on first call (fallback)
   - Reset mocks, mock cloud to return success on second call
   - Call chat() again
   - Verify cloud IS tried on second call (no cooldown set for 500 errors)
   - This is the key integration test for CONTEXT.md: "Other errors: Try cloud again on very next request"

   **test_chat_both_fail_queued:**
   - Mock both cloud and local to fail
   - Call chat()
   - Verify LLMUnavailableError raised with request_id
   - Verify request in queue
   - Verify error message matches format: "LLM unavailable. Request queued for retry. ID: <uuid>"

   **test_embed_uses_nomic:**
   - Call embed()
   - Verify nomic-embed-text model used (from config)

   **test_get_status_complete:**
   - Call get_status()
   - Verify returns provider, quota, queue info

   **test_singleton_client:**
   - Call get_client() twice
   - Verify same instance returned

   **test_queue_retry_processing:**
   - Queue some failed requests
   - Mock client to succeed
   - Call process_queue()
   - Verify items processed and removed

3. **Use monkeypatch for:**
   - Environment variables (OLLAMA_API_KEY)
   - File paths (llm_state.json, queue path)
   - ollama.Client class
  </action>
  <verify>Run `pytest tests/test_llm_integration.py -v` - all tests should pass</verify>
  <done>Integration tests verify complete LLM flow with mocked services, including non-429 cloud retry and error message format.</done>
</task>

</tasks>

<checkpoint type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete LLM integration module with configuration, client, quota tracking, request queue, and comprehensive test suite.</what-built>
  <how-to-verify>
1. Run full test suite: `pytest tests/test_llm*.py -v`
2. Verify all tests pass
3. Check test coverage is reasonable (config, client, quota, queue, integration)
4. Review that mocking approach doesn't require actual Ollama service

Optional manual verification (if local Ollama available):
1. Start local Ollama: `ollama serve`
2. Ensure model pulled: `ollama pull llama3.2:3b`
3. Test local fallback:
   ```python
   from src.llm import chat, get_status
   response = chat([{"role": "user", "content": "Say hello"}])
   print(response)
   print(get_status())
   ```
  </how-to-verify>
  <resume-signal>Type "approved" if tests pass and implementation looks correct, or describe any issues.</resume-signal>
</checkpoint>

<verification>
- [ ] `pytest tests/test_llm_config.py -v` passes
- [ ] `pytest tests/test_llm_client.py -v` passes
- [ ] `pytest tests/test_llm_quota.py -v` passes
- [ ] `pytest tests/test_llm_queue.py -v` passes
- [ ] `pytest tests/test_llm_integration.py -v` passes
- [ ] All tests run without requiring actual Ollama service
- [ ] Tests cover config, failover, retry, cooldown, quota, queue scenarios
- [ ] Tests verify non-429 errors do NOT set cooldown
- [ ] Tests verify LLMUnavailableError message format matches CONTEXT.md
</verification>

<success_criteria>
- All LLM tests pass
- Tests cover:
  - Config loading from TOML with env overrides
  - Client failover from cloud to local
  - Retry with fixed delay via tenacity
  - Rate-limit 429 triggers cooldown
  - Non-429 errors do NOT trigger cooldown (cloud retried on next request)
  - Cooldown state persisted/loaded
  - Quota tracking via headers and local counting
  - Request queue bounded with TTL
  - LLMUnavailableError message format (with and without request_id)
  - End-to-end integration flow
- No external dependencies required for tests (fully mocked)
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-05-SUMMARY.md`
</output>
