---
phase: 03-llm-integration
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/llm/client.py
autonomous: true

must_haves:
  truths:
    - "Cloud Ollama is tried first when not in cooldown"
    - "Errors trigger retry with fixed delay before failover"
    - "Rate-limit (429) triggers 10-minute cooldown"
    - "Failover to local Ollama happens automatically on cloud failure"
    - "Missing local models fail with clear instructions"
  artifacts:
    - path: "src/llm/client.py"
      provides: "OllamaClient with failover logic"
      exports: ["OllamaClient"]
      min_lines: 150
  key_links:
    - from: "src/llm/client.py"
      to: "ollama.Client"
      via: "cloud and local client instances"
      pattern: "Client\\(host="
    - from: "src/llm/client.py"
      to: "tenacity"
      via: "retry decorator"
      pattern: "@retry"
    - from: "src/llm/client.py"
      to: "src/llm/config.py"
      via: "LLMConfig import"
      pattern: "from src\\.llm\\.config import"
---

<objective>
Implement OllamaClient with cloud-first failover and tenacity-based retry logic.

Purpose: The core LLM client that implements the cloud-first/local-fallback pattern. All LLM operations flow through this client which handles retries, cooldowns, and automatic failover per the decisions in CONTEXT.md.
Output: OllamaClient class with chat(), generate(), and embed() methods, all with automatic failover.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-integration/03-CONTEXT.md
@.planning/phases/03-llm-integration/03-RESEARCH.md
@.planning/phases/03-llm-integration/03-01-SUMMARY.md
@src/llm/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement OllamaClient with cloud/local failover</name>
  <files>src/llm/client.py</files>
  <action>
Create `src/llm/client.py` with:

1. **Imports:**
   - ollama.Client, ollama.ResponseError
   - tenacity (retry, stop_after_attempt, wait_fixed, retry_if_exception_type)
   - httpx.Timeout for granular timeout control
   - time for cooldown tracking
   - structlog for failover logging
   - LLMConfig from src.llm.config

2. **OllamaClient class:**

   **__init__(self, config: LLMConfig):**
   - Store config
   - Create cloud Client with:
     - host=config.cloud_endpoint
     - headers={'Authorization': f'Bearer {config.cloud_api_key}'} if api_key present
     - timeout=httpx.Timeout(connect=5.0, read=config.request_timeout_seconds, write=10.0, pool=20.0)
   - Create local Client with:
     - host=config.local_endpoint
     - timeout=httpx.Timeout(connect=2.0, read=60.0, write=10.0, pool=5.0)
   - Initialize cloud_cooldown_until = 0 (unix timestamp)
   - Load cooldown state from llm_state.json if exists (per RESEARCH.md pitfall 5)
   - Initialize structlog logger
   - Track _current_provider: str = "none" for status reporting

   **_is_cloud_available(self) -> bool:**
   - Check if cloud_api_key is set
   - Check if current time >= cloud_cooldown_until
   - Return True if both conditions met

   **_save_cooldown_state(self):**
   - Write {"cloud_cooldown_until": self.cloud_cooldown_until} to llm_state.json
   - Handle file write errors gracefully (log warning, don't crash)

   **_load_cooldown_state(self):**
   - Read from llm_state.json if exists
   - Set self.cloud_cooldown_until from file
   - Handle missing/corrupt file gracefully

   **_retry_cloud(self, operation: str, **kwargs):**
   - Use tenacity @retry decorator with:
     - stop=stop_after_attempt(config.retry_max_attempts)
     - wait=wait_fixed(config.retry_delay_seconds)
     - retry=retry_if_exception_type((ConnectionError, ResponseError))
     - before_sleep callback to log retry attempt
   - Call appropriate cloud client method based on operation
   - On success, set _current_provider = "cloud"
   - Return result

   **_try_local(self, operation: str, model: str | None, **kwargs):**
   - First, verify local Ollama is running and has required models
   - If model not specified, select from config.local_models using fallback chain
   - Try each model in fallback chain until one works
   - If model not pulled, raise clear error: "Model {model} not available. Run: ollama pull {model}"
   - On success, set _current_provider = "local"
   - Return result

   **_check_local_models(self) -> list[str]:**
   - Call local_client.list()
   - Return list of available model names
   - On connection error, raise with message: "Local Ollama not running. Start with: ollama serve"

   **_handle_cloud_error(self, error: ResponseError):**
   - If status_code == 429:
     - Set cooldown: cloud_cooldown_until = time.time() + config.rate_limit_cooldown_seconds
     - Save cooldown state
     - Log warning about rate limit
   - Else:
     - Log warning with status code and error message
   - If config.failover_logging: log failover event

   **chat(self, model: str | None = None, messages: list[dict], **kwargs):**
   - If _is_cloud_available():
     - Try _retry_cloud("chat", model=model or cloud_default, messages=messages, **kwargs)
     - On ResponseError: _handle_cloud_error(e), fall through to local
   - Return _try_local("chat", model, messages=messages, **kwargs)

   **generate(self, model: str | None = None, prompt: str, **kwargs):**
   - Same pattern as chat()

   **embed(self, model: str | None = None, input: str | list[str], **kwargs):**
   - Default model to config.embeddings_model
   - Same failover pattern
   - Note: embeddings typically use local nomic-embed-text, but support cloud

   **@property current_provider(self) -> str:**
   - Return _current_provider ("cloud", "local", or "none")

3. **Helper function:**

   **get_largest_available_model(models: list[str], available: list[str]) -> str | None:**
   - Filter models to only those in available
   - Sort by parameter count (extract from name like "9b" -> 9_000_000_000)
   - Return largest, or None if none available
   - Use regex to handle variations (9b, 9B, etc.)

4. **Exception class:**

   **LLMUnavailableError(Exception):**
   - message: str
   - request_id: str | None (for queue tracking, populated by queue module)
   - Raised when both cloud and local fail

Important implementation notes:
- Do NOT use circuit breaker pattern (per CONTEXT.md - always try cloud on each request)
- Use fixed delay, NOT exponential backoff (per CONTEXT.md)
- Streaming errors: When stream=True, check each yielded object for 'error' key (per RESEARCH.md pitfall 4)
  </action>
  <verify>
Run unit test:
```python
# Test that client initializes with config
from src.llm.config import load_config
from src.llm.client import OllamaClient
config = load_config()
client = OllamaClient(config)
print(f"Cloud available: {client._is_cloud_available()}")
print(f"Current provider: {client.current_provider}")
```
Verify: Should print "Cloud available: False" (no API key) and "Current provider: none"
  </verify>
  <done>OllamaClient implements cloud-first with local fallback, retry with fixed delay, rate-limit cooldown, and clear error messages for missing models.</done>
</task>

<task type="auto">
  <name>Task 2: Add OllamaClient to llm module exports</name>
  <files>src/llm/__init__.py</files>
  <action>
Update `src/llm/__init__.py` to export:
- OllamaClient
- LLMUnavailableError

Add to existing exports from Plan 01 (LLMConfig, load_config).

Update __all__ list to include all exports.
  </action>
  <verify>Run `python -c "from src.llm import OllamaClient, LLMUnavailableError, LLMConfig, load_config; print('All exports available')"` - should succeed</verify>
  <done>OllamaClient and LLMUnavailableError exported from src.llm module.</done>
</task>

</tasks>

<verification>
- [ ] OllamaClient initializes with LLMConfig
- [ ] _is_cloud_available() returns False when no API key configured
- [ ] _is_cloud_available() returns False when in cooldown period
- [ ] Retry decorator configured with fixed delay (not exponential)
- [ ] Rate-limit (429) sets 10-minute cooldown
- [ ] Cooldown state persists to llm_state.json
- [ ] Local fallback tries models in configured order
- [ ] Missing model raises clear error with "ollama pull" instruction
- [ ] All classes exported from src.llm module
</verification>

<success_criteria>
- OllamaClient with chat(), generate(), embed() methods
- Cloud-first with automatic local fallback
- tenacity retry with fixed delay
- Rate-limit cooldown persisted across restarts
- Clear error messages for local Ollama issues
- current_provider property reports active provider
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-02-SUMMARY.md`
</output>
