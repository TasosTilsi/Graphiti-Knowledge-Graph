---
phase: 03-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/llm/__init__.py
  - src/llm/config.py
  - config/llm.toml
autonomous: true
user_setup:
  - service: ollama-cloud
    why: "Cloud Ollama API access"
    env_vars:
      - name: OLLAMA_API_KEY
        source: "Ollama Cloud Dashboard -> API Keys (https://ollama.com/settings/keys)"
    account_setup:
      - task: "Create Ollama Cloud account"
        url: "https://ollama.com"

must_haves:
  truths:
    - "LLM configuration loads from TOML file with sensible defaults"
    - "Environment variables override TOML settings"
    - "Required dependencies (ollama, tenacity, persist-queue) are installed"
    - "Configuration template provides extensive WHY/WHEN guidance for every option"
  artifacts:
    - path: "src/llm/config.py"
      provides: "LLMConfig dataclass and load_config() function"
      exports: ["LLMConfig", "load_config"]
    - path: "config/llm.toml"
      provides: "Default LLM configuration template with extensive documentation"
      contains: "[cloud]"
    - path: "pyproject.toml"
      provides: "Updated dependencies"
      contains: "ollama"
  key_links:
    - from: "src/llm/config.py"
      to: "config/llm.toml"
      via: "tomllib.load"
      pattern: "tomllib\\.load"
    - from: "src/llm/config.py"
      to: "os.environ"
      via: "environment variable overrides"
      pattern: "os\\.getenv|os\\.environ"
---

<objective>
Create LLM configuration foundation with TOML-based settings and environment variable overrides.

Purpose: All LLM components need configuration for endpoints, timeouts, retry settings, and quota thresholds. This plan establishes the configuration system that other plans will depend on.
Output: LLMConfig dataclass, load_config() function, default config template, and updated dependencies.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-integration/03-CONTEXT.md
@.planning/phases/03-llm-integration/03-RESEARCH.md
@pyproject.toml
@src/config/paths.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add LLM dependencies to pyproject.toml</name>
  <files>pyproject.toml</files>
  <action>
Add the following dependencies to pyproject.toml:
- ollama==0.6.1 (official Ollama Python client)
- tenacity==9.1.2 (retry logic with decorators)
- persist-queue==1.1.0 (SQLite-backed persistent queue)
- httpx (used by ollama, provides granular timeout control)

Note: structlog is already a dependency from Phase 2.

Add dependencies in the `[project].dependencies` list, maintaining alphabetical order where practical.
  </action>
  <verify>Run `pip install -e .` and verify all dependencies install without errors. Run `python -c "import ollama; import tenacity; from persistqueue import SQLiteAckQueue"` to confirm imports work.</verify>
  <done>All LLM dependencies are declared and importable.</done>
</task>

<task type="auto">
  <name>Task 2: Create LLM configuration module</name>
  <files>src/llm/__init__.py, src/llm/config.py</files>
  <action>
Create `src/llm/config.py` with:

1. **LLMConfig dataclass** (frozen=True for immutability) with fields:
   - cloud_endpoint: str = "https://ollama.com" (configurable per CONTEXT.md)
   - cloud_api_key: str | None = None (from env)
   - local_endpoint: str = "http://localhost:11434"
   - local_auto_start: bool = False (default OFF per CONTEXT.md)
   - local_models: list[str] = ["gemma2:9b", "llama3.2:3b"] (fallback chain)
   - embeddings_model: str = "nomic-embed-text"
   - retry_max_attempts: int = 3 (1 initial + 2 retries per CONTEXT.md)
   - retry_delay_seconds: int = 10 (fixed delay, not exponential per CONTEXT.md)
   - request_timeout_seconds: int = 90 (per CONTEXT.md)
   - quota_warning_threshold: float = 0.8 (warn at 80% per CONTEXT.md)
   - rate_limit_cooldown_seconds: int = 600 (10 minutes per CONTEXT.md)
   - failover_logging: bool = True (log every failover per CONTEXT.md)
   - queue_max_size: int = 1000 (bounded queue per RESEARCH.md pitfall 8)
   - queue_item_ttl_hours: int = 24 (skip stale items per RESEARCH.md)

2. **load_config(config_path: Path | None = None) -> LLMConfig** function:
   - Default config_path: ~/.graphiti/llm.toml
   - Use tomllib.load() to parse TOML (Python 3.11+ stdlib)
   - Apply environment variable overrides (OLLAMA_API_KEY, OLLAMA_CLOUD_ENDPOINT, OLLAMA_LOCAL_ENDPOINT)
   - Return frozen LLMConfig dataclass
   - If config file doesn't exist, return defaults

3. **get_state_path() -> Path** function:
   - Returns ~/.graphiti/llm_state.json for cooldown persistence (per RESEARCH.md pitfall 5)

Create `src/llm/__init__.py` with:
- Export LLMConfig and load_config
- Module docstring explaining purpose

Use Path.home() / ".graphiti" pattern consistent with src/config/paths.py.
  </action>
  <verify>Run `python -c "from src.llm import LLMConfig, load_config; cfg = load_config(); print(cfg.cloud_endpoint)"` - should print "https://ollama.com"</verify>
  <done>LLMConfig loads from TOML with environment overrides, all fields have sensible defaults.</done>
</task>

<task type="auto">
  <name>Task 3: Create default configuration template with extensive documentation</name>
  <files>config/llm.toml</files>
  <action>
Create `config/llm.toml` as an extensively documented configuration template per CONTEXT.md requirement: "Extensive documentation required for all configurable options."

Documentation must go beyond WHAT each option does -- include WHY it exists and WHEN to change it. Each section should have a header comment explaining the section's purpose, and each option should have multi-line comments covering:
- What the option controls
- Why the default was chosen
- When/why you might change it
- Any gotchas or relationships with other settings

```toml
# =============================================================================
# LLM Configuration for Graphiti Knowledge Graph
# =============================================================================
#
# Copy this file to ~/.graphiti/llm.toml and customize for your setup.
#
# Configuration priority (highest to lowest):
#   1. Environment variables (OLLAMA_API_KEY, OLLAMA_CLOUD_ENDPOINT, etc.)
#   2. Values in ~/.graphiti/llm.toml
#   3. Built-in defaults (shown below)
#
# All options have sensible defaults. You only need to set values you want
# to override. For cloud access, set the OLLAMA_API_KEY environment variable.
# =============================================================================

[cloud]
# Cloud Ollama endpoint URL.
#
# WHY: The default points to Ollama's official cloud service, which provides
# access to larger models without local GPU requirements.
#
# WHEN TO CHANGE: Point this to a custom/self-hosted Ollama instance if you
# run your own cloud infrastructure, or to a different cloud provider that
# exposes an Ollama-compatible API.
#
# NOTE: The API key is set via OLLAMA_API_KEY environment variable (not here)
# to avoid accidentally committing secrets.
endpoint = "https://ollama.com"

# Default model for cloud operations.
#
# WHY: llama3.2:latest provides a good balance of capability and speed for
# knowledge graph operations (entity extraction, relationship inference).
#
# WHEN TO CHANGE: Use a larger model (e.g., llama3.1:70b) for higher-quality
# extractions at the cost of slower responses and higher quota usage. Use a
# smaller model if you're hitting quota limits frequently.
default_model = "llama3.2:latest"

[local]
# Local Ollama server endpoint.
#
# WHY: Defaults to Ollama's standard local port. Local Ollama serves as the
# fallback when cloud is unavailable (rate-limited, down, or unreachable).
#
# WHEN TO CHANGE: Only if you've configured Ollama to run on a different
# port or on a different machine on your local network.
endpoint = "http://localhost:11434"

# Whether to automatically start the Ollama service if not running.
#
# WHY: Default OFF because auto-starting services can be surprising and may
# conflict with system service managers (systemd, launchd). Most users
# prefer explicit control over when Ollama runs.
#
# WHEN TO CHANGE: Set to true if you want a fully hands-off experience and
# don't manage Ollama as a system service. When false (default), you'll get
# a clear error message: "Local Ollama not running. Start with: ollama serve"
#
# NOTE: This setting is config-ready but auto-start is not yet implemented.
# Currently, the system always fails with instructions regardless of this value.
auto_start = false

# Local model fallback chain -- tried in order until one works.
#
# WHY: Multiple models provide resilience. If your preferred model isn't
# pulled or encounters an error, the system tries the next one. Models are
# ordered largest-first for best quality, with smaller models as backup.
#
# WHEN TO CHANGE: Replace with models you've actually pulled locally.
# Run "ollama list" to see what's available. Add models with "ollama pull <name>".
# Order matters: put your preferred (usually largest) model first.
#
# GOTCHA: If none of these models are pulled locally, you'll get an error
# with instructions to pull one. The system won't auto-pull models.
models = ["gemma2:9b", "llama3.2:3b"]

[embeddings]
# Model for generating vector embeddings of text.
#
# WHY: nomic-embed-text is specifically designed for embeddings (not chat),
# produces high-quality 768-dimension vectors, and runs efficiently on CPU.
# It's the standard embedding model in the Ollama ecosystem.
#
# WHEN TO CHANGE: Only if you need a different embedding dimension or have
# a domain-specific embedding model. Changing this after data has been
# embedded will require re-embedding all existing data.
#
# GOTCHA: This model must be pulled separately: "ollama pull nomic-embed-text"
model = "nomic-embed-text"

[retry]
# Maximum number of attempts per request (1 initial + N-1 retries).
#
# WHY: 3 attempts balances reliability with responsiveness. Most transient
# errors (network blips, temporary overload) resolve within 2 retries.
# More retries would delay failover to local unnecessarily.
#
# WHEN TO CHANGE: Increase if you're on an unstable connection and prefer
# waiting longer before falling back to local. Decrease to 1 (no retries)
# if you want instant failover on any error.
max_attempts = 3

# Fixed delay between retries in seconds.
#
# WHY: A fixed 10-second delay (not exponential backoff) keeps behavior
# predictable and simple. Exponential backoff is better for high-throughput
# services; for a single-user knowledge graph tool, fixed delay is clearer.
#
# WHEN TO CHANGE: Decrease for faster failover (at risk of hitting a
# temporarily overloaded server repeatedly). Increase if the cloud service
# recommends longer retry intervals.
#
# NOTE: Total worst-case delay before failover = (max_attempts - 1) * delay_seconds
# With defaults: (3-1) * 10 = 20 seconds maximum before trying local.
delay_seconds = 10

# Request timeout in seconds (how long to wait for a response).
#
# WHY: LLM generation can take 30-60+ seconds for complex prompts. The
# 90-second default gives ample time for generation while still failing
# on truly stuck requests. Connection timeout is separate (5s hardcoded).
#
# WHEN TO CHANGE: Decrease if you prefer faster failures (at risk of
# timing out long but valid generations). Increase for very large context
# prompts or slow network connections.
#
# GOTCHA: This is the read timeout only. Connection timeout (5s) and write
# timeout (10s) are hardcoded for predictable behavior.
timeout_seconds = 90

[quota]
# Warn when cloud quota usage reaches this percentage (0.0 to 1.0).
#
# WHY: Early warning at 80% gives you time to adjust usage patterns or
# upgrade your plan before hitting hard limits and forced failover.
#
# WHEN TO CHANGE: Lower the threshold if you want earlier warnings (e.g.,
# 0.5 for 50%). Set to 1.0 to disable warnings entirely (not recommended).
# The system continues using cloud until the actual limit is hit regardless
# of this setting -- it only controls when warnings appear in logs.
warning_threshold = 0.8

# Cooldown period in seconds after receiving a rate-limit response (HTTP 429).
#
# WHY: 10 minutes (600s) avoids hammering a rate-limited API. Cloud providers
# typically reset rate limits in 1-15 minute windows. During cooldown, all
# requests go directly to local Ollama without attempting cloud.
#
# WHEN TO CHANGE: Decrease if your cloud provider has shorter rate-limit
# windows. Increase if you're consistently hitting rate limits (suggests
# you need a higher-tier plan).
#
# NOTE: This cooldown ONLY applies to rate-limit (429) errors. Other errors
# (500, timeout, connection failures) do NOT trigger cooldown -- cloud is
# tried again on the very next request for non-rate-limit errors.
rate_limit_cooldown_seconds = 600

[queue]
# Maximum number of requests to keep in the retry queue.
#
# WHY: Prevents unbounded disk growth if the LLM is down for extended
# periods. 1000 requests is generous for a local knowledge graph tool.
# When the limit is hit, oldest items are dropped to make room.
#
# WHEN TO CHANGE: Increase if you process large batches and want to ensure
# nothing is lost during extended outages. Decrease if disk space is limited
# (queue uses SQLite, ~1KB per queued request).
max_size = 1000

# Skip queued requests older than this many hours.
#
# WHY: Stale requests may reference data that's changed or no longer relevant.
# 24 hours is a reasonable window -- if LLM was down for more than a day,
# the queued work likely needs human review anyway.
#
# WHEN TO CHANGE: Increase for batch processing workflows where old requests
# remain valid. Decrease if your data changes rapidly and stale operations
# could cause inconsistencies.
item_ttl_hours = 24

[logging]
# Log every failover event (when cloud fails and local is used).
#
# WHY: Default ON so you can see how often failover happens and diagnose
# connectivity or quota issues. Each failover log includes the error that
# triggered it.
#
# WHEN TO CHANGE: Set to false if failover is expected (e.g., no cloud
# API key configured, intentionally local-only) and the log messages are
# noise. Keep true during initial setup and troubleshooting.
failover_events = true
```

Ensure directory `config/` exists.
  </action>
  <verify>Run `python -c "import tomllib; f=open('config/llm.toml', 'rb'); data=tomllib.load(f); print(f'Sections: {list(data.keys())}')"` - should parse without errors and show all sections. Verify that each option has WHY/WHEN comments by checking line count is substantial (expect 150+ lines with documentation).</verify>
  <done>Default configuration template exists with extensive WHY/WHEN documentation for every configurable option, meeting CONTEXT.md "extensive documentation" requirement.</done>
</task>

</tasks>

<verification>
- [ ] `pip install -e .` installs all dependencies
- [ ] `python -c "import ollama; import tenacity; from persistqueue import SQLiteAckQueue"` succeeds
- [ ] `python -c "from src.llm import LLMConfig, load_config"` succeeds
- [ ] load_config() returns LLMConfig with default values when no file exists
- [ ] config/llm.toml parses as valid TOML
- [ ] Every configuration option has WHY and WHEN-TO-CHANGE documentation
- [ ] Configuration template includes priority explanation (env > TOML > defaults)
- [ ] rate_limit_cooldown_seconds comment clarifies 429-only behavior
</verification>

<success_criteria>
- LLMConfig dataclass exists with all fields from CONTEXT.md decisions
- load_config() reads TOML and applies env var overrides
- Default config template provides extensive WHY/WHEN documentation for all options
- Each option explains what it does, why the default was chosen, and when to change it
- Dependencies (ollama, tenacity, persist-queue) install and import
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-01-SUMMARY.md`
</output>
