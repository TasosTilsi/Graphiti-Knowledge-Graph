---
phase: 03-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/llm/__init__.py
  - src/llm/config.py
  - config/llm.toml
autonomous: true
user_setup:
  - service: ollama-cloud
    why: "Cloud Ollama API access"
    env_vars:
      - name: OLLAMA_API_KEY
        source: "Ollama Cloud Dashboard -> API Keys (https://ollama.com/settings/keys)"
    account_setup:
      - task: "Create Ollama Cloud account"
        url: "https://ollama.com"

must_haves:
  truths:
    - "LLM configuration loads from TOML file with sensible defaults"
    - "Environment variables override TOML settings"
    - "Required dependencies (ollama, tenacity, persist-queue) are installed"
  artifacts:
    - path: "src/llm/config.py"
      provides: "LLMConfig dataclass and load_config() function"
      exports: ["LLMConfig", "load_config"]
    - path: "config/llm.toml"
      provides: "Default LLM configuration template"
      contains: "[cloud]"
    - path: "pyproject.toml"
      provides: "Updated dependencies"
      contains: "ollama"
  key_links:
    - from: "src/llm/config.py"
      to: "config/llm.toml"
      via: "tomllib.load"
      pattern: "tomllib\\.load"
    - from: "src/llm/config.py"
      to: "os.environ"
      via: "environment variable overrides"
      pattern: "os\\.getenv|os\\.environ"
---

<objective>
Create LLM configuration foundation with TOML-based settings and environment variable overrides.

Purpose: All LLM components need configuration for endpoints, timeouts, retry settings, and quota thresholds. This plan establishes the configuration system that other plans will depend on.
Output: LLMConfig dataclass, load_config() function, default config template, and updated dependencies.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-integration/03-CONTEXT.md
@.planning/phases/03-llm-integration/03-RESEARCH.md
@pyproject.toml
@src/config/paths.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add LLM dependencies to pyproject.toml</name>
  <files>pyproject.toml</files>
  <action>
Add the following dependencies to pyproject.toml:
- ollama==0.6.1 (official Ollama Python client)
- tenacity==9.1.2 (retry logic with decorators)
- persist-queue==1.1.0 (SQLite-backed persistent queue)
- httpx (used by ollama, provides granular timeout control)

Note: structlog is already a dependency from Phase 2.

Add dependencies in the `[project].dependencies` list, maintaining alphabetical order where practical.
  </action>
  <verify>Run `pip install -e .` and verify all dependencies install without errors. Run `python -c "import ollama; import tenacity; from persistqueue import SQLiteAckQueue"` to confirm imports work.</verify>
  <done>All LLM dependencies are declared and importable.</done>
</task>

<task type="auto">
  <name>Task 2: Create LLM configuration module</name>
  <files>src/llm/__init__.py, src/llm/config.py</files>
  <action>
Create `src/llm/config.py` with:

1. **LLMConfig dataclass** (frozen=True for immutability) with fields:
   - cloud_endpoint: str = "https://ollama.com" (configurable per CONTEXT.md)
   - cloud_api_key: str | None = None (from env)
   - local_endpoint: str = "http://localhost:11434"
   - local_auto_start: bool = False (default OFF per CONTEXT.md)
   - local_models: list[str] = ["gemma2:9b", "llama3.2:3b"] (fallback chain)
   - embeddings_model: str = "nomic-embed-text"
   - retry_max_attempts: int = 3 (1 initial + 2 retries per CONTEXT.md)
   - retry_delay_seconds: int = 10 (fixed delay, not exponential per CONTEXT.md)
   - request_timeout_seconds: int = 90 (per CONTEXT.md)
   - quota_warning_threshold: float = 0.8 (warn at 80% per CONTEXT.md)
   - rate_limit_cooldown_seconds: int = 600 (10 minutes per CONTEXT.md)
   - failover_logging: bool = True (log every failover per CONTEXT.md)
   - queue_max_size: int = 1000 (bounded queue per RESEARCH.md pitfall 8)
   - queue_item_ttl_hours: int = 24 (skip stale items per RESEARCH.md)

2. **load_config(config_path: Path | None = None) -> LLMConfig** function:
   - Default config_path: ~/.graphiti/llm.toml
   - Use tomllib.load() to parse TOML (Python 3.11+ stdlib)
   - Apply environment variable overrides (OLLAMA_API_KEY, OLLAMA_CLOUD_ENDPOINT, OLLAMA_LOCAL_ENDPOINT)
   - Return frozen LLMConfig dataclass
   - If config file doesn't exist, return defaults

3. **get_state_path() -> Path** function:
   - Returns ~/.graphiti/llm_state.json for cooldown persistence (per RESEARCH.md pitfall 5)

Create `src/llm/__init__.py` with:
- Export LLMConfig and load_config
- Module docstring explaining purpose

Use Path.home() / ".graphiti" pattern consistent with src/config/paths.py.
  </action>
  <verify>Run `python -c "from src.llm import LLMConfig, load_config; cfg = load_config(); print(cfg.cloud_endpoint)"` - should print "https://ollama.com"</verify>
  <done>LLMConfig loads from TOML with environment overrides, all fields have sensible defaults.</done>
</task>

<task type="auto">
  <name>Task 3: Create default configuration template</name>
  <files>config/llm.toml</files>
  <action>
Create `config/llm.toml` as a well-documented configuration template.

Include all sections with extensive documentation (per CONTEXT.md requirement):

```toml
# LLM Configuration for Graphiti Knowledge Graph
# Copy to ~/.graphiti/llm.toml and customize

[cloud]
# Cloud Ollama endpoint (default: official Ollama cloud)
# Can be set to custom/self-hosted endpoint
endpoint = "https://ollama.com"

# Default model for cloud operations
# Note: Actual model depends on cloud tier
default_model = "llama3.2:latest"

[local]
# Local Ollama endpoint
endpoint = "http://localhost:11434"

# Auto-start Ollama service if not running (default: false)
# When false, fails with instructions: "Run: ollama serve"
auto_start = false

# Fallback chain: tried in order until one works
# Default: largest-param models first for best quality
models = ["gemma2:9b", "llama3.2:3b"]

[embeddings]
# Model for generating embeddings
model = "nomic-embed-text"

[retry]
# Maximum attempts (1 initial + N retries)
max_attempts = 3

# Fixed delay between retries in seconds (not exponential)
delay_seconds = 10

# Request timeout in seconds
# Set long for LLM generation, but connection timeout is separate (5s)
timeout_seconds = 90

[quota]
# Log warning when quota usage reaches this percentage
warning_threshold = 0.8

# Cooldown period in seconds after rate limit (HTTP 429)
rate_limit_cooldown_seconds = 600

[queue]
# Maximum queued requests (prevents unbounded growth)
max_size = 1000

# Skip queued items older than this many hours
item_ttl_hours = 24

[logging]
# Log every failover event (cloud -> local)
failover_events = true
```

Ensure directory `config/` exists.
  </action>
  <verify>Run `python -c "import tomllib; f=open('config/llm.toml', 'rb'); tomllib.load(f)"` - should parse without errors</verify>
  <done>Default configuration template exists with extensive documentation for all options.</done>
</task>

</tasks>

<verification>
- [ ] `pip install -e .` installs all dependencies
- [ ] `python -c "import ollama; import tenacity; from persistqueue import SQLiteAckQueue"` succeeds
- [ ] `python -c "from src.llm import LLMConfig, load_config"` succeeds
- [ ] load_config() returns LLMConfig with default values when no file exists
- [ ] config/llm.toml parses as valid TOML
- [ ] All configuration options documented in template
</verification>

<success_criteria>
- LLMConfig dataclass exists with all fields from CONTEXT.md decisions
- load_config() reads TOML and applies env var overrides
- Default config template provides documentation for all options
- Dependencies (ollama, tenacity, persist-queue) install and import
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-01-SUMMARY.md`
</output>
