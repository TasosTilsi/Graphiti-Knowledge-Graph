---
phase: 8.6-gap-closure-runtime-bugs-inserted
plan: "02"
type: execute
wave: 2
depends_on: ["8.6-01"]
files_modified:
  - src/queue/__init__.py
autonomous: true
gap_closure: true
requirements:
  - R4.2

must_haves:
  truths:
    - "graphiti queue process completes all jobs before exiting"
    - "Worker is not stopped while jobs are still in-flight (unacked state)"
    - "Queue shows 0 pending after process command completes successfully"
  artifacts:
    - path: "src/queue/__init__.py"
      provides: "process_queue() that waits for worker thread to fully finish, not just qsize() == 0"
      contains: "worker.stop"
  key_links:
    - from: "src/queue/__init__.py"
      to: "BackgroundWorker"
      via: "process_queue() calls worker.stop() only after queue truly drained"
      pattern: "process_queue"
---

<objective>
Fix the race condition in `process_queue()` (`src/queue/__init__.py`) where the main thread stops the background worker prematurely.

**Root cause**: `SQLiteAckQueue.qsize()` counts only jobs in "ready" state. When the worker calls `get_batch()`, jobs move to "unacked" (in-flight) state and `qsize()` drops to 0 — even though the jobs haven't been processed yet. The main loop in `process_queue()` sees 0 and calls `worker.stop()`, interrupting in-flight processing.

**Observed behaviour (2026-02-26)**:
- `graphiti queue process` logged only 2 startup lines then appeared frozen for 20+ minutes
- Queue showed `pending: 2, worker: stopped` after exit
- `pending_commits` file was never cleared (worker stopped before calling `process_pending_commits()`)

**Fix**: Instead of polling `qsize()`, introduce a worker-level "done" signal. After the worker processes all jobs and the queue is empty (no ready AND no unacked jobs), it sets an `Event`. The main thread waits on this event instead of polling `qsize()`.

Simpler alternative (preferred for minimal change): After `worker.start()`, poll both `queue.get_pending_count()` (ready) AND check `worker.is_running()`. Stop only when pending==0 AND worker has gone idle (no batch in progress). Use a short sleep and a "consecutive zero" counter — stop after 3 consecutive seconds of pending==0 with no active batch.

**Simplest correct fix** (least code change): Replace the `qsize()`-based loop with a fixed pattern:
1. Start worker
2. Wait until `get_pending_count() == 0` (ready jobs drained) — same as before
3. Then call `worker.stop(timeout=60)` — `stop()` already joins the thread, so if a job is still running in the thread it will complete before the join returns (up to 60s)
4. Return counts

This works because `worker.stop()` sets the stop event and then joins the thread. The thread won't exit until it finishes its current `_process_single_job()` call (the retry loop checks `_stop_event` only between jobs, not mid-job). So the join in `stop()` ensures in-flight work completes.

The real bug was that `qsize()` returned 0 while a job was still in-flight (unacked). The fix is to NOT rely on qsize() as the sole signal — instead rely on `worker.stop()` (which joins the thread) to be the final gate.
</objective>

<execution_context>
@/home/tasostilsi/.claude/get-shit-done/workflows/execute-plan.md
@/home/tasostilsi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@src/queue/__init__.py
@src/queue/worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix process_queue() to not stop worker while jobs are in-flight</name>
  <files>src/queue/__init__.py</files>
  <action>
Read `src/queue/__init__.py` fully. Find `process_queue()` (currently around line 212).

Current (broken) implementation:
```python
def process_queue() -> tuple[int, int]:
    worker = get_worker()
    if not worker.is_running():
        worker.start()
        logger.info("worker_started_for_manual_processing")

    queue = get_queue()
    import time
    while queue.get_pending_count() > 0:
        time.sleep(1)

    worker.stop()
    logger.info("manual_processing_complete")
    return (0, 0)
```

**Replace with**:
```python
def process_queue() -> tuple[int, int]:
    """Process queue manually (CLI fallback).

    Starts worker if not running and processes all pending jobs.
    Waits until the queue is empty AND the worker thread has fully
    finished its current job before returning.

    The previous implementation only waited for qsize()==0, but
    SQLiteAckQueue.qsize() returns 0 as soon as jobs are dequeued
    into "unacked" (in-flight) state — before they are actually
    processed. This caused the worker to be stopped mid-job.

    Fix: after qsize() drops to 0, call worker.stop() which joins
    the worker thread. The thread completes its current job before
    the stop event causes it to exit, so join() returns only after
    all in-flight work is done.

    Returns:
        Tuple of (success_count, failure_count) — placeholder counts
        (full tracking deferred, consistent with original implementation)
    """
    import time

    worker = get_worker()

    if not worker.is_running():
        worker.start()
        logger.info("worker_started_for_manual_processing")

    queue = get_queue()

    # Wait for all "ready" jobs to be picked up by the worker.
    # Once qsize()==0, all jobs are either in-flight or done.
    while queue.get_pending_count() > 0:
        time.sleep(0.5)

    # Stop the worker and JOIN the thread. worker.stop() signals the
    # stop event and then calls thread.join(timeout). The worker thread
    # only checks the stop event between jobs — it completes any
    # in-flight job before exiting. This ensures in-flight work finishes.
    worker.stop(timeout=120.0)

    logger.info("manual_processing_complete")

    return (0, 0)
```

Key changes:
- Sleep reduced to 0.5s for faster polling
- `worker.stop(timeout=120.0)` replaces the old stop() after loop — the 120s timeout allows for slow LLM calls
- Added docstring explaining the race condition and fix
  </action>
  <verify>
```bash
# Correct function is in place
grep -n -A 20 "def process_queue" src/queue/__init__.py | head -30

# Module imports cleanly
.venv/bin/python -c "from src.queue import process_queue; print('ok')"
```
Expected: Function body contains `worker.stop(timeout=120` and `time.sleep(0.5)`. Module imports without error.
  </verify>
  <done>
`process_queue()` updated with the fix. Module imports cleanly. Docstring explains the race condition.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify fix with a live queue process run</name>
  <files>src/queue/__init__.py</files>
  <action>
Re-enqueue a test job and verify `graphiti queue process` completes it. Since `pending_commits` was cleared by the earlier direct invocation, we need to create fresh test conditions.

Check the current queue state first:
```bash
.venv/bin/graphiti queue status
```

If queue is empty (pending=0, dead_letter=0), the fix can be verified by making a small test commit that triggers the hook, then running queue process:

```bash
# Check queue
.venv/bin/graphiti queue status

# If empty — check that pending_commits is empty too
cat ~/.graphiti/pending_commits 2>/dev/null | wc -l
```

If there is nothing to process, just verify the code change is correct structurally — the actual end-to-end verification will happen as part of the Phase 06 human verification re-run (Test 3 and Test 4) after this phase completes.

For the commit:
```bash
git add .planning/phases/08.6-gap-closure-runtime-bugs-inserted/
git add .planning/ROADMAP.md
git add .planning/STATE.md
git add src/queue/__init__.py
git add src/graph/adapters.py
git commit -m "fix: normalize dot-prefixed LLM field names and fix process_queue() race condition"
```
  </action>
  <verify>
```bash
# Commit exists
git log --oneline -1

# Both fixed files in commit
git show --name-only HEAD | grep -E "adapters|__init__"
```
Expected: Commit present, both `src/graph/adapters.py` and `src/queue/__init__.py` in the diff.
  </verify>
  <done>
Changes committed. Both bug fixes in a single atomic commit.
  </done>
</task>

</tasks>

<verification>
1. `process_queue()` docstring mentions "in-flight" and fix:
   ```bash
   grep -n "in.flight\|timeout=120" src/queue/__init__.py
   ```
   Expected: both present.

2. Module clean:
   ```bash
   .venv/bin/python -c "from src.queue import process_queue, get_queue, get_worker; print('ok')"
   ```

3. Queue status shows healthy state after the commit:
   ```bash
   .venv/bin/graphiti queue status
   ```
</verification>

<success_criteria>
- `process_queue()` no longer stops the worker while jobs are in-flight
- Worker thread is joined (via `stop(timeout=120)`) ensuring in-flight jobs complete before function returns
- Fix documented with clear explanation of the race condition
- Both bugs committed atomically
- Phase 06 human verification Test 3 (excluded files) and Test 4 (captured knowledge queryable) can now be re-run successfully
</success_criteria>

<output>
After completion, create `.planning/phases/08.6-gap-closure-runtime-bugs-inserted/8.6-02-SUMMARY.md` following the GSD summary template.
</output>
